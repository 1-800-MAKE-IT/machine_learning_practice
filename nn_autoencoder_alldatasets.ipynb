{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyOSRtwEeS7quHd6n30Ytg/x"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["\n","\n","---\n","\n","# Intro - Autoencoder\n","\n","**Plan** - produce proof of concept autoencoder architecture.\n","\n","**Purpose**: dimensionality reduction for input data.\n","\n","**Hypothesis**: with automatic HP tuning an autoencoder can be used to reduce dimensionality of input data, whilst retaining adequate information to accurately reproduce input data.\n","\n","**Methodology**: Test on multiple datasets - first the Iris dataset, then Pima Indians, then finally credit card fraud dataset. Evaluate and assess model architecture and visualize latent space using PCA/UMAP etc."],"metadata":{"id":"eYdyevAw-Xto"}},{"cell_type":"markdown","source":["\n","# First dataset - iris dataset\n","---\n","## Data sourcing and processing\n"],"metadata":{"id":"wwPqhLkz_hhD"}},{"cell_type":"code","source":["\n","#import packages :\n","\n","import os\n","os.environ['NUMBAPRO_NVVM'] = '/usr/local/cuda/nvvm/lib64/libnvvm.so'\n","os.environ['NUMBAPRO_LIBDEVICE'] = '/usr/local/cuda/nvvm/libdevice/'\n","\n","from google.colab import drive\n","\n","try:\n","  import google.colab\n","  IN_COLAB = True\n","except:\n","  IN_COLAB = False\n","\n","if IN_COLAB:\n","  # Check if drive is mounted by looking for the mount point in the file system.\n","  # This is a more robust approach than relying on potentially internal variables.\n","  import os\n","  if not os.path.exists('/content/drive'):\n","    drive.mount('/content/drive')\n","\n","#basics\n","import os\n","from google.colab import drive\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","!pip install tqdm\n","from tqdm import tqdm\n","\n","#table one\n","!pip install tableone\n","from tableone import TableOne\n","\n","#torch\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","#sklearn\n","!pip install scikit-optimize\n","from skopt import gp_minimize\n","from skopt.space import Integer, Real, Categorical\n","\n","from imblearn.over_sampling import RandomOverSampler\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import StratifiedKFold, GridSearchCV\n","from sklearn.metrics import roc_auc_score\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import RepeatedStratifiedKFold\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","from torch.utils.data import DataLoader, random_split, Dataset\n","from sklearn.datasets import load_breast_cancer\n","!pip install skopt\n","from skopt import gp_minimize\n","from skopt.space import Integer, Real, Categorical\n","from skopt.utils import use_named_args\n","from sklearn.preprocessing import MinMaxScaler\n","\n","\n","\n","from imblearn.over_sampling import RandomOverSampler"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5MvDG0rB_st0","executionInfo":{"status":"ok","timestamp":1733775232864,"user_tz":0,"elapsed":95477,"user":{"displayName":"Archie Goodman","userId":"05960694724077487952"}},"outputId":"6de39cc5-7633-4a9a-c1ff-2ef46ca1a92c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n","Collecting tableone\n","  Downloading tableone-0.9.1-py3-none-any.whl.metadata (8.5 kB)\n","Requirement already satisfied: jinja2>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from tableone) (3.1.4)\n","Requirement already satisfied: numpy>=1.19.1 in /usr/local/lib/python3.10/dist-packages (from tableone) (1.26.4)\n","Requirement already satisfied: openpyxl>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from tableone) (3.1.5)\n","Requirement already satisfied: pandas>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from tableone) (2.2.2)\n","Requirement already satisfied: scipy>=1.10.1 in /usr/local/lib/python3.10/dist-packages (from tableone) (1.13.1)\n","Requirement already satisfied: statsmodels>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from tableone) (0.14.4)\n","Requirement already satisfied: tabulate>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tableone) (0.9.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=3.1.4->tableone) (3.0.2)\n","Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl>=3.1.2->tableone) (2.0.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.3->tableone) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.3->tableone) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.3->tableone) (2024.2)\n","Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.14.1->tableone) (1.0.1)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.14.1->tableone) (24.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.3->tableone) (1.16.0)\n","Downloading tableone-0.9.1-py3-none-any.whl (41 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tableone\n","Successfully installed tableone-0.9.1\n","Collecting scikit-optimize\n","  Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl.metadata (9.7 kB)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.4.2)\n","Collecting pyaml>=16.9 (from scikit-optimize)\n","  Downloading pyaml-24.9.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.26.4)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.13.1)\n","Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.5.2)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (24.2)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.5.0)\n","Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl (107 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.8/107.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyaml-24.9.0-py3-none-any.whl (24 kB)\n","Installing collected packages: pyaml, scikit-optimize\n","Successfully installed pyaml-24.9.0 scikit-optimize-0.10.2\n","\u001b[31mERROR: Could not find a version that satisfies the requirement skopt (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for skopt\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["#import data and visualize\n","\n","# Load the breast cancer dataset\n","data = load_breast_cancer()\n","\n","# Convert to Pandas DataFrame\n","bc_df = pd.DataFrame(data.data, columns=data.feature_names)\n","\n","# Add the target column (malignant or benign)\n","bc_df['target'] = data.target\n","\n","bc_columns = bc_df.columns.tolist()\n","\n","print(f\"The dataset lenghth is {str(len(bc_df))}\")\n","print(f\"The number of columns is {str(len(bc_columns))}\")\n","print(f\"The column names are {str(bc_columns)} \\n\")\n","\n","table1 = TableOne(bc_df, columns=bc_columns, groupby= 'target', pval=True)\n","print(table1)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hAl3GsVr_yWX","executionInfo":{"status":"ok","timestamp":1733775234514,"user_tz":0,"elapsed":1655,"user":{"displayName":"Archie Goodman","userId":"05960694724077487952"}},"outputId":"8a52e42a-0d41-4d86-dabd-b2287796ecd4"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["The dataset lenghth is 569\n","The number of columns is 31\n","The column names are ['mean radius', 'mean texture', 'mean perimeter', 'mean area', 'mean smoothness', 'mean compactness', 'mean concavity', 'mean concave points', 'mean symmetry', 'mean fractal dimension', 'radius error', 'texture error', 'perimeter error', 'area error', 'smoothness error', 'compactness error', 'concavity error', 'concave points error', 'symmetry error', 'fractal dimension error', 'worst radius', 'worst texture', 'worst perimeter', 'worst area', 'worst smoothness', 'worst compactness', 'worst concavity', 'worst concave points', 'worst symmetry', 'worst fractal dimension', 'target'] \n","\n","                                    Grouped by target                                                      \n","                                              Missing        Overall               0              1 P-Value\n","n                                                                569             212            357        \n","mean radius, mean (SD)                              0     14.1 (3.5)      17.5 (3.2)     12.1 (1.8)  <0.001\n","mean texture, mean (SD)                             0     19.3 (4.3)      21.6 (3.8)     17.9 (4.0)  <0.001\n","mean perimeter, mean (SD)                           0    92.0 (24.3)    115.4 (21.9)    78.1 (11.8)  <0.001\n","mean area, mean (SD)                                0  654.9 (351.9)   978.4 (367.9)  462.8 (134.3)  <0.001\n","mean smoothness, mean (SD)                          0      0.1 (0.0)       0.1 (0.0)      0.1 (0.0)  <0.001\n","mean compactness, mean (SD)                         0      0.1 (0.1)       0.1 (0.1)      0.1 (0.0)  <0.001\n","mean concavity, mean (SD)                           0      0.1 (0.1)       0.2 (0.1)      0.0 (0.0)  <0.001\n","mean concave points, mean (SD)                      0      0.0 (0.0)       0.1 (0.0)      0.0 (0.0)  <0.001\n","mean symmetry, mean (SD)                            0      0.2 (0.0)       0.2 (0.0)      0.2 (0.0)  <0.001\n","mean fractal dimension, mean (SD)                   0      0.1 (0.0)       0.1 (0.0)      0.1 (0.0)   0.767\n","radius error, mean (SD)                             0      0.4 (0.3)       0.6 (0.3)      0.3 (0.1)  <0.001\n","texture error, mean (SD)                            0      1.2 (0.6)       1.2 (0.5)      1.2 (0.6)   0.835\n","perimeter error, mean (SD)                          0      2.9 (2.0)       4.3 (2.6)      2.0 (0.8)  <0.001\n","area error, mean (SD)                               0    40.3 (45.5)     72.7 (61.4)     21.1 (8.8)  <0.001\n","smoothness error, mean (SD)                         0      0.0 (0.0)       0.0 (0.0)      0.0 (0.0)   0.105\n","compactness error, mean (SD)                        0      0.0 (0.0)       0.0 (0.0)      0.0 (0.0)  <0.001\n","concavity error, mean (SD)                          0      0.0 (0.0)       0.0 (0.0)      0.0 (0.0)  <0.001\n","concave points error, mean (SD)                     0      0.0 (0.0)       0.0 (0.0)      0.0 (0.0)  <0.001\n","symmetry error, mean (SD)                           0      0.0 (0.0)       0.0 (0.0)      0.0 (0.0)   0.887\n","fractal dimension error, mean (SD)                  0      0.0 (0.0)       0.0 (0.0)      0.0 (0.0)   0.042\n","worst radius, mean (SD)                             0     16.3 (4.8)      21.1 (4.3)     13.4 (2.0)  <0.001\n","worst texture, mean (SD)                            0     25.7 (6.1)      29.3 (5.4)     23.5 (5.5)  <0.001\n","worst perimeter, mean (SD)                          0   107.3 (33.6)    141.4 (29.5)    87.0 (13.5)  <0.001\n","worst area, mean (SD)                               0  880.6 (569.4)  1422.3 (598.0)  558.9 (163.6)  <0.001\n","worst smoothness, mean (SD)                         0      0.1 (0.0)       0.1 (0.0)      0.1 (0.0)  <0.001\n","worst compactness, mean (SD)                        0      0.3 (0.2)       0.4 (0.2)      0.2 (0.1)  <0.001\n","worst concavity, mean (SD)                          0      0.3 (0.2)       0.5 (0.2)      0.2 (0.1)  <0.001\n","worst concave points, mean (SD)                     0      0.1 (0.1)       0.2 (0.0)      0.1 (0.0)  <0.001\n","worst symmetry, mean (SD)                           0      0.3 (0.1)       0.3 (0.1)      0.3 (0.0)  <0.001\n","worst fractal dimension, mean (SD)                  0      0.1 (0.0)       0.1 (0.0)      0.1 (0.0)  <0.001\n","\n"]}]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","# Autoencoder (Breast cancer dataset)"],"metadata":{"id":"qURIYCUa6Eu9"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, random_split, Dataset\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import mean_squared_error\n","import numpy as np\n","from skopt import gp_minimize\n","from skopt.space import Integer, Real, Categorical\n","\n","# **Set device for GPU acceleration**\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","if device.type != 'cuda':\n","    print(\"WARNING: GPU is not available. The model will run on the CPU, which might be slower.\")\n","else:\n","    print(\"Cuda setup successful\")\n","\n","# Load and preprocess the Breast Cancer dataset\n","breast_cancer = load_breast_cancer()\n","data = breast_cancer.data\n","\n","# Scale data to the range [0, 1] for better convergence\n","scaler = MinMaxScaler()\n","data = scaler.fit_transform(data)\n","\n","# Convert to PyTorch Dataset\n","class BreastCancerDataset(Dataset):\n","    def __init__(self, data):\n","        self.data = torch.tensor(data, dtype=torch.float32)\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        return self.data[idx]\n","\n","dataset = BreastCancerDataset(data)\n","\n","#----------------- Split into training, validation, and test sets -----------------#\n","\n","#70% train data\n","train_size = int(0.7 * len(dataset))\n","#20% validation set holdout (aka dev set)\n","val_size = int(0.2 * len(dataset))\n","#remaining data is test data\n","test_size = len(dataset) - train_size - val_size\n","#randomize split\n","train_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size])\n","\n","# Flexible Autoencoder architecture\n","class FlexibleAutoencoder(nn.Module):\n","    def __init__(self, input_dim, encoder_layers, decoder_layers, activations, dropout_prob, use_batchnorm):\n","        super(FlexibleAutoencoder, self).__init__()\n","        # Build the encoder\n","        encoder = []\n","        in_dim = input_dim\n","\n","        #flexible number of encoder layers (independent of decoder layers as that number is usually greater)\n","        for i in range(len(encoder_layers)):\n","            encoder.append(nn.Linear(in_dim, encoder_layers[i]))\n","\n","            #Batchnorm will be a HP\n","            if use_batchnorm:\n","                encoder.append(nn.BatchNorm1d(encoder_layers[i]))\n","            encoder.append(getattr(nn, activations[i])())\n","\n","            if dropout_prob > 0:  # Apply dropout only if > 0\n","                encoder.append(nn.Dropout(dropout_prob))\n","\n","            in_dim = encoder_layers[i]\n","\n","        self.encoder = nn.Sequential(*encoder)\n","\n","        # Build the decoder\n","        decoder = []\n","        in_dim = encoder_layers[-1]\n","\n","        #flexible number of decoder layers\n","        for i in range(len(decoder_layers)):\n","            out_dim = input_dim if i == len(decoder_layers) - 1 else decoder_layers[i]\n","            decoder.append(nn.Linear(in_dim, out_dim))\n","\n","            #batch norm will be a HP\n","            if use_batchnorm and i != len(decoder_layers) - 1:\n","                decoder.append(nn.BatchNorm1d(out_dim))\n","            decoder.append(getattr(nn, activations[len(encoder_layers) + i])())\n","\n","            #whether or not we use dropout is also a HP\n","            if dropout_prob > 0 and i != len(decoder_layers) - 1:\n","                decoder.append(nn.Dropout(dropout_prob))\n","            in_dim = decoder_layers[i]\n","\n","        self.decoder = nn.Sequential(*decoder)\n","\n","    def forward(self, x):\n","        #define forward pass through encoder to get latent space representation\n","        latent = self.encoder(x)\n","        #reconstruct data from latent space representation\n","        reconstructed = self.decoder(latent)\n","        return latent, reconstructed\n","\n","# Custom loss function combining MSE and latent space regularization\n","def custom_loss_function(reconstructed, original, latent_representation, alpha=0.01):\n","    #the purpose of this is to balance latent space dimensionality with MSE loss\n","    reconstruction_loss = nn.MSELoss()(reconstructed, original)\n","    regularization_loss = alpha * torch.norm(latent_representation, p=1)  # L1 norm for compactness of latent space representation\n","    return reconstruction_loss + regularization_loss\n","\n","    # Training and validation function\n","def train_validate_autoencoder(encoder_layers, decoder_layers, activations, lr, batch_size, dropout_prob, latent_dim):\n","    # Prepare dataloaders\n","    batch_size = int(batch_size)\n","    #load data in batches\n","    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n","\n","    # The following line is changed to set drop_last=False in the DataLoader.\n","    # This makes sure that all data is used in the validation\n","    # phase, even if the last batch is smaller than the specified batch size.\n","    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, drop_last=False)  # Change drop_last to False\n","\n","    # Instantiate the model\n","    model = FlexibleAutoencoder(\n","        input_dim=data.shape[1],\n","        encoder_layers=encoder_layers,\n","        decoder_layers=decoder_layers,\n","        activations=activations,\n","        dropout_prob=dropout_prob,\n","        use_batchnorm=True  # Batch normalization always enabled\n","    ).to(device)\n","\n","    # Define optimizer\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","    num_epochs = 100\n","    train_losses, val_losses = [], []\n","\n","    #define what happens in each epoch\n","    for epoch in range(num_epochs):\n","        #fit model\n","        model.train()\n","        train_loss = 0\n","\n","        for batch in train_loader:\n","            batch = batch.to(device)\n","            optimizer.zero_grad()\n","            latent_representation, reconstructed = model(batch)\n","            loss = custom_loss_function(reconstructed, batch, latent_representation)\n","            #backprop based on the batch we pass in\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item()\n","        train_losses.append(train_loss / len(train_loader))\n","\n","        # Validation phase for this particular combo of HPs\n","        model.eval()\n","        val_loss = 0\n","        with torch.no_grad():\n","            for batch in val_loader:\n","                batch = batch.to(device)\n","                latent_representation, reconstructed = model(batch)\n","                loss = custom_loss_function(reconstructed, batch, latent_representation)\n","                val_loss += loss.item()\n","\n","        # Add a check to prevent division by zero if val_loader is empty.\n","        # This check calculates validation loss per batch to avoid division errors.\n","        val_losses.append(val_loss / len(val_loader) if len(val_loader) else 0) # Handle potential ZeroDivisionError\n","\n","    return val_losses[-1], train_losses, val_losses\n","\n","# Define hyperparameter space\n","space = [\n","    Integer(1, 5, name='num_encoder_layers'),\n","    Integer(1, 5, name='num_decoder_layers'),\n","    Integer(4, 128, name='num_neurons'),\n","    Categorical(['ReLU', 'Sigmoid', 'Tanh'], name='activation_fn'),\n","    Real(1e-4, 1e-2, prior='log-uniform', name='lr'),\n","    Integer(32, 128, name='batch_size'),\n","    Real(0.0, 0.5, name='dropout_prob'),  # Includes 0 for no dropout\n","    Real(2, 32, name='latent_dim')       # Optimize latent space dimension\n","]\n","\n","# Objective function for optimization\n","def objective(params):\n","    num_encoder_layers, num_decoder_layers, num_neurons, activation_fn, lr, batch_size, dropout_prob, latent_dim = params\n","    encoder_layers = [num_neurons] * num_encoder_layers\n","    decoder_layers = [num_neurons] * num_decoder_layers\n","    activations = [activation_fn] * (num_encoder_layers + num_decoder_layers)\n","\n","    val_loss, train_losses, val_losses = train_validate_autoencoder(\n","        encoder_layers, decoder_layers, activations, lr, batch_size, dropout_prob, latent_dim\n","    )\n","    return val_loss\n","\n","# Perform Bayesian optimization\n","result = gp_minimize(objective, space, n_calls=30, random_state=42)\n","\n","# Output the best hyperparameters\n","print(\"Best hyperparameters:\")\n","print(f\"Encoder layers: {result.x[0]}\")\n","print(f\"Decoder layers: {result.x[1]}\")\n","print(f\"Neurons per layer: {result.x[2]}\")\n","print(f\"Activation function: {result.x[3]}\")\n","print(f\"Learning rate: {result.x[4]}\")\n","print(f\"Batch size: {result.x[5]}\")\n","print(f\"Dropout probability: {result.x[6]}\")\n","print(f\"Latent dimension: {result.x[7]}\")\n","\n","\n","# Get the best parameters and re-train the model\n","best_params = result.x\n","num_encoder_layers, num_decoder_layers, num_neurons, activation_fn, lr, batch_size, dropout_prob, latent_dim = best_params\n","encoder_layers = [num_neurons] * num_encoder_layers\n","decoder_layers = [num_neurons] * num_decoder_layers\n","activations = [activation_fn] * (num_encoder_layers + num_decoder_layers)\n","\n","# Function to evaluate on the test set\n","def evaluate_on_test_set(encoder_layers, decoder_layers, activations, lr, batch_size, dropout_prob, latent_dim):\n","    # Prepare test dataloader\n","    batch_size = int(batch_size)\n","    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, drop_last=False)\n","\n","    # Instantiate the model\n","    model = FlexibleAutoencoder(\n","        input_dim=data.shape[1],\n","        encoder_layers=encoder_layers,\n","        decoder_layers=decoder_layers,\n","        activations=activations,\n","        dropout_prob=dropout_prob,\n","        use_batchnorm=True\n","    ).to(device)\n","\n","    # Define optimizer (not used for evaluation, but needed for model instantiation)\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","    model.eval()\n","    test_loss = 0\n","    with torch.no_grad():\n","        for batch in test_loader:\n","            batch = batch.to(device)\n","            latent_representation, reconstructed = model(batch)\n","            loss = custom_loss_function(reconstructed, batch, latent_representation)\n","            test_loss += loss.item()\n","\n","    avg_test_loss = test_loss / len(test_loader)\n","    return avg_test_loss\n","\n","# Evaluate on the test set with best parameters\n","final_test_loss = evaluate_on_test_set(encoder_layers, decoder_layers, activations, lr, batch_size, dropout_prob, latent_dim)\n","\n","# Print final test loss\n","print(\"\\nFinal Test Loss:\", final_test_loss)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vS5JD7Mb6ET7","executionInfo":{"status":"ok","timestamp":1733775348223,"user_tz":0,"elapsed":113715,"user":{"displayName":"Archie Goodman","userId":"05960694724077487952"}},"outputId":"f3cff972-dc33-4d97-f51b-2bffb8d80b53"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Cuda setup successful\n","Best hyperparameters:\n","Encoder layers: 2\n","Decoder layers: 4\n","Neurons per layer: 11\n","Activation function: Tanh\n","Learning rate: 0.007535384509295551\n","Batch size: 32\n","Dropout probability: 0.49610577964560887\n","Latent dimension: 20.5244452888315\n","\n","Final Test Loss: 0.6310520172119141\n"]}]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","# Auto push to github\n","\n","\n"],"metadata":{"id":"RB7Z-FfYBCVy"}},{"cell_type":"code","source":["import datetime\n","import os\n","\n","def commit_to_github(commit_msg):\n","  \"\"\"\n","  Funct to autopush to github\n","  \"\"\"\n","\n","  # Navigate to the repository directory\n","  %cd /content/drive/MyDrive/Colab_Notebooks/Deep_Learning_Practice\n","\n","  !git add .\n","\n","  with open('/content/drive/MyDrive/IAM/PAT.txt', 'r') as file:\n","        github_pat = file.read().strip()\n","  os.environ['GITHUB_PAT'] = github_pat\n","\n","  !git remote add origin \"https://github.com/archiegoodman2/machine_learning_practice\"\n","\n","  # Replace with your actual username and email\n","  USERNAME=\"archiegoodman2\"\n","  EMAIL=\"archiegoodman2011@gmail.com\"\n","\n","  # Set global username and email configuration\n","  !git config --global user.name \"$USERNAME\"\n","  !git config --global user.email \"$EMAIL\"\n","\n","  now = datetime.datetime.now()\n","  current_datetime = now.strftime(\"%Y-%m-%d %H:%M\")\n","\n","  # Set remote URL using the PAT from environment variable\n","  !git remote set-url origin https://{os.environ['GITHUB_PAT']}@github.com/archiegoodman2/machine_learning_practice.git\n","\n","  # Replace with your desired commit message\n","  COMMIT_MESSAGE = str(current_datetime) + \" \" + str(commit_msg)\n","\n","  # Commit the changes\n","  !git commit -m \"$COMMIT_MESSAGE\"\n","\n","  # Push to origin (force push if necessary)\n","  !git push -f origin master\n","\n","  return 1\n","\n","commit_to_github(\"added test set validation - to do, improve kfold validation, improve loss functions\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8vO8P6TOBG2R","outputId":"cd045860-247c-48db-fcce-22c53568e47b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab_Notebooks/Deep_Learning_Practice\n"]}]}]}