{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyP92W52w51bwZok3RfL0/ar"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["\n","\n","---\n","\n","# Intro - Autoencoder\n","\n","**Plan** - produce proof of concept autoencoder architecture.\n","\n","**Purpose**: dimensionality reduction for input data.\n","\n","**Hypothesis**: with automatic HP tuning an autoencoder can be used to reduce dimensionality of input data, whilst retaining adequate information to accurately reproduce input data.\n","\n","**Methodology**: Test on multiple datasets - first the Iris dataset, then Pima Indians, then finally credit card fraud dataset. Evaluate and assess model architecture and visualize latent space using PCA/UMAP etc."],"metadata":{"id":"eYdyevAw-Xto"}},{"cell_type":"markdown","source":["\n","# First dataset - iris dataset\n","---\n","## Data sourcing and processing\n"],"metadata":{"id":"wwPqhLkz_hhD"}},{"cell_type":"code","source":["\n","#import packages :\n","\n","import os\n","os.environ['NUMBAPRO_NVVM'] = '/usr/local/cuda/nvvm/lib64/libnvvm.so'\n","os.environ['NUMBAPRO_LIBDEVICE'] = '/usr/local/cuda/nvvm/libdevice/'\n","\n","from google.colab import drive\n","\n","try:\n","  import google.colab\n","  IN_COLAB = True\n","except:\n","  IN_COLAB = False\n","\n","if IN_COLAB:\n","  # Check if drive is mounted by looking for the mount point in the file system.\n","  # This is a more robust approach than relying on potentially internal variables.\n","  import os\n","  if not os.path.exists('/content/drive'):\n","    drive.mount('/content/drive')\n","\n","#basics\n","import os\n","from google.colab import drive\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","!pip install tqdm\n","from tqdm import tqdm\n","\n","#table one\n","!pip install tableone\n","from tableone import TableOne\n","\n","#torch\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","#sklearn\n","!pip install scikit-optimize\n","from skopt import gp_minimize\n","from skopt.space import Integer, Real, Categorical\n","\n","from imblearn.over_sampling import RandomOverSampler\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import StratifiedKFold, GridSearchCV\n","from sklearn.metrics import roc_auc_score\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import RepeatedStratifiedKFold\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","from torch.utils.data import DataLoader, random_split, Dataset\n","from sklearn.datasets import load_breast_cancer\n","from skopt import gp_minimize\n","from skopt.space import Integer, Real, Categorical\n","from skopt.utils import use_named_args\n","from sklearn.preprocessing import MinMaxScaler\n","\n","\n","\n","from imblearn.over_sampling import RandomOverSampler"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5MvDG0rB_st0","executionInfo":{"status":"ok","timestamp":1733423339559,"user_tz":0,"elapsed":6581,"user":{"displayName":"Archie Goodman","userId":"05960694724077487952"}},"outputId":"ba11a74b-e4b0-4863-984d-38a2da0075fa"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n","Requirement already satisfied: tableone in /usr/local/lib/python3.10/dist-packages (0.9.1)\n","Requirement already satisfied: jinja2>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from tableone) (3.1.4)\n","Requirement already satisfied: numpy>=1.19.1 in /usr/local/lib/python3.10/dist-packages (from tableone) (1.26.4)\n","Requirement already satisfied: openpyxl>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from tableone) (3.1.5)\n","Requirement already satisfied: pandas>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from tableone) (2.2.2)\n","Requirement already satisfied: scipy>=1.10.1 in /usr/local/lib/python3.10/dist-packages (from tableone) (1.13.1)\n","Requirement already satisfied: statsmodels>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from tableone) (0.14.4)\n","Requirement already satisfied: tabulate>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tableone) (0.9.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=3.1.4->tableone) (3.0.2)\n","Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl>=3.1.2->tableone) (2.0.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.3->tableone) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.3->tableone) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.3->tableone) (2024.2)\n","Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.14.1->tableone) (1.0.1)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.14.1->tableone) (24.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.3->tableone) (1.16.0)\n","Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.10/dist-packages (0.10.2)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.4.2)\n","Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (24.9.0)\n","Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.26.4)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.13.1)\n","Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.5.2)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (24.2)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.5.0)\n"]}]},{"cell_type":"code","source":["#import data and visualize\n","\n","# Load the breast cancer dataset\n","data = load_breast_cancer()\n","\n","# Convert to Pandas DataFrame\n","bc_df = pd.DataFrame(data.data, columns=data.feature_names)\n","\n","# Add the target column (malignant or benign)\n","bc_df['target'] = data.target\n","\n","bc_columns = bc_df.columns.tolist()\n","\n","print(f\"The dataset lenghth is {str(len(bc_df))}\")\n","print(f\"The number of columns is {str(len(bc_columns))}\")\n","print(f\"The column names are {str(bc_columns)} \\n\")\n","\n","table1 = TableOne(bc_df, columns=bc_columns, groupby= 'target', pval=True)\n","print(table1)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hAl3GsVr_yWX","executionInfo":{"status":"ok","timestamp":1733423012741,"user_tz":0,"elapsed":273,"user":{"displayName":"Archie Goodman","userId":"05960694724077487952"}},"outputId":"c2e190d5-6beb-408e-8b37-4b0526059f5e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["The dataset lenghth is 569\n","The number of columns is 31\n","The column names are ['mean radius', 'mean texture', 'mean perimeter', 'mean area', 'mean smoothness', 'mean compactness', 'mean concavity', 'mean concave points', 'mean symmetry', 'mean fractal dimension', 'radius error', 'texture error', 'perimeter error', 'area error', 'smoothness error', 'compactness error', 'concavity error', 'concave points error', 'symmetry error', 'fractal dimension error', 'worst radius', 'worst texture', 'worst perimeter', 'worst area', 'worst smoothness', 'worst compactness', 'worst concavity', 'worst concave points', 'worst symmetry', 'worst fractal dimension', 'target'] \n","\n","                                    Grouped by target                                                      \n","                                              Missing        Overall               0              1 P-Value\n","n                                                                569             212            357        \n","mean radius, mean (SD)                              0     14.1 (3.5)      17.5 (3.2)     12.1 (1.8)  <0.001\n","mean texture, mean (SD)                             0     19.3 (4.3)      21.6 (3.8)     17.9 (4.0)  <0.001\n","mean perimeter, mean (SD)                           0    92.0 (24.3)    115.4 (21.9)    78.1 (11.8)  <0.001\n","mean area, mean (SD)                                0  654.9 (351.9)   978.4 (367.9)  462.8 (134.3)  <0.001\n","mean smoothness, mean (SD)                          0      0.1 (0.0)       0.1 (0.0)      0.1 (0.0)  <0.001\n","mean compactness, mean (SD)                         0      0.1 (0.1)       0.1 (0.1)      0.1 (0.0)  <0.001\n","mean concavity, mean (SD)                           0      0.1 (0.1)       0.2 (0.1)      0.0 (0.0)  <0.001\n","mean concave points, mean (SD)                      0      0.0 (0.0)       0.1 (0.0)      0.0 (0.0)  <0.001\n","mean symmetry, mean (SD)                            0      0.2 (0.0)       0.2 (0.0)      0.2 (0.0)  <0.001\n","mean fractal dimension, mean (SD)                   0      0.1 (0.0)       0.1 (0.0)      0.1 (0.0)   0.767\n","radius error, mean (SD)                             0      0.4 (0.3)       0.6 (0.3)      0.3 (0.1)  <0.001\n","texture error, mean (SD)                            0      1.2 (0.6)       1.2 (0.5)      1.2 (0.6)   0.835\n","perimeter error, mean (SD)                          0      2.9 (2.0)       4.3 (2.6)      2.0 (0.8)  <0.001\n","area error, mean (SD)                               0    40.3 (45.5)     72.7 (61.4)     21.1 (8.8)  <0.001\n","smoothness error, mean (SD)                         0      0.0 (0.0)       0.0 (0.0)      0.0 (0.0)   0.105\n","compactness error, mean (SD)                        0      0.0 (0.0)       0.0 (0.0)      0.0 (0.0)  <0.001\n","concavity error, mean (SD)                          0      0.0 (0.0)       0.0 (0.0)      0.0 (0.0)  <0.001\n","concave points error, mean (SD)                     0      0.0 (0.0)       0.0 (0.0)      0.0 (0.0)  <0.001\n","symmetry error, mean (SD)                           0      0.0 (0.0)       0.0 (0.0)      0.0 (0.0)   0.887\n","fractal dimension error, mean (SD)                  0      0.0 (0.0)       0.0 (0.0)      0.0 (0.0)   0.042\n","worst radius, mean (SD)                             0     16.3 (4.8)      21.1 (4.3)     13.4 (2.0)  <0.001\n","worst texture, mean (SD)                            0     25.7 (6.1)      29.3 (5.4)     23.5 (5.5)  <0.001\n","worst perimeter, mean (SD)                          0   107.3 (33.6)    141.4 (29.5)    87.0 (13.5)  <0.001\n","worst area, mean (SD)                               0  880.6 (569.4)  1422.3 (598.0)  558.9 (163.6)  <0.001\n","worst smoothness, mean (SD)                         0      0.1 (0.0)       0.1 (0.0)      0.1 (0.0)  <0.001\n","worst compactness, mean (SD)                        0      0.3 (0.2)       0.4 (0.2)      0.2 (0.1)  <0.001\n","worst concavity, mean (SD)                          0      0.3 (0.2)       0.5 (0.2)      0.2 (0.1)  <0.001\n","worst concave points, mean (SD)                     0      0.1 (0.1)       0.2 (0.0)      0.1 (0.0)  <0.001\n","worst symmetry, mean (SD)                           0      0.3 (0.1)       0.3 (0.1)      0.3 (0.0)  <0.001\n","worst fractal dimension, mean (SD)                  0      0.1 (0.0)       0.1 (0.0)      0.1 (0.0)  <0.001\n","\n"]}]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","# Autoencoder (Breast cancer dataset)"],"metadata":{"id":"qURIYCUa6Eu9"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, random_split, Dataset\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import mean_squared_error\n","import numpy as np\n","from skopt import gp_minimize\n","from skopt.space import Integer, Real, Categorical\n","\n","# **Set device for GPU acceleration**\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# **Error warning if no GPU is detected**\n","if device.type != 'cuda':\n","    print(\"WARNING: GPU is not available. The model will run on the CPU, which might be slower.\")\n","else:\n","    print(\"Cuda setup successful\")\n","\n","# **Load and preprocess the Breast Cancer dataset**\n","breast_cancer = load_breast_cancer()\n","data = breast_cancer.data\n","\n","# **Define custom dataset class**\n","class BreastCancerDataset(Dataset):\n","    def __init__(self, data):\n","        self.data = torch.tensor(data, dtype=torch.float32)\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        return self.data[idx]\n","\n","# **Scale the data between 0 and 1**\n","scaler = MinMaxScaler()\n","data = scaler.fit_transform(data)\n","\n","# **Create PyTorch dataset and split into train/validation/test sets**\n","dataset = BreastCancerDataset(data)\n","train_size = int(0.7 * len(dataset))\n","val_size = int(0.2 * len(dataset))\n","test_size = len(dataset) - train_size - val_size\n","train_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size])\n","\n","# **Define flexible autoencoder class**\n","class FlexibleAutoencoder(nn.Module):\n","    def __init__(self, input_dim, encoder_layers, latent_dim, decoder_layers, activations, dropout_prob, use_batchnorm):\n","        \"\"\"\n","        A flexible autoencoder implementation that supports customizable encoder and decoder architectures,\n","        latent space dimensionality, activation functions, dropout, and batch normalization.\n","        \"\"\"\n","        super(FlexibleAutoencoder, self).__init__()\n","\n","        # **Build the encoder**\n","        encoder = []\n","        in_dim = input_dim\n","        for i in range(len(encoder_layers)):\n","            encoder.append(nn.Linear(in_dim, encoder_layers[i]))\n","            if use_batchnorm:\n","                encoder.append(nn.BatchNorm1d(encoder_layers[i]))\n","            encoder.append(getattr(nn, activations[i])())\n","            if dropout_prob > 0:\n","                encoder.append(nn.Dropout(dropout_prob))\n","            in_dim = encoder_layers[i]\n","\n","        # Add the latent space layer\n","        encoder.append(nn.Linear(in_dim, latent_dim))\n","        if use_batchnorm:\n","            encoder.append(nn.BatchNorm1d(latent_dim))\n","        encoder.append(getattr(nn, activations[len(encoder_layers)])())\n","        self.encoder = nn.Sequential(*encoder)\n","\n","        # **Build the decoder**\n","        decoder = []\n","        in_dim = latent_dim\n","        for i in range(len(decoder_layers)):\n","            out_dim = input_dim if i == len(decoder_layers) - 1 else decoder_layers[i]\n","            decoder.append(nn.Linear(in_dim, out_dim))\n","            if use_batchnorm and i != len(decoder_layers) - 1:\n","                decoder.append(nn.BatchNorm1d(out_dim))\n","            if i != len(decoder_layers) - 1:\n","                decoder.append(getattr(nn, activations[len(encoder_layers) + 1 + i])())\n","            if dropout_prob > 0 and i != len(decoder_layers) - 1:\n","                decoder.append(nn.Dropout(dropout_prob))\n","            in_dim = out_dim\n","        self.decoder = nn.Sequential(*decoder)\n","\n","    def forward(self, x):\n","        \"\"\"Defines the forward pass of the autoencoder.\"\"\"\n","        latent = self.encoder(x)\n","        reconstructed = self.decoder(latent)\n","        return reconstructed\n","\n","# **Training and validation function**\n","def train_validate_autoencoder(encoder_layers, latent_dim, decoder_layers, activations, lr, batch_size, dropout_prob, use_batchnorm):\n","    \"\"\"\n","    Train and validate the autoencoder using the given hyperparameters.\n","    \"\"\"\n","    batch_size = int(batch_size)\n","    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n","    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, drop_last=True)\n","\n","    # **Initialize the model**\n","    model = FlexibleAutoencoder(\n","        input_dim=data.shape[1],\n","        encoder_layers=encoder_layers,\n","        latent_dim=latent_dim,\n","        decoder_layers=decoder_layers,\n","        activations=activations,\n","        dropout_prob=dropout_prob,\n","        use_batchnorm=use_batchnorm\n","    ).to(device)\n","\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","    num_epochs = 100\n","    train_losses, val_losses = [], []\n","    for epoch in tqdm(range(num_epochs)):\n","        model.train()\n","        train_loss = 0\n","        for batch in train_loader:\n","            batch = batch.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(batch)\n","            loss = criterion(outputs, batch)\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item()\n","\n","        # Validation phase\n","        model.eval()\n","        val_loss = 0\n","        with torch.no_grad():\n","            for batch in val_loader:\n","                batch = batch.to(device)\n","                outputs = model(batch)\n","                loss = criterion(outputs, batch)\n","                val_loss += loss.item()\n","        train_losses.append(train_loss / len(train_loader))\n","        val_losses.append(val_loss / len(val_loader))\n","\n","    return val_losses[-1]  # Return the final validation loss\n","\n","# **Define the search space for hyperparameter optimization**\n","space = [\n","    Integer(1, 5, name='num_encoder_layers'),\n","    Integer(1, 5, name='num_decoder_layers'),\n","    Integer(4, 128, name='num_neurons'),\n","    Integer(2, 64, name='latent_dim'),  # Latent space dimensionality\n","    Categorical(['ReLU', 'Sigmoid', 'Tanh'], name='activation_fn'),\n","    Real(1e-4, 1e-2, prior='log-uniform', name='lr'),\n","    Integer(2, 64, name='batch_size'),\n","    Categorical([True, False], name='use_dropout'),\n","    Real(0.1, 0.5, name='dropout_prob'),\n","    Categorical([True, False], name='use_batchnorm')\n","]\n","\n","# **Objective function for Bayesian optimization**\n","def objective(params):\n","    num_encoder_layers, num_decoder_layers, num_neurons, latent_dim, activation_fn, lr, batch_size, use_dropout, dropout_prob, use_batchnorm = params\n","\n","    # Construct activation function list\n","    activations = [activation_fn] * (num_encoder_layers + num_decoder_layers + 1)\n","\n","    # Define encoder and decoder layers\n","    encoder_layers = [num_neurons] * num_encoder_layers\n","    decoder_layers = [num_neurons] * num_decoder_layers\n","\n","    # Train and validate the autoencoder\n","    final_val_loss = train_validate_autoencoder(encoder_layers, latent_dim, decoder_layers, activations, lr, batch_size, dropout_prob, use_batchnorm)\n","    return final_val_loss\n","\n","# **Run Bayesian optimization**\n","result = gp_minimize(objective, space, n_calls=30, random_state=42)\n","\n","# **Output the best parameters**\n","print(\"Best hyperparameters:\")\n","print(f\"Number of encoder layers: {result.x[0]}\")\n","print(f\"Number of decoder layers: {result.x[1]}\")\n","print(f\"Number of neurons per layer: {result.x[2]}\")\n","print(f\"Latent space dimensionality: {result.x[3]}\")\n","print(f\"Activation function: {result.x[4]}\")\n","print(f\"Learning rate: {result.x[5]}\")\n","print(f\"Batch size: {result.x[6]}\")\n","print(f\"Use dropout: {result.x[7]}\")\n","print(f\"Dropout probability: {result.x[8]}\")\n","print(f\"Use batch normalization: {result.x[9]}\")\n","print(f\"Final validation loss: {result.fun}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vS5JD7Mb6ET7","executionInfo":{"status":"ok","timestamp":1733423665212,"user_tz":0,"elapsed":322730,"user":{"displayName":"Archie Goodman","userId":"05960694724077487952"}},"outputId":"c98d9789-2932-4e29-c393-77dd56a9ccfc"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Cuda setup successful\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 100/100 [00:04<00:00, 23.56it/s]\n","100%|██████████| 100/100 [00:02<00:00, 36.22it/s]\n","100%|██████████| 100/100 [00:03<00:00, 30.35it/s]\n","100%|██████████| 100/100 [00:36<00:00,  2.74it/s]\n","100%|██████████| 100/100 [00:05<00:00, 16.70it/s]\n","100%|██████████| 100/100 [00:03<00:00, 33.32it/s]\n","100%|██████████| 100/100 [00:07<00:00, 12.63it/s]\n","100%|██████████| 100/100 [00:02<00:00, 44.41it/s]\n","100%|██████████| 100/100 [00:04<00:00, 23.27it/s]\n","100%|██████████| 100/100 [00:04<00:00, 24.25it/s]\n","100%|██████████| 100/100 [00:03<00:00, 30.45it/s]\n","100%|██████████| 100/100 [00:04<00:00, 23.29it/s]\n","100%|██████████| 100/100 [00:02<00:00, 38.87it/s]\n","100%|██████████| 100/100 [00:02<00:00, 44.25it/s]\n","100%|██████████| 100/100 [00:03<00:00, 25.91it/s]\n","100%|██████████| 100/100 [01:36<00:00,  1.03it/s]\n","100%|██████████| 100/100 [00:01<00:00, 51.12it/s]\n","100%|██████████| 100/100 [00:04<00:00, 21.92it/s]\n","100%|██████████| 100/100 [00:08<00:00, 11.69it/s]\n","100%|██████████| 100/100 [00:13<00:00,  7.35it/s]\n","100%|██████████| 100/100 [00:02<00:00, 41.49it/s]\n","100%|██████████| 100/100 [00:01<00:00, 54.46it/s]\n","100%|██████████| 100/100 [00:19<00:00,  5.16it/s]\n","100%|██████████| 100/100 [00:01<00:00, 61.59it/s]\n","100%|██████████| 100/100 [00:02<00:00, 38.21it/s]\n","100%|██████████| 100/100 [00:40<00:00,  2.47it/s]\n","100%|██████████| 100/100 [00:01<00:00, 60.29it/s]\n","100%|██████████| 100/100 [00:04<00:00, 23.68it/s]\n","100%|██████████| 100/100 [00:16<00:00,  6.14it/s]\n","100%|██████████| 100/100 [00:01<00:00, 53.46it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Best hyperparameters:\n","Number of encoder layers: 1\n","Number of decoder layers: 1\n","Number of neurons per layer: 85\n","Latent space dimensionality: 63\n","Activation function: ReLU\n","Learning rate: 0.0004582550799359247\n","Batch size: 14\n","Use dropout: False\n","Dropout probability: 0.23045489658159404\n","Use batch normalization: False\n","Final validation loss: 0.0013084623569739051\n"]}]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","# Auto push to github\n","\n","\n"],"metadata":{"id":"RB7Z-FfYBCVy"}},{"cell_type":"code","source":["import datetime\n","import os\n","\n","def commit_to_github(commit_msg):\n","  \"\"\"\n","  Funct to autopush to github\n","  \"\"\"\n","\n","  # Navigate to the repository directory\n","  %cd /content/drive/MyDrive/Colab_Notebooks/Deep_Learning_Practice\n","\n","  !git add .\n","\n","  with open('/content/drive/MyDrive/IAM/PAT.txt', 'r') as file:\n","        github_pat = file.read().strip()\n","  os.environ['GITHUB_PAT'] = github_pat\n","\n","  !git remote add origin \"https://github.com/archiegoodman2/machine_learning_practice\"\n","\n","  # Replace with your actual username and email\n","  USERNAME=\"archiegoodman2\"\n","  EMAIL=\"archiegoodman2011@gmail.com\"\n","\n","  # Set global username and email configuration\n","  !git config --global user.name \"$USERNAME\"\n","  !git config --global user.email \"$EMAIL\"\n","\n","  now = datetime.datetime.now()\n","  current_datetime = now.strftime(\"%Y-%m-%d %H:%M\")\n","\n","  # Set remote URL using the PAT from environment variable\n","  !git remote set-url origin https://{os.environ['GITHUB_PAT']}@github.com/archiegoodman2/machine_learning_practice.git\n","\n","  # Replace with your desired commit message\n","  COMMIT_MESSAGE = str(current_datetime) + \" \" + str(commit_msg)\n","\n","  # Commit the changes\n","  !git commit -m \"$COMMIT_MESSAGE\"\n","\n","  # Push to origin (force push if necessary)\n","  !git push -f origin master\n","\n","  return 1\n","\n","commit_to_github(\"added latent space dimension as a HP to be tuned\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8vO8P6TOBG2R","outputId":"51acdd1e-2765-4f34-8eab-c5c832683d2a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab_Notebooks/Deep_Learning_Practice\n"]}]}]}