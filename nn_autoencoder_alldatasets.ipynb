{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyPzthf2vaRuIragvOOzd0VX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["\n","\n","---\n","\n","# Intro - Autoencoder\n","\n","**Plan** - produce proof of concept autoencoder architecture.\n","\n","**Purpose**: dimensionality reduction for input data.\n","\n","**Hypothesis**: with automatic HP tuning an autoencoder can be used to reduce dimensionality of input data, whilst retaining adequate information to accurately reproduce input data.\n","\n","**Methodology**: Test on multiple datasets - first the Iris dataset, then Pima Indians, then finally credit card fraud dataset. Evaluate and assess model architecture and visualize latent space using PCA/UMAP etc."],"metadata":{"id":"eYdyevAw-Xto"}},{"cell_type":"markdown","source":["\n","# First dataset - iris dataset\n","---\n","## Data sourcing and processing\n"],"metadata":{"id":"wwPqhLkz_hhD"}},{"cell_type":"code","source":["\n","#import packages :\n","\n","import os\n","os.environ['NUMBAPRO_NVVM'] = '/usr/local/cuda/nvvm/lib64/libnvvm.so'\n","os.environ['NUMBAPRO_LIBDEVICE'] = '/usr/local/cuda/nvvm/libdevice/'\n","\n","from google.colab import drive\n","\n","try:\n","  import google.colab\n","  IN_COLAB = True\n","except:\n","  IN_COLAB = False\n","\n","if IN_COLAB:\n","  # Check if drive is mounted by looking for the mount point in the file system.\n","  # This is a more robust approach than relying on potentially internal variables.\n","  import os\n","  if not os.path.exists('/content/drive'):\n","    drive.mount('/content/drive')\n","\n","#basics\n","import os\n","from google.colab import drive\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","#table one\n","!pip install tableone\n","from tableone import TableOne\n","\n","#torch\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","#sklearn\n","!pip install scikit-optimize\n","from skopt import gp_minimize\n","from skopt.space import Integer, Real, Categorical\n","\n","from imblearn.over_sampling import RandomOverSampler\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import StratifiedKFold, GridSearchCV\n","from sklearn.metrics import roc_auc_score\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import RepeatedStratifiedKFold\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","from torch.utils.data import DataLoader, random_split, Dataset\n","from sklearn.datasets import load_breast_cancer\n","from skopt import gp_minimize\n","from skopt.space import Integer, Real, Categorical\n","from skopt.utils import use_named_args\n","from sklearn.preprocessing import MinMaxScaler\n","\n","\n","\n","from imblearn.over_sampling import RandomOverSampler"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5MvDG0rB_st0","executionInfo":{"status":"ok","timestamp":1733344848310,"user_tz":0,"elapsed":5705,"user":{"displayName":"Archie Goodman","userId":"05960694724077487952"}},"outputId":"b017072a-c9d1-4331-9361-727264c4735b"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tableone in /usr/local/lib/python3.10/dist-packages (0.9.1)\n","Requirement already satisfied: jinja2>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from tableone) (3.1.4)\n","Requirement already satisfied: numpy>=1.19.1 in /usr/local/lib/python3.10/dist-packages (from tableone) (1.26.4)\n","Requirement already satisfied: openpyxl>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from tableone) (3.1.5)\n","Requirement already satisfied: pandas>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from tableone) (2.2.2)\n","Requirement already satisfied: scipy>=1.10.1 in /usr/local/lib/python3.10/dist-packages (from tableone) (1.13.1)\n","Requirement already satisfied: statsmodels>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from tableone) (0.14.4)\n","Requirement already satisfied: tabulate>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tableone) (0.9.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=3.1.4->tableone) (3.0.2)\n","Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl>=3.1.2->tableone) (2.0.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.3->tableone) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.3->tableone) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.3->tableone) (2024.2)\n","Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.14.1->tableone) (1.0.1)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.14.1->tableone) (24.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.3->tableone) (1.16.0)\n","Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.10/dist-packages (0.10.2)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.4.2)\n","Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (24.9.0)\n","Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.26.4)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.13.1)\n","Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.5.2)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (24.2)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.5.0)\n"]}]},{"cell_type":"code","source":["#import data and visualize\n","\n","# Load the breast cancer dataset\n","data = load_breast_cancer()\n","\n","# Convert to Pandas DataFrame\n","bc_df = pd.DataFrame(data.data, columns=data.feature_names)\n","\n","# Add the target column (malignant or benign)\n","bc_df['target'] = data.target\n","\n","bc_columns = bc_df.columns.tolist()\n","\n","print(f\"The dataset lenghth is {str(len(bc_df))}\")\n","print(f\"The number of columns is {str(len(bc_columns))}\")\n","print(f\"The column names are {str(bc_columns)} \\n\")\n","\n","table1 = TableOne(bc_df, columns=bc_columns, groupby= 'target', pval=True)\n","print(table1)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hAl3GsVr_yWX","executionInfo":{"status":"ok","timestamp":1733342561606,"user_tz":0,"elapsed":8,"user":{"displayName":"Archie Goodman","userId":"05960694724077487952"}},"outputId":"a317f48a-d26b-4485-8230-62774255ff2d"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["The dataset lenghth is 569\n","The number of columns is 31\n","The column names are ['mean radius', 'mean texture', 'mean perimeter', 'mean area', 'mean smoothness', 'mean compactness', 'mean concavity', 'mean concave points', 'mean symmetry', 'mean fractal dimension', 'radius error', 'texture error', 'perimeter error', 'area error', 'smoothness error', 'compactness error', 'concavity error', 'concave points error', 'symmetry error', 'fractal dimension error', 'worst radius', 'worst texture', 'worst perimeter', 'worst area', 'worst smoothness', 'worst compactness', 'worst concavity', 'worst concave points', 'worst symmetry', 'worst fractal dimension', 'target'] \n","\n","                                    Grouped by target                                                      \n","                                              Missing        Overall               0              1 P-Value\n","n                                                                569             212            357        \n","mean radius, mean (SD)                              0     14.1 (3.5)      17.5 (3.2)     12.1 (1.8)  <0.001\n","mean texture, mean (SD)                             0     19.3 (4.3)      21.6 (3.8)     17.9 (4.0)  <0.001\n","mean perimeter, mean (SD)                           0    92.0 (24.3)    115.4 (21.9)    78.1 (11.8)  <0.001\n","mean area, mean (SD)                                0  654.9 (351.9)   978.4 (367.9)  462.8 (134.3)  <0.001\n","mean smoothness, mean (SD)                          0      0.1 (0.0)       0.1 (0.0)      0.1 (0.0)  <0.001\n","mean compactness, mean (SD)                         0      0.1 (0.1)       0.1 (0.1)      0.1 (0.0)  <0.001\n","mean concavity, mean (SD)                           0      0.1 (0.1)       0.2 (0.1)      0.0 (0.0)  <0.001\n","mean concave points, mean (SD)                      0      0.0 (0.0)       0.1 (0.0)      0.0 (0.0)  <0.001\n","mean symmetry, mean (SD)                            0      0.2 (0.0)       0.2 (0.0)      0.2 (0.0)  <0.001\n","mean fractal dimension, mean (SD)                   0      0.1 (0.0)       0.1 (0.0)      0.1 (0.0)   0.767\n","radius error, mean (SD)                             0      0.4 (0.3)       0.6 (0.3)      0.3 (0.1)  <0.001\n","texture error, mean (SD)                            0      1.2 (0.6)       1.2 (0.5)      1.2 (0.6)   0.835\n","perimeter error, mean (SD)                          0      2.9 (2.0)       4.3 (2.6)      2.0 (0.8)  <0.001\n","area error, mean (SD)                               0    40.3 (45.5)     72.7 (61.4)     21.1 (8.8)  <0.001\n","smoothness error, mean (SD)                         0      0.0 (0.0)       0.0 (0.0)      0.0 (0.0)   0.105\n","compactness error, mean (SD)                        0      0.0 (0.0)       0.0 (0.0)      0.0 (0.0)  <0.001\n","concavity error, mean (SD)                          0      0.0 (0.0)       0.0 (0.0)      0.0 (0.0)  <0.001\n","concave points error, mean (SD)                     0      0.0 (0.0)       0.0 (0.0)      0.0 (0.0)  <0.001\n","symmetry error, mean (SD)                           0      0.0 (0.0)       0.0 (0.0)      0.0 (0.0)   0.887\n","fractal dimension error, mean (SD)                  0      0.0 (0.0)       0.0 (0.0)      0.0 (0.0)   0.042\n","worst radius, mean (SD)                             0     16.3 (4.8)      21.1 (4.3)     13.4 (2.0)  <0.001\n","worst texture, mean (SD)                            0     25.7 (6.1)      29.3 (5.4)     23.5 (5.5)  <0.001\n","worst perimeter, mean (SD)                          0   107.3 (33.6)    141.4 (29.5)    87.0 (13.5)  <0.001\n","worst area, mean (SD)                               0  880.6 (569.4)  1422.3 (598.0)  558.9 (163.6)  <0.001\n","worst smoothness, mean (SD)                         0      0.1 (0.0)       0.1 (0.0)      0.1 (0.0)  <0.001\n","worst compactness, mean (SD)                        0      0.3 (0.2)       0.4 (0.2)      0.2 (0.1)  <0.001\n","worst concavity, mean (SD)                          0      0.3 (0.2)       0.5 (0.2)      0.2 (0.1)  <0.001\n","worst concave points, mean (SD)                     0      0.1 (0.1)       0.2 (0.0)      0.1 (0.0)  <0.001\n","worst symmetry, mean (SD)                           0      0.3 (0.1)       0.3 (0.1)      0.3 (0.0)  <0.001\n","worst fractal dimension, mean (SD)                  0      0.1 (0.0)       0.1 (0.0)      0.1 (0.0)  <0.001\n","\n"]}]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","# Autoencoder (Breast cancer dataset)"],"metadata":{"id":"qURIYCUa6Eu9"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, random_split, Dataset\n","from sklearn.datasets import load_iris\n","from sklearn.metrics import mean_squared_error\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from skopt import gp_minimize\n","from skopt.space import Integer, Real, Categorical\n","\n","# **Set device for GPU acceleration**\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# **Error warning if no GPU is detected**\n","if device.type != 'cuda':\n","    print(\"WARNING: GPU is not available. The model will run on the CPU, which might be slower.\")\n","else:\n","    print(\"Cuda setup successful\")\n","\n","# Load and preprocess the Breast Cancer dataset\n","breast_cancer = load_breast_cancer()\n","data = breast_cancer.data\n","\n","# Convert to PyTorch Dataset\n","class BreastCancerDataset(Dataset): # Rename the class\n","    def __init__(self, data):\n","        self.data = torch.tensor(data, dtype=torch.float32)\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        return self.data[idx]\n","\n","\n","# Create a scaler object\n","scaler = MinMaxScaler()\n","\n","# Fit the scaler to your data and transform it\n","data = scaler.fit_transform(data)\n","\n","dataset = BreastCancerDataset(data) # Use the new dataset class\n","\n","# Split into training, validation, and test sets\n","train_size = int(0.7 * len(dataset))\n","val_size = int(0.2 * len(dataset))\n","test_size = len(dataset) - train_size - val_size\n","train_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size])\n","\n","# Flexible Autoencoder with Dropout and Batch Normalization\n","\n","# Flexible Autoencoder with Dropout and Batch Normalization\n","class FlexibleAutoencoder(nn.Module):\n","    def __init__(self, input_dim, encoder_layers, decoder_layers, activations, dropout_prob, use_batchnorm):\n","        super(FlexibleAutoencoder, self).__init__()\n","        # Build the encoder\n","        encoder = []\n","        in_dim = input_dim\n","        for i in range(len(encoder_layers)):\n","            encoder.append(nn.Linear(in_dim, encoder_layers[i]))\n","            if use_batchnorm:\n","                encoder.append(nn.BatchNorm1d(encoder_layers[i]))\n","            encoder.append(getattr(nn, activations[i])())\n","            # Apply dropout if dropout_prob > 0\n","            if dropout_prob > 0:\n","                encoder.append(nn.Dropout(dropout_prob))\n","            in_dim = encoder_layers[i]\n","        self.encoder = nn.Sequential(*encoder)\n","\n","        # Build the decoder\n","        decoder = []\n","        in_dim = encoder_layers[-1]\n","        for i in range(len(decoder_layers)):\n","            # Output layer of decoder should have 'input_dim' units\n","            out_dim = input_dim if i == len(decoder_layers) - 1 else decoder_layers[i]\n","            decoder.append(nn.Linear(in_dim, out_dim))\n","            if use_batchnorm and i != len(decoder_layers) - 1:  # No batchnorm on the output layer\n","                decoder.append(nn.BatchNorm1d(out_dim))\n","            decoder.append(getattr(nn, activations[len(encoder_layers) + i])())\n","            # Apply dropout if dropout_prob > 0 and not the output layer\n","            if dropout_prob > 0 and i != len(decoder_layers) - 1:\n","                decoder.append(nn.Dropout(dropout_prob))\n","            in_dim = decoder_layers[i]\n","        self.decoder = nn.Sequential(*decoder)\n","\n","    def forward(self, x):\n","        latent = self.encoder(x)\n","        reconstructed = self.decoder(latent)\n","        return reconstructed\n","\n","# Training and validation function\n","def train_validate_autoencoder(encoder_layers, decoder_layers, activations, lr, batch_size, dropout_prob, use_batchnorm):\n","    batch_size = int(batch_size)\n","    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n","    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, drop_last=True)\n","\n","    model = FlexibleAutoencoder(\n","        input_dim=data.shape[1],\n","        encoder_layers=encoder_layers,\n","        decoder_layers=decoder_layers,\n","        activations=activations,\n","        dropout_prob=dropout_prob,\n","        use_batchnorm=use_batchnorm\n","    ).to(device)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","    num_epochs = 100\n","    train_losses, val_losses = [], []\n","    for epoch in range(num_epochs):\n","        model.train()\n","        train_loss = 0\n","        for batch in train_loader:\n","            batch = batch.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(batch)\n","            loss = criterion(outputs, batch)\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item()\n","\n","        # Check for ZeroDivisionError in training loss:\n","        if len(train_loader) > 0:\n","            train_losses.append(train_loss / len(train_loader))\n","        else:\n","            print(\"Warning: No batches in train_loader. Check your dataset and batch size.\")\n","            train_losses.append(1e10)  # Returning a very large loss to the Optimizer\n","\n","        model.eval()\n","        val_loss = 0\n","        with torch.no_grad():\n","            for batch in val_loader:\n","                batch = batch.to(device)\n","                outputs = model(batch)\n","                loss = criterion(outputs, batch)\n","                val_loss += loss.item()\n","\n","        # Check for ZeroDivisionError in validation loss:\n","        if len(val_loader) > 0:\n","            val_losses.append(val_loss / len(val_loader))\n","        else:\n","            print(\"Warning: No batches in val_loader. Check your dataset and batch size.\")\n","            return 1e10, train_losses, val_losses  # Return large loss for empty validation set\n","\n","    return val_losses[-1], train_losses, val_losses\n","\n","# Define the search space for Bayesian Optimization\n","space = [\n","    Integer(1, 6, name='num_encoder_layers'),  # Increased range\n","    Integer(1, 6, name='num_decoder_layers'),  # Increased range\n","    Integer(4, 256, name='num_neurons'),     # Increased range\n","    Categorical(['ReLU', 'Sigmoid', 'Tanh', 'LeakyReLU'], name='activation_fn'), # Added LeakyReLU\n","    Real(1e-6, 1e-2, prior='log-uniform', name='lr'),  # Wider range\n","    Integer(2, 128, name='batch_size'),       # Wider range\n","    Real(0.0, 0.5, name='dropout_prob'), #dropout probability\n","    Categorical([True, False], name='use_batchnorm')\n","]\n","\n","# Objective function for Bayesian Optimization\n","@use_named_args(space)  # Use named arguments for clarity\n","def objective(**params):\n","    # Extract hyperparameters\n","    num_encoder_layers = params['num_encoder_layers']\n","    num_decoder_layers = params['num_decoder_layers']\n","    num_neurons = params['num_neurons']\n","    activation_fn = params['activation_fn']\n","    lr = params['lr']\n","    batch_size = params['batch_size']\n","    dropout_prob = params['dropout_prob']  # Access dropout_prob directly\n","    use_batchnorm = params['use_batchnorm']\n","\n","    batch_size = int(batch_size)\n","    encoder_layers = [num_neurons] * num_encoder_layers\n","    decoder_layers = [num_neurons] * num_decoder_layers\n","    activations = [activation_fn] * (num_encoder_layers + num_decoder_layers)\n","\n","    # Call train_validate_autoencoder without use_dropout\n","    val_loss, _, _ = train_validate_autoencoder(\n","        encoder_layers=encoder_layers,\n","        decoder_layers=decoder_layers,\n","        activations=activations,\n","        lr=lr,\n","        batch_size=batch_size,\n","        dropout_prob=dropout_prob,  # Pass dropout_prob directly\n","        use_batchnorm=use_batchnorm\n","    )\n","    return val_loss\n","\n","# basic Bayesian Optimization\n","result = gp_minimize(objective, space, n_calls=30, random_state=42)\n","\n","# Extract best hyperparameters\n","best_hyperparams = result.x\n","print(f'''\n","Best Hyperparameters:\n","[num encoder layers, num decoder layers, num neurons, activation function, learning rate, batch size, amount of dropout, batchnorm bool]\n","{best_hyperparams}\n","''')\n","\n","# Train the best model and plot losses\n","best_encoder_layers = [best_hyperparams[2]] * best_hyperparams[0]\n","best_decoder_layers = [best_hyperparams[2]] * best_hyperparams[1]\n","best_activations = [best_hyperparams[3]] * (best_hyperparams[0] + best_hyperparams[1])\n","\n","#call autoencoder\n","_, train_losses, val_losses = train_validate_autoencoder(\n","    encoder_layers=best_encoder_layers,\n","    decoder_layers=best_decoder_layers,\n","    activations=best_activations,\n","    lr=best_hyperparams[4],\n","    batch_size=best_hyperparams[5],\n","    dropout_prob=best_hyperparams[6], # Pass dropout_prob directly\n","    use_batchnorm=best_hyperparams[7] # Use index 7 for use_batchnorm\n",")\n","\n","# Plot training and validation losses\n","plt.figure(figsize=(10, 6))\n","plt.plot(train_losses, label='Training Loss')\n","plt.plot(val_losses, label='Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.title('Training and Validation Loss')\n","plt.legend()\n","plt.show()\n","\n","# Evaluate on test set\n","test_loader = DataLoader(test_set, batch_size=min(int(best_hyperparams[5]), len(test_set)), shuffle=False,drop_last=True) # Cast best_hyperparams[5] to int\n","\n","model = FlexibleAutoencoder(\n","    input_dim=data.shape[1],\n","    encoder_layers=best_encoder_layers,\n","    decoder_layers=best_decoder_layers,\n","    activations=best_activations,\n","    dropout_prob=best_hyperparams[6],  # Pass dropout_prob directly\n","    use_batchnorm=best_hyperparams[7] # Use index 7 for use_batchnorm\n",").to(device)\n","\n","\n","model.eval()\n","test_loss = 0\n","true_values, predictions = [], []\n","with torch.no_grad():\n","    for batch in test_loader:\n","        batch = batch.to(device)\n","        outputs = model(batch)\n","        loss = nn.MSELoss()(outputs, batch)\n","        test_loss += loss.item()\n","        true_values.append(batch.cpu().numpy())\n","        predictions.append(outputs.cpu().numpy())\n","\n","test_loss /= len(test_loader)\n","print(f\"Test Loss: {test_loss}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vS5JD7Mb6ET7","outputId":"a512f517-8abb-4a91-f4a5-1aa1e45c107e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cuda setup successful\n","Warning: No batches in val_loader. Check your dataset and batch size.\n","Warning: No batches in val_loader. Check your dataset and batch size.\n"]}]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","# Auto push to github\n","\n","\n"],"metadata":{"id":"RB7Z-FfYBCVy"}},{"cell_type":"code","source":["import datetime\n","import os\n","\n","def commit_to_github(commit_msg):\n","  \"\"\"\n","  Funct to autopush to github\n","  \"\"\"\n","\n","  # Navigate to the repository directory\n","  %cd /content/drive/MyDrive/Colab_Notebooks/Deep_Learning_Practice\n","\n","  !git add .\n","\n","  with open('/content/drive/MyDrive/IAM/PAT.txt', 'r') as file:\n","        github_pat = file.read().strip()\n","  os.environ['GITHUB_PAT'] = github_pat\n","\n","  !git remote add origin \"https://github.com/archiegoodman2/machine_learning_practice\"\n","\n","  # Replace with your actual username and email\n","  USERNAME=\"archiegoodman2\"\n","  EMAIL=\"archiegoodman2011@gmail.com\"\n","\n","  # Set global username and email configuration\n","  !git config --global user.name \"$USERNAME\"\n","  !git config --global user.email \"$EMAIL\"\n","\n","  now = datetime.datetime.now()\n","  current_datetime = now.strftime(\"%Y-%m-%d %H:%M\")\n","\n","  # Set remote URL using the PAT from environment variable\n","  !git remote set-url origin https://{os.environ['GITHUB_PAT']}@github.com/archiegoodman2/machine_learning_practice.git\n","\n","  # Replace with your desired commit message\n","  COMMIT_MESSAGE = str(current_datetime) + \" \" + str(commit_msg)\n","\n","  # Commit the changes\n","  !git commit -m \"$COMMIT_MESSAGE\"\n","\n","  # Push to origin (force push if necessary)\n","  !git push -f origin master\n","\n","  return 1\n","\n","commit_to_github(\"fixed infinity error\")\n"],"metadata":{"id":"8vO8P6TOBG2R"},"execution_count":null,"outputs":[]}]}