{"cells":[{"cell_type":"markdown","metadata":{"id":"eYdyevAw-Xto"},"source":["\n","\n","---\n","\n","# Intro - Autoencoder\n","\n","**Plan** - Produce proof of concept autoencoder architecture.\n","\n","**Purpose**: Produce a robust, flexible and stable autoencoder architecture that can be used for dimensionality reduction on input data at work. The overall purpose is to be able to use this to encode data that has thousands of features into a latent space representation with fewer (to reduce time complexity of modelling) whilst preserving predictive performance.\n","\n","**Hypothesis**: with automatic HP tuning an autoencoder can be used to reduce dimensionality of input data, whilst retaining adequate information to accurately reproduce input data.\n","\n","**Methodology**: Test on multiple datasets - first the Iris dataset, then Pima Indians, then finally credit card fraud dataset. Evaluate and assess model architecture and visualize latent space using PCA/UMAP etc. See below for details on evaluation methods."]},{"cell_type":"markdown","metadata":{"id":"wwPqhLkz_hhD"},"source":["\n","# First dataset - iris dataset\n","---\n","## Data sourcing and processing\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":65198,"status":"ok","timestamp":1734097089713,"user":{"displayName":"Archie Goodman","userId":"05960694724077487952"},"user_tz":0},"id":"5MvDG0rB_st0"},"outputs":[{"ename":"ValueError","evalue":"mount failed","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-2-45614c04ba41\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 15\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 20\u001b[0;31m     \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#basics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         )\n\u001b[0;32m--\u003e 277\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: mount failed"]}],"source":["\n","#import packages :\n","\n","import os\n","os.environ['NUMBAPRO_NVVM'] = '/usr/local/cuda/nvvm/lib64/libnvvm.so'\n","os.environ['NUMBAPRO_LIBDEVICE'] = '/usr/local/cuda/nvvm/libdevice/'\n","\n","from google.colab import drive\n","\n","try:\n","  import google.colab\n","  IN_COLAB = True\n","except:\n","  IN_COLAB = False\n","\n","if IN_COLAB:\n","  # Check if drive is mounted by looking for the mount point in the file system.\n","  # This is a more robust approach than relying on potentially internal variables.\n","  import os\n","  if not os.path.exists('/content/drive'):\n","    drive.mount('/content/drive')\n","\n","#basics\n","import os\n","from google.colab import drive\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from tqdm import tqdm\n","!pip install umap-learn\n","!pip install optuna\n","from umap import UMAP\n","import optuna\n","\n","\n","#table one\n","!pip install tableone\n","from tableone import TableOne\n","\n","#torch\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, TensorDataset\n","import torch.optim as optim  # Add this import\n","from torch.utils.data import Subset\n","\n","\n","#sklearn\n","!pip install scikit-optimize\n","from skopt import gp_minimize\n","from skopt.space import Integer, Real, Categorical\n","\n","from imblearn.over_sampling import RandomOverSampler\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import StratifiedKFold, GridSearchCV\n","from sklearn.metrics import roc_auc_score\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import RepeatedStratifiedKFold\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","from torch.utils.data import DataLoader, random_split, Dataset\n","!pip install skopt\n","from skopt import gp_minimize\n","from skopt.space import Integer, Real, Categorical\n","from skopt.utils import use_named_args\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.datasets import load_iris  # Import the Iris dataset\n","from sklearn.model_selection import KFold\n","\n","\n","\n","\n","\n","from imblearn.over_sampling import RandomOverSampler"]},{"cell_type":"markdown","metadata":{"id":"qURIYCUa6Eu9"},"source":["\n","\n","---\n","\n","# Autoencoder (Iris dataset)"]},{"cell_type":"markdown","metadata":{"id":"sOqCbTSbdzIH"},"source":["\n","1.   First we import our data and perform a dimensionality reduction (using UMAP) to see how separable the classes are. We will do the same afterwards on our latent space to check it.\n","\n","2.   We will then get a Table One of the input data, and later of the reconstructed output data, to compare.\n","\n","3. We will check the distribution/separation of our latent space by plotting it in 2 dimensions - therefore we will fix our latent space dimensionality HP at 2.\n","\n","4. Finally as a final evaluation method we will do some basic predictive modelling on the input space vs the reconstructed data, and finally on the latent space representation.\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":862},"executionInfo":{"elapsed":9294,"status":"ok","timestamp":1734097099000,"user":{"displayName":"Archie Goodman","userId":"05960694724077487952"},"user_tz":0},"id":"DJ4L88aQdZWr"},"outputs":[],"source":["# **Set device for GPU acceleration**\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","if device.type != 'cuda':\n","    print(\"WARNING: GPU is not available. The model will run on the CPU, which might be slower.\")\n","else:\n","    print(\"Cuda setup successful\")\n","\n","\n","#---------------- Dimensionality reduction ----------------#\n","\n","\n","# Load the Iris dataset\n","iris = load_iris()\n","data = iris.data\n","target = iris.target\n","target_names = iris.target_names\n","\n","# Create a Pandas DataFrame for TableOne\n","iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n","iris_df['target'] = iris.target  # Add the target column\n","iris_columns = iris_df.columns.tolist()\n","\n","# Print dataset information and TableOne\n","print(f\"The dataset length is {str(len(iris_df))}\")\n","print(f\"The number of columns is {str(len(iris_columns))}\")\n","print(f\"The column names are {str(iris_columns)} \\n\")\n","print(\"Table one of input data: \")\n","table1 = TableOne(iris_df, columns=iris_columns, groupby='target', pval=True)\n","print(table1)\n","\n","# Scale the data for UMAP\n","scaler = StandardScaler()\n","data_scaled = scaler.fit_transform(data)\n","\n","# Apply UMAP\n","reducer = UMAP(n_components=2, random_state=42)\n","data_umap = reducer.fit_transform(data_scaled)\n","\n","# Plot UMAP projection\n","plt.figure(figsize=(8, 6))\n","for class_idx, class_name in enumerate(target_names):\n","    plt.scatter(\n","        data_umap[target == class_idx, 0],\n","        data_umap[target == class_idx, 1],\n","        label=class_name,\n","        s=50,\n","    )\n","\n","plt.title(\"UMAP Projection of Iris Dataset\")\n","plt.xlabel(\"UMAP Dimension 1\")\n","plt.ylabel(\"UMAP Dimension 2\")\n","plt.legend(title=\"Classes\")\n","plt.grid(True, alpha=0.3)\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"Lt_u8zj_uMxy"},"source":["In this trial of the autoencoder, in order to get a meaningful latent space representation we are going to fix latent space dimensionality at 2, so we can compare it to our UMAP."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"raenz6Qh9zz1"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":50041,"status":"ok","timestamp":1734109827610,"user":{"displayName":"Archie Goodman","userId":"05960694724077487952"},"user_tz":0},"id":"1H2nKn6O4cXv","outputId":"98cdbd76-d160-405a-fa99-b480ebf0128c"},"outputs":[{"name":"stderr","output_type":"stream","text":["[I 2024-12-13 17:09:37,711] A new study created in memory with name: no-name-8083de57-19fb-4deb-9def-7f9ccb687986\n"]},{"name":"stdout","output_type":"stream","text":["Cuda setup successful\n"]},{"name":"stderr","output_type":"stream","text":["\u003cipython-input-31-62070ad59d81\u003e:178: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  optimizer = optim.Adam(model.parameters(), lr=trial.suggest_loguniform(\"lr\", 1e-5, 1e-1))\n","[I 2024-12-13 17:09:40,047] Trial 0 finished with value: 0.18431433141231537 and parameters: {'num_encoder_layers': 4, 'num_decoder_layers': 3, 'encoder_neurons_0': 16, 'encoder_neurons_1': 64, 'encoder_neurons_2': 32, 'encoder_neurons_3': 32, 'decoder_neurons_0': 80, 'decoder_neurons_1': 128, 'decoder_neurons_2': 64, 'encoder_activation_0': 'Tanh', 'encoder_activation_1': 'Tanh', 'encoder_activation_2': 'Tanh', 'encoder_activation_3': 'ReLU', 'decoder_activation_0': 'Sigmoid', 'decoder_activation_1': 'Sigmoid', 'decoder_activation_2': 'LeakyReLU', 'dropout_prob': 0.006428088756407713, 'use_batchnorm': True, 'batch_size': 38, 'lr': 0.06634759612555172}. Best is trial 0 with value: 0.18431433141231537.\n","[I 2024-12-13 17:09:42,083] Trial 1 finished with value: 0.0347425764426589 and parameters: {'num_encoder_layers': 2, 'num_decoder_layers': 3, 'encoder_neurons_0': 128, 'encoder_neurons_1': 16, 'decoder_neurons_0': 16, 'decoder_neurons_1': 16, 'decoder_neurons_2': 112, 'encoder_activation_0': 'Tanh', 'encoder_activation_1': 'LeakyReLU', 'decoder_activation_0': 'Tanh', 'decoder_activation_1': 'ReLU', 'decoder_activation_2': 'ReLU', 'dropout_prob': 0.020023780601262586, 'use_batchnorm': True, 'batch_size': 38, 'lr': 0.00027909071278628156}. Best is trial 1 with value: 0.0347425764426589.\n","[I 2024-12-13 17:09:43,542] Trial 2 finished with value: 0.036813712865114215 and parameters: {'num_encoder_layers': 3, 'num_decoder_layers': 3, 'encoder_neurons_0': 16, 'encoder_neurons_1': 80, 'encoder_neurons_2': 64, 'decoder_neurons_0': 64, 'decoder_neurons_1': 64, 'decoder_neurons_2': 48, 'encoder_activation_0': 'Tanh', 'encoder_activation_1': 'Tanh', 'encoder_activation_2': 'LeakyReLU', 'decoder_activation_0': 'Sigmoid', 'decoder_activation_1': 'Sigmoid', 'decoder_activation_2': 'Sigmoid', 'dropout_prob': 0.31463452446264506, 'use_batchnorm': True, 'batch_size': 54, 'lr': 0.0002689466453515003}. Best is trial 1 with value: 0.0347425764426589.\n","[I 2024-12-13 17:09:44,623] Trial 3 finished with value: 0.07289649546146393 and parameters: {'num_encoder_layers': 3, 'num_decoder_layers': 3, 'encoder_neurons_0': 32, 'encoder_neurons_1': 96, 'encoder_neurons_2': 48, 'decoder_neurons_0': 128, 'decoder_neurons_1': 96, 'decoder_neurons_2': 16, 'encoder_activation_0': 'Sigmoid', 'encoder_activation_1': 'Tanh', 'encoder_activation_2': 'Tanh', 'decoder_activation_0': 'Sigmoid', 'decoder_activation_1': 'Tanh', 'decoder_activation_2': 'Sigmoid', 'dropout_prob': 0.2941945407204067, 'use_batchnorm': False, 'batch_size': 63, 'lr': 3.1015501782122614e-05}. Best is trial 1 with value: 0.0347425764426589.\n","[I 2024-12-13 17:09:48,718] Trial 4 finished with value: 0.2688218176364899 and parameters: {'num_encoder_layers': 4, 'num_decoder_layers': 5, 'encoder_neurons_0': 80, 'encoder_neurons_1': 16, 'encoder_neurons_2': 112, 'encoder_neurons_3': 16, 'decoder_neurons_0': 80, 'decoder_neurons_1': 48, 'decoder_neurons_2': 16, 'decoder_neurons_3': 96, 'decoder_neurons_4': 32, 'encoder_activation_0': 'ReLU', 'encoder_activation_1': 'ReLU', 'encoder_activation_2': 'Sigmoid', 'encoder_activation_3': 'LeakyReLU', 'decoder_activation_0': 'ReLU', 'decoder_activation_1': 'LeakyReLU', 'decoder_activation_2': 'Tanh', 'decoder_activation_3': 'Sigmoid', 'decoder_activation_4': 'ReLU', 'dropout_prob': 0.3232950943134144, 'use_batchnorm': True, 'batch_size': 26, 'lr': 0.060832777943143033}. Best is trial 1 with value: 0.0347425764426589.\n","[I 2024-12-13 17:09:52,667] Trial 5 finished with value: 0.13531646095216274 and parameters: {'num_encoder_layers': 4, 'num_decoder_layers': 2, 'encoder_neurons_0': 32, 'encoder_neurons_1': 128, 'encoder_neurons_2': 16, 'encoder_neurons_3': 80, 'decoder_neurons_0': 64, 'decoder_neurons_1': 96, 'encoder_activation_0': 'ReLU', 'encoder_activation_1': 'Tanh', 'encoder_activation_2': 'ReLU', 'encoder_activation_3': 'LeakyReLU', 'decoder_activation_0': 'Sigmoid', 'decoder_activation_1': 'Tanh', 'dropout_prob': 0.11737765394162664, 'use_batchnorm': True, 'batch_size': 21, 'lr': 1.2419406668071472e-05}. Best is trial 1 with value: 0.0347425764426589.\n","[I 2024-12-13 17:09:54,262] Trial 6 finished with value: 0.012506713904440404 and parameters: {'num_encoder_layers': 5, 'num_decoder_layers': 3, 'encoder_neurons_0': 48, 'encoder_neurons_1': 80, 'encoder_neurons_2': 128, 'encoder_neurons_3': 32, 'encoder_neurons_4': 96, 'decoder_neurons_0': 64, 'decoder_neurons_1': 112, 'decoder_neurons_2': 64, 'encoder_activation_0': 'Tanh', 'encoder_activation_1': 'LeakyReLU', 'encoder_activation_2': 'Tanh', 'encoder_activation_3': 'LeakyReLU', 'encoder_activation_4': 'Tanh', 'decoder_activation_0': 'Tanh', 'decoder_activation_1': 'LeakyReLU', 'decoder_activation_2': 'Tanh', 'dropout_prob': 0.3407385707485904, 'use_batchnorm': True, 'batch_size': 55, 'lr': 0.005061466617094738}. Best is trial 6 with value: 0.012506713904440404.\n","[I 2024-12-13 17:09:56,247] Trial 7 finished with value: 0.009670288395136594 and parameters: {'num_encoder_layers': 3, 'num_decoder_layers': 5, 'encoder_neurons_0': 128, 'encoder_neurons_1': 48, 'encoder_neurons_2': 128, 'decoder_neurons_0': 64, 'decoder_neurons_1': 80, 'decoder_neurons_2': 32, 'decoder_neurons_3': 112, 'decoder_neurons_4': 48, 'encoder_activation_0': 'ReLU', 'encoder_activation_1': 'LeakyReLU', 'encoder_activation_2': 'Tanh', 'decoder_activation_0': 'Tanh', 'decoder_activation_1': 'Sigmoid', 'decoder_activation_2': 'ReLU', 'decoder_activation_3': 'ReLU', 'decoder_activation_4': 'ReLU', 'dropout_prob': 0.1089784956399295, 'use_batchnorm': False, 'batch_size': 34, 'lr': 0.008827615323085338}. Best is trial 7 with value: 0.009670288395136594.\n","[I 2024-12-13 17:09:59,408] Trial 8 finished with value: 0.01442292685387656 and parameters: {'num_encoder_layers': 2, 'num_decoder_layers': 4, 'encoder_neurons_0': 80, 'encoder_neurons_1': 64, 'decoder_neurons_0': 112, 'decoder_neurons_1': 64, 'decoder_neurons_2': 112, 'decoder_neurons_3': 96, 'encoder_activation_0': 'ReLU', 'encoder_activation_1': 'LeakyReLU', 'decoder_activation_0': 'Tanh', 'decoder_activation_1': 'ReLU', 'decoder_activation_2': 'ReLU', 'decoder_activation_3': 'Sigmoid', 'dropout_prob': 0.41421999769759654, 'use_batchnorm': False, 'batch_size': 21, 'lr': 0.00634193891366265}. Best is trial 7 with value: 0.009670288395136594.\n","[I 2024-12-13 17:10:01,029] Trial 9 finished with value: 0.06919513940811158 and parameters: {'num_encoder_layers': 2, 'num_decoder_layers': 3, 'encoder_neurons_0': 16, 'encoder_neurons_1': 112, 'decoder_neurons_0': 48, 'decoder_neurons_1': 96, 'decoder_neurons_2': 96, 'encoder_activation_0': 'Tanh', 'encoder_activation_1': 'ReLU', 'decoder_activation_0': 'Tanh', 'decoder_activation_1': 'LeakyReLU', 'decoder_activation_2': 'Sigmoid', 'dropout_prob': 0.42093844282418846, 'use_batchnorm': False, 'batch_size': 41, 'lr': 0.00014342835824211024}. Best is trial 7 with value: 0.009670288395136594.\n","[I 2024-12-13 17:10:04,295] Trial 10 finished with value: 0.07421996369957924 and parameters: {'num_encoder_layers': 5, 'num_decoder_layers': 5, 'encoder_neurons_0': 128, 'encoder_neurons_1': 48, 'encoder_neurons_2': 96, 'encoder_neurons_3': 128, 'encoder_neurons_4': 16, 'decoder_neurons_0': 16, 'decoder_neurons_1': 32, 'decoder_neurons_2': 32, 'decoder_neurons_3': 16, 'decoder_neurons_4': 96, 'encoder_activation_0': 'LeakyReLU', 'encoder_activation_1': 'Sigmoid', 'encoder_activation_2': 'Sigmoid', 'encoder_activation_3': 'Sigmoid', 'encoder_activation_4': 'LeakyReLU', 'decoder_activation_0': 'LeakyReLU', 'decoder_activation_1': 'Sigmoid', 'decoder_activation_2': 'ReLU', 'decoder_activation_3': 'ReLU', 'decoder_activation_4': 'LeakyReLU', 'dropout_prob': 0.16627159817393214, 'use_batchnorm': False, 'batch_size': 31, 'lr': 0.005046100266278064}. Best is trial 7 with value: 0.009670288395136594.\n","[I 2024-12-13 17:10:05,650] Trial 11 finished with value: 0.01409400012344122 and parameters: {'num_encoder_layers': 5, 'num_decoder_layers': 4, 'encoder_neurons_0': 64, 'encoder_neurons_1': 48, 'encoder_neurons_2': 128, 'encoder_neurons_3': 64, 'encoder_neurons_4': 112, 'decoder_neurons_0': 48, 'decoder_neurons_1': 128, 'decoder_neurons_2': 80, 'decoder_neurons_3': 128, 'encoder_activation_0': 'Sigmoid', 'encoder_activation_1': 'LeakyReLU', 'encoder_activation_2': 'Tanh', 'encoder_activation_3': 'Tanh', 'encoder_activation_4': 'Tanh', 'decoder_activation_0': 'Tanh', 'decoder_activation_1': 'LeakyReLU', 'decoder_activation_2': 'Tanh', 'decoder_activation_3': 'ReLU', 'dropout_prob': 0.16655324226407048, 'use_batchnorm': False, 'batch_size': 50, 'lr': 0.005006478904968962}. Best is trial 7 with value: 0.009670288395136594.\n","[I 2024-12-13 17:10:06,835] Trial 12 finished with value: 0.01232475060969591 and parameters: {'num_encoder_layers': 3, 'num_decoder_layers': 2, 'encoder_neurons_0': 96, 'encoder_neurons_1': 48, 'encoder_neurons_2': 128, 'decoder_neurons_0': 96, 'decoder_neurons_1': 112, 'encoder_activation_0': 'LeakyReLU', 'encoder_activation_1': 'LeakyReLU', 'encoder_activation_2': 'Tanh', 'decoder_activation_0': 'Tanh', 'decoder_activation_1': 'Sigmoid', 'dropout_prob': 0.48779065037168573, 'use_batchnorm': True, 'batch_size': 53, 'lr': 0.013822819883075684}. Best is trial 7 with value: 0.009670288395136594.\n","[I 2024-12-13 17:10:08,483] Trial 13 finished with value: 0.017569706588983536 and parameters: {'num_encoder_layers': 3, 'num_decoder_layers': 2, 'encoder_neurons_0': 112, 'encoder_neurons_1': 32, 'encoder_neurons_2': 96, 'decoder_neurons_0': 96, 'decoder_neurons_1': 80, 'encoder_activation_0': 'LeakyReLU', 'encoder_activation_1': 'LeakyReLU', 'encoder_activation_2': 'Tanh', 'decoder_activation_0': 'LeakyReLU', 'decoder_activation_1': 'Sigmoid', 'dropout_prob': 0.4996672993256617, 'use_batchnorm': False, 'batch_size': 44, 'lr': 0.017698877928628443}. Best is trial 7 with value: 0.009670288395136594.\n","[I 2024-12-13 17:10:10,233] Trial 14 finished with value: 0.010620595328509808 and parameters: {'num_encoder_layers': 3, 'num_decoder_layers': 5, 'encoder_neurons_0': 96, 'encoder_neurons_1': 48, 'encoder_neurons_2': 96, 'decoder_neurons_0': 96, 'decoder_neurons_1': 80, 'decoder_neurons_2': 48, 'decoder_neurons_3': 48, 'decoder_neurons_4': 32, 'encoder_activation_0': 'LeakyReLU', 'encoder_activation_1': 'Sigmoid', 'encoder_activation_2': 'ReLU', 'decoder_activation_0': 'ReLU', 'decoder_activation_1': 'Sigmoid', 'decoder_activation_2': 'LeakyReLU', 'decoder_activation_3': 'LeakyReLU', 'decoder_activation_4': 'ReLU', 'dropout_prob': 0.09974559591509796, 'use_batchnorm': True, 'batch_size': 63, 'lr': 0.001512530627534839}. Best is trial 7 with value: 0.009670288395136594.\n","[I 2024-12-13 17:10:11,546] Trial 15 finished with value: 0.02239966057240963 and parameters: {'num_encoder_layers': 3, 'num_decoder_layers': 5, 'encoder_neurons_0': 112, 'encoder_neurons_1': 32, 'encoder_neurons_2': 96, 'decoder_neurons_0': 128, 'decoder_neurons_1': 80, 'decoder_neurons_2': 48, 'decoder_neurons_3': 32, 'decoder_neurons_4': 32, 'encoder_activation_0': 'ReLU', 'encoder_activation_1': 'Sigmoid', 'encoder_activation_2': 'ReLU', 'decoder_activation_0': 'ReLU', 'decoder_activation_1': 'Sigmoid', 'decoder_activation_2': 'LeakyReLU', 'decoder_activation_3': 'LeakyReLU', 'decoder_activation_4': 'ReLU', 'dropout_prob': 0.08577295519883454, 'use_batchnorm': False, 'batch_size': 62, 'lr': 0.001184583287105207}. Best is trial 7 with value: 0.009670288395136594.\n","[I 2024-12-13 17:10:14,291] Trial 16 finished with value: 0.012644093111157417 and parameters: {'num_encoder_layers': 4, 'num_decoder_layers': 4, 'encoder_neurons_0': 96, 'encoder_neurons_1': 32, 'encoder_neurons_2': 80, 'encoder_neurons_3': 128, 'decoder_neurons_0': 96, 'decoder_neurons_1': 64, 'decoder_neurons_2': 32, 'decoder_neurons_3': 64, 'encoder_activation_0': 'LeakyReLU', 'encoder_activation_1': 'Sigmoid', 'encoder_activation_2': 'ReLU', 'encoder_activation_3': 'Tanh', 'decoder_activation_0': 'ReLU', 'decoder_activation_1': 'Sigmoid', 'decoder_activation_2': 'LeakyReLU', 'decoder_activation_3': 'Tanh', 'dropout_prob': 0.23345496837129415, 'use_batchnorm': False, 'batch_size': 32, 'lr': 0.0009979377520163897}. Best is trial 7 with value: 0.009670288395136594.\n","[I 2024-12-13 17:10:20,774] Trial 17 finished with value: 0.013598675839602947 and parameters: {'num_encoder_layers': 2, 'num_decoder_layers': 5, 'encoder_neurons_0': 112, 'encoder_neurons_1': 64, 'decoder_neurons_0': 32, 'decoder_neurons_1': 48, 'decoder_neurons_2': 48, 'decoder_neurons_3': 48, 'decoder_neurons_4': 48, 'encoder_activation_0': 'ReLU', 'encoder_activation_1': 'Sigmoid', 'decoder_activation_0': 'ReLU', 'decoder_activation_1': 'Sigmoid', 'decoder_activation_2': 'ReLU', 'decoder_activation_3': 'LeakyReLU', 'decoder_activation_4': 'ReLU', 'dropout_prob': 0.07787300851584425, 'use_batchnorm': True, 'batch_size': 16, 'lr': 0.001419110375080621}. Best is trial 7 with value: 0.009670288395136594.\n","[I 2024-12-13 17:10:23,479] Trial 18 finished with value: 0.01264299899339676 and parameters: {'num_encoder_layers': 3, 'num_decoder_layers': 4, 'encoder_neurons_0': 96, 'encoder_neurons_1': 96, 'encoder_neurons_2': 112, 'decoder_neurons_0': 112, 'decoder_neurons_1': 80, 'decoder_neurons_2': 32, 'decoder_neurons_3': 128, 'encoder_activation_0': 'LeakyReLU', 'encoder_activation_1': 'Sigmoid', 'encoder_activation_2': 'LeakyReLU', 'decoder_activation_0': 'ReLU', 'decoder_activation_1': 'ReLU', 'decoder_activation_2': 'LeakyReLU', 'decoder_activation_3': 'ReLU', 'dropout_prob': 0.20966794643563702, 'use_batchnorm': False, 'batch_size': 32, 'lr': 0.02230257571753195}. Best is trial 7 with value: 0.009670288395136594.\n","[I 2024-12-13 17:10:26,442] Trial 19 finished with value: 0.00933446567505598 and parameters: {'num_encoder_layers': 4, 'num_decoder_layers': 5, 'encoder_neurons_0': 128, 'encoder_neurons_1': 48, 'encoder_neurons_2': 80, 'encoder_neurons_3': 96, 'decoder_neurons_0': 48, 'decoder_neurons_1': 48, 'decoder_neurons_2': 80, 'decoder_neurons_3': 80, 'decoder_neurons_4': 64, 'encoder_activation_0': 'Sigmoid', 'encoder_activation_1': 'ReLU', 'encoder_activation_2': 'ReLU', 'encoder_activation_3': 'ReLU', 'decoder_activation_0': 'LeakyReLU', 'decoder_activation_1': 'Tanh', 'decoder_activation_2': 'ReLU', 'decoder_activation_3': 'LeakyReLU', 'decoder_activation_4': 'Sigmoid', 'dropout_prob': 0.13173536509616735, 'use_batchnorm': True, 'batch_size': 44, 'lr': 0.0014407570216105543}. Best is trial 19 with value: 0.00933446567505598.\n"]},{"name":"stdout","output_type":"stream","text":["Best hyperparameters: {'num_encoder_layers': 4, 'num_decoder_layers': 5, 'encoder_neurons_0': 128, 'encoder_neurons_1': 48, 'encoder_neurons_2': 80, 'encoder_neurons_3': 96, 'decoder_neurons_0': 48, 'decoder_neurons_1': 48, 'decoder_neurons_2': 80, 'decoder_neurons_3': 80, 'decoder_neurons_4': 64, 'encoder_activation_0': 'Sigmoid', 'encoder_activation_1': 'ReLU', 'encoder_activation_2': 'ReLU', 'encoder_activation_3': 'ReLU', 'decoder_activation_0': 'LeakyReLU', 'decoder_activation_1': 'Tanh', 'decoder_activation_2': 'ReLU', 'decoder_activation_3': 'LeakyReLU', 'decoder_activation_4': 'Sigmoid', 'dropout_prob': 0.13173536509616735, 'use_batchnorm': True, 'batch_size': 44, 'lr': 0.0014407570216105543}\n","Test loss on holdout set: 0.010010541416704655\n"]}],"source":["# **Set device for GPU acceleration**\n","\n","# Set device for GPU acceleration\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","if device.type != \"cuda\":\n","    print(\"WARNING: GPU is not available. The model will run on the CPU, which might be slower.\")\n","else:\n","    print(\"Cuda setup successful\")\n","\n","# Load and preprocess the iris dataset\n","iris = load_iris()\n","data = iris.data\n","\n","# Scale data to the range [0, 1] for better convergence\n","scaler = MinMaxScaler()\n","data = scaler.fit_transform(data)\n","\n","# Convert to PyTorch Dataset\n","class IrisDataset(Dataset):\n","    def __init__(self, data):\n","        self.data = torch.tensor(data, dtype=torch.float32)\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        return self.data[idx]\n","\n","dataset = IrisDataset(data)\n","\n","# split into train + validation set, to be dynamically split later\n","train_val_size = int(0.8 * len(dataset))\n","test_size = len(dataset) - train_val_size\n","train_val_set, test_set = random_split(dataset, [train_val_size, test_size])\n","\n","# Define Flexible Autoencoder architecture\n","class FlexibleAutoencoder(nn.Module):\n","    def __init__(self, input_dim, num_encoder_layers, encoder_neurons, num_decoder_layers, decoder_neurons, activations, dropout_prob, use_batchnorm):\n","        super(FlexibleAutoencoder, self).__init__()\n","\n","        # Build the encoder\n","        encoder = []\n","        in_dim = input_dim\n","\n","        for i in range(num_encoder_layers):\n","            out_dim = encoder_neurons[i] if i \u003c num_encoder_layers - 1 else 2  # Latent space dimensionality is fixed at 2\n","            encoder.append(nn.Linear(in_dim, out_dim))\n","            if use_batchnorm and i \u003c num_encoder_layers - 1:\n","                encoder.append(nn.BatchNorm1d(out_dim))\n","            encoder.append(getattr(nn, activations[i])())  # Use activations from encoder_activations list\n","            if dropout_prob \u003e 0:\n","                encoder.append(nn.Dropout(dropout_prob))\n","            in_dim = out_dim\n","        self.encoder = nn.Sequential(*encoder)\n","\n","        # Build the decoder\n","        decoder = []\n","        in_dim = 2  # Latent space dimensionality\n","        for i in range(num_decoder_layers):\n","            out_dim = decoder_neurons[i] if i \u003c num_decoder_layers - 1 else input_dim  # Output layer has input_dim neurons\n","            decoder.append(nn.Linear(in_dim, out_dim))\n","            if use_batchnorm and i \u003c num_decoder_layers - 1:\n","                decoder.append(nn.BatchNorm1d(out_dim))\n","            decoder.append(getattr(nn, activations[num_encoder_layers + i])())  # Use activations from decoder_activations list\n","            if dropout_prob \u003e 0 and i \u003c num_decoder_layers - 1:\n","                decoder.append(nn.Dropout(dropout_prob))\n","            in_dim = out_dim\n","        self.decoder = nn.Sequential(*decoder)\n","\n","    def forward(self, x):\n","        latent = self.encoder(x)\n","        reconstructed = self.decoder(latent)\n","        return latent, reconstructed\n","\n","\n","# Training and evaluation functions\n","def train_model(model, train_loader, val_loader, optimizer, criterion, epochs=50):\n","    model.to(device)  # Move model to the correct device (GPU or CPU)\n","\n","    # Lists to track the loss for each epoch\n","    train_loss_history = []\n","    val_loss_history = []\n","\n","    # Loop over the specified number of epochs\n","    for epoch in range(epochs):\n","        model.train()  # Set the model to training mode\n","        total_train_loss = 0.0\n","\n","        # Training loop\n","        for batch in train_loader:\n","            batch = batch.to(device)  # Move batch to the device\n","            optimizer.zero_grad()  # Zero out the gradients\n","            latent, reconstructed = model(batch)  # Get model outputs # unpack output tuple here\n","            loss = criterion(reconstructed, batch)  # Calculate the loss using reconstructed output\n","            loss.backward()  # Backpropagate the loss\n","            optimizer.step()  # Update model parameters\n","\n","            total_train_loss += loss.item()  # Accumulate the training loss\n","\n","        # Calculate average training loss for the epoch\n","        avg_train_loss = total_train_loss / len(train_loader)\n","        train_loss_history.append(avg_train_loss)\n","\n","        # Validation loop\n","        model.eval()  # Set the model to evaluation mode\n","        total_val_loss = 0.0\n","\n","        with torch.no_grad():  # No gradients needed for validation\n","            for batch in val_loader:\n","                batch = batch.to(device)  # Move batch to the device\n","                latent, reconstructed = model(batch) # unpack output tuple here\n","                loss = criterion(reconstructed, batch)  # Calculate the loss using reconstructed output\n","                total_val_loss += loss.item()  # Accumulate the validation loss\n","\n","        # Calculate average validation loss for the epoch\n","        avg_val_loss = total_val_loss / len(val_loader)\n","        val_loss_history.append(avg_val_loss)\n","\n","    return train_loss_history, val_loss_history\n","\n","def evaluate(model, dataloader, criterion):\n","    model.eval()\n","    total_loss = 0.0\n","    with torch.no_grad():\n","        for batch in dataloader:\n","            batch = batch.to(device)\n","            _, outputs = model(batch)  # Unpack the output tuple, using only the reconstructed output\n","            loss = criterion(outputs, batch)\n","            total_loss += loss.item()\n","    return total_loss / len(dataloader)\n","\n","# K-Fold Cross-Validation with Optuna\n","from sklearn.model_selection import KFold\n","import torch.optim as optim\n","\n","# K-Fold Cross-Validation with Optuna\n","def objective(trial):\n","    # Number of encoder and decoder layers\n","    num_encoder_layers = trial.suggest_int(\"num_encoder_layers\", 2, 5)\n","    num_decoder_layers = trial.suggest_int(\"num_decoder_layers\", 2, 5)\n","\n","    # List of possible activation functions\n","    activation_choices = [\"ReLU\", \"Tanh\", \"LeakyReLU\", \"Sigmoid\"]\n","\n","    # Vary the number of neurons for each encoder and decoder layer\n","    encoder_neurons = []\n","    decoder_neurons = []\n","\n","    for i in range(num_encoder_layers):\n","        encoder_neurons.append(trial.suggest_int(f\"encoder_neurons_{i}\", 16, 128, step=16))\n","\n","    for i in range(num_decoder_layers):\n","        decoder_neurons.append(trial.suggest_int(f\"decoder_neurons_{i}\", 16, 128, step=16))\n","\n","    activations = [trial.suggest_categorical(f\"encoder_activation_{i}\", activation_choices) for i in range(num_encoder_layers)]\n","    activations += [trial.suggest_categorical(f\"decoder_activation_{i}\", activation_choices) for i in range(num_decoder_layers)]\n","\n","    # Set dropout and batch normalization parameters\n","    dropout_prob = trial.suggest_float(\"dropout_prob\", 0.0, 0.5)\n","    use_batchnorm = trial.suggest_categorical(\"use_batchnorm\", [True, False])\n","\n","    # Suggest batch_size to Optuna:\n","    batch_size = trial.suggest_int(\"batch_size\", 16, 64) # Suggest batch sizes between 16 and 64\n","\n","    # Create the model\n","    model = FlexibleAutoencoder(\n","        input_dim=data.shape[1],\n","        num_encoder_layers=num_encoder_layers,\n","        encoder_neurons=encoder_neurons,\n","        num_decoder_layers=num_decoder_layers,\n","        decoder_neurons=decoder_neurons,\n","        activations=activations,\n","        dropout_prob=dropout_prob,\n","        use_batchnorm=use_batchnorm\n","    )\n","\n","    # Define optimizer and loss function\n","    optimizer = optim.Adam(model.parameters(), lr=trial.suggest_loguniform(\"lr\", 1e-5, 1e-1))\n","    criterion = torch.nn.MSELoss()  # Using MSE loss as it's an autoencoder\n","\n","    # K-Fold Cross Validation\n","    kfold = KFold(n_splits=5, shuffle=True)\n","    fold_losses = []\n","\n","    for fold, (train_idx, val_idx) in enumerate(kfold.split(train_val_set)):\n","        # Create train and validation datasets for this fold\n","        train_subset = torch.utils.data.Subset(train_val_set, train_idx)\n","        val_subset = torch.utils.data.Subset(train_val_set, val_idx)\n","\n","        # DataLoader for the current fold\n","        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, drop_last = True) # Use suggested batch_size\n","        val_batch_size = min(batch_size, len(val_subset)) # Ensure val_batch_size \u003c= validation set size\n","        val_loader = DataLoader(val_subset, batch_size=val_batch_size, shuffle=False, drop_last = False) # Use suggested batch_size\n","\n","        # Train the model for the current fold\n","        train_loss_history, val_loss_history = train_model(model, train_loader, val_loader, optimizer, criterion, epochs=50)\n","\n","        # Calculate the average validation loss for the fold\n","        fold_losses.append(val_loss_history[-1])  # Get last epoch's validation loss\n","\n","    # Return the average loss across all folds\n","    avg_fold_loss = sum(fold_losses) / len(fold_losses)\n","\n","    return avg_fold_loss\n","\n","\n","# Optimize hyperparameters using Optuna\n","study = optuna.create_study(direction=\"minimize\")\n","study.optimize(objective, n_trials=20)\n","\n","# Best hyperparameters\n","print(\"Best hyperparameters:\", study.best_params)\n","\n","# Evaluate on holdout test set\n","# Evaluate on holdout test set\n","best_params = study.best_params\n","num_encoder_layers = best_params[\"num_encoder_layers\"]\n","num_decoder_layers = best_params[\"num_decoder_layers\"]\n","\n","# Extract encoder and decoder neurons based on the number of layers\n","encoder_neurons = [best_params[f\"encoder_neurons_{i}\"] for i in range(num_encoder_layers)]\n","decoder_neurons = [best_params[f\"decoder_neurons_{i}\"] for i in range(num_decoder_layers)]\n","\n","# Extract activation functions based on the number of layers\n","activations = [best_params[f\"encoder_activation_{i}\"] for i in range(num_encoder_layers)]\n","activations += [best_params[f\"decoder_activation_{i}\"] for i in range(num_decoder_layers)]\n","\n","final_model = FlexibleAutoencoder(\n","    input_dim=data.shape[1],\n","    num_encoder_layers=num_encoder_layers,\n","    encoder_neurons=encoder_neurons,  # Use the extracted encoder_neurons\n","    num_decoder_layers=num_decoder_layers,\n","    decoder_neurons=decoder_neurons,  # Use the extracted decoder_neurons\n","    activations=activations,  # Use the extracted activations\n","    dropout_prob=best_params[\"dropout_prob\"],\n","    use_batchnorm=best_params[\"use_batchnorm\"],\n",").to(device)\n","\n","test_loader = DataLoader(test_set, batch_size=best_params[\"batch_size\"])\n","optimizer = torch.optim.Adam(final_model.parameters(), lr=best_params[\"lr\"])\n","criterion = nn.MSELoss()\n","\n","# Train final model\n","train_model(final_model, DataLoader(train_val_set, batch_size=best_params[\"batch_size\"], shuffle=True), test_loader, optimizer, criterion) # Added test_loader as the validation loader here\n","\n","# Evaluate on the test set\n","test_loss = evaluate(final_model, test_loader, criterion)\n","print(\"Test loss on holdout set:\", test_loss)\n"]},{"cell_type":"markdown","metadata":{"id":"96jRNy_Q4Ufs"},"source":["Now let's visualize our latent space representation to compare it to our UMAP - this will allow us to see how well the data is clustered. TO DO"]},{"cell_type":"markdown","metadata":{"id":"RB7Z-FfYBCVy"},"source":["\n","\n","---\n","\n","# Auto push to github\n","\n","\n"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3747,"status":"ok","timestamp":1734109748193,"user":{"displayName":"Archie Goodman","userId":"05960694724077487952"},"user_tz":0},"id":"8vO8P6TOBG2R"},"outputs":[],"source":["import datetime\n","import os\n","\n","def commit_to_github(commit_msg):\n","  \"\"\"\n","  Funct to autopush to github\n","  \"\"\"\n","\n","  # Navigate to the repository directory\n","  %cd /content/drive/MyDrive/Colab_Notebooks/Deep_Learning_Practice\n","\n","  !git add .\n","\n","  with open('/content/drive/MyDrive/IAM/PAT.txt', 'r') as file:\n","        github_pat = file.read().strip()\n","  os.environ['GITHUB_PAT'] = github_pat\n","\n","  !git remote add origin \"https://github.com/archiegoodman2/machine_learning_practice\"\n","\n","  # Replace with your actual username and email\n","  USERNAME=\"archiegoodman2\"\n","  EMAIL=\"archiegoodman2011@gmail.com\"\n","\n","  # Set global username and email configuration\n","  !git config --global user.name \"$USERNAME\"\n","  !git config --global user.email \"$EMAIL\"\n","\n","  now = datetime.datetime.now()\n","  current_datetime = now.strftime(\"%Y-%m-%d %H:%M\")\n","\n","  # Set remote URL using the PAT from environment variable\n","  !git remote set-url origin https://{os.environ['GITHUB_PAT']}@github.com/archiegoodman2/machine_learning_practice.git\n","\n","  # Replace with your desired commit message\n","  COMMIT_MESSAGE = str(current_datetime) + \" \" + str(commit_msg)\n","\n","  # Commit the changes\n","  !git commit -m \"$COMMIT_MESSAGE\"\n","\n","  # Push to origin (force push if necessary)\n","  !git push origin master\n","\n","  return 1\n","\n","commit_to_github(\"added kfold cross validation. added flexible number of neurons for each individual layer.\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNIjdv47kQ/XA1/Afw6jfJb","gpuType":"T4","machine_shape":"hm","name":"","provenance":[{"file_id":"1iPCbFw6VccL3dYtmaySZgWMX_LvDDXXJ","timestamp":1733764551144}],"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}