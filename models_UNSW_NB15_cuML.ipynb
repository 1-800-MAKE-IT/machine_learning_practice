{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","mount_file_id":"1bRPN2-9luXnYH0cyOl3DFjyOdCkXkGGk","authorship_tag":"ABX9TyO45gLgbS5dDeysqslQKXIG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":[],"metadata":{"id":"iuEjIjTkbdSK"}},{"cell_type":"markdown","source":["# Practice run of analysing/testing different models on the UNSW_NB15 dataset, before trying Deep Learning.\n","\n","Prior research suggests this is a largely non-linear, less separable dataset so deep learning may be necessary, but I will try simpler, more interpretable models first for the sake of completeness, and to gain Variable Importances"],"metadata":{"id":"Yhq0AM-GxTw6"}},{"cell_type":"code","source":["#import packages:\n","\n","\n","from google.colab import drive\n","\n","try:\n","  import google.colab\n","  IN_COLAB = True\n","except:\n","  IN_COLAB = False\n","\n","if IN_COLAB:\n","  # Check if drive is mounted by looking for the mount point in the file system.\n","  # This is a more robust approach than relying on potentially internal variables.\n","  import os\n","  if not os.path.exists('/content/drive'):\n","    drive.mount('/content/drive')\n","  else:\n","    print(\"Google Drive is already mounted.\")\n","else:\n","  print(\"Not running in Google Colab. Drive mounting skipped.\")\n","\n","from IPython import get_ipython\n","from IPython.display import display\n","import cudf\n","try:\n","    import cuml\n","    print(\"RAPIDS is already installed.\")\n","except ImportError:\n","    print(\"RAPIDS not found, installing...\")\n","    !git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n","    !python rapidsai-csp-utils/colab/pip-install.py\n","    print(\"RAPIDS installed successfully.\")\n","finally:\n","  import cuml\n","\n","print(\"RAPIDS version:\", cuml.__version__)\n","from cuml.preprocessing import StandardScaler\n","from cuml.model_selection import StratifiedKFold, GridSearchCV\n","from cuml.linear_model import LogisticRegression\n","from cuml.pipeline import Pipeline\n","from cuml.ensemble import RandomForestClassifier\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from tqdm import tqdm\n"],"metadata":{"id":"WSMa2bmU5XCI","colab":{"base_uri":"https://localhost:8080/","height":401},"executionInfo":{"status":"error","timestamp":1730246396483,"user_tz":0,"elapsed":7970,"user":{"displayName":"Archie Goodman","userId":"05960694724077487952"}},"outputId":"d21aa315-12ec-4fa2-8154-7a6192122eb7"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Google Drive is already mounted.\n"]},{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'cuml'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-7766fce4846b>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcudf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcuml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RAPIDS version:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcuml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cuml'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"markdown","source":["Let's load our packages and data"],"metadata":{"id":"BwpZ83SO1wod"}},{"cell_type":"code","source":["#if using colabs - will need to first mount your drive\n","\n","#change these for different users\n","test_set_filepath = '/content/drive/MyDrive/Colab_Notebooks/Data/UNSW_NB15_testing-set.parquet'\n","training_set_filepath = '/content/drive/MyDrive/Colab_Notebooks/Data/UNSW_NB15_training-set.parquet'\n","\n","# Import the two CSV files\n","test_set = pd.read_parquet(test_set_filepath)\n","train_set = pd.read_parquet(training_set_filepath)\n","\n","print(\"Data loaded\")\n"],"metadata":{"id":"OsQcbrhZyWcN","executionInfo":{"status":"aborted","timestamp":1730246396485,"user_tz":0,"elapsed":6,"user":{"displayName":"Archie Goodman","userId":"05960694724077487952"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The next cell does some basic analysis, and one hot encodes some of the features:"],"metadata":{"id":"N7RupKUW7OgV"}},{"cell_type":"code","source":["def preprocess_data(data_set):\n","    # Remove 'attack_cat' column if it exists\n","    if 'attack_cat' in data_set.columns.tolist():\n","        data_set.drop('attack_cat', axis=1, inplace=True)\n","\n","    if 'proto' in data_set.columns.tolist():\n","        # Ensure 'proto' is of type 'object' to avoid categorical issues\n","        data_set['proto'] = data_set['proto'].astype(str)\n","\n","        # Calculate percentage occurrences of each category\n","        category_percentages = data_set['proto'].value_counts(normalize=True) * 100\n","        top_6_categories = category_percentages.head(6).index.tolist()\n","\n","        # Group less frequent categories under 'other' using vectorized operations\n","        data_set['proto_grouped'] = data_set['proto']\n","        data_set.loc[~data_set['proto'].isin(top_6_categories), 'proto_grouped'] = 'other'\n","\n","        # Drop the original 'proto' column\n","        data_set.drop('proto', axis=1, inplace=True)\n","\n","        # One-hot encode the 'proto_grouped' column\n","        data_set = pd.get_dummies(data_set, columns=['proto_grouped'], prefix='proto_grouped')\n","\n","    # One-hot encode any remaining categorical columns\n","    categorical_cols = data_set.select_dtypes(include=['object', 'category']).columns.tolist()\n","    if categorical_cols:\n","        data_set = pd.get_dummies(data_set, columns=categorical_cols, prefix_sep='_')\n","\n","    # Convert boolean columns to integers\n","    binary_cols = data_set.select_dtypes(include=['bool']).columns\n","\n","    if not binary_cols.empty:\n","        data_set[binary_cols] = data_set[binary_cols].astype(int)\n","\n","    return data_set\n","\n","\n","# Preprocess the datasets\n","train_set = preprocess_data(train_set)\n","test_set = preprocess_data(test_set)\n","\n","print(f\"Column sizes before further processing: Train: {len(train_set.columns)}, Test: {len(test_set.columns)}\")\n","\n","# Identify missing columns\n","train_columns = set(train_set.columns)\n","test_columns = set(test_set.columns)\n","\n","missing_in_test = train_columns - test_columns\n","missing_in_train = test_columns - train_columns\n","\n","# Remove 'label' from missing columns if present\n","missing_in_test.discard('label')\n","missing_in_train.discard('label')\n","\n","# Add missing columns to test_set\n","for col in missing_in_test:\n","    test_set[col] = 0\n","\n","# Add missing columns to train_set\n","for col in missing_in_train:\n","    train_set[col] = 0\n","\n","# Ensure the columns are in the same order\n","common_columns = sorted(train_set.columns)\n","\n","train_set = train_set[common_columns]\n","test_set = test_set[common_columns]\n","\n","# Verify the columns\n","print(f\"Number of columns in train_set: {len(train_set.columns)}\")\n","print(f\"Number of columns in test_set: {len(test_set.columns)}\")\n","\n","print(f\"Columns in train_set: {train_set.columns.tolist()}\")\n","print(f\"Columns in test_set: {test_set.columns.tolist()}\")\n","\n","#turn both pandas dataframes into cudf\n","train_set = cudf.DataFrame.from_pandas(train_set)\n","#we might not go ahead and use the test set but in case we do, it's here\n","test_set = cudf.DataFrame.from_pandas(test_set)\n","\n","print(\"Data preprocessed\")"],"metadata":{"id":"VengASbQCV3w","executionInfo":{"status":"aborted","timestamp":1730246396485,"user_tz":0,"elapsed":5,"user":{"displayName":"Archie Goodman","userId":"05960694724077487952"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["NOTE TO SELF -\n","1. THIS IS FOR BINARY CLASSIFICATION, WE WANT MULTICLASS EVENTUALLY, BUT FOR NOW WE WILL JUST DO BN\n"],"metadata":{"id":"eFYJj2gn1rqe"}},{"cell_type":"markdown","source":["Based on the high number of columns in the Proto column, we may want to consider an Embeddings layer with the Deep Learning that we plan to undertake later. However since DT/RF perform somewhat poorly on sparse vector datasets (like one hot encoded ones) we will group all the extremely rare categories into an 'other'.\n"],"metadata":{"id":"sYPGA0w_tgSK"}},{"cell_type":"code","source":["def run_models(model_type, X, y):\n","    \"\"\"\n","    Runs Logistic Regression (LR) or Random Forest (RF) model using nested cross-validation with cuML.\n","    \"\"\"\n","\n","    # Scale the data\n","    scaler = StandardScaler()\n","    X_scaled = scaler.fit_transform(X)\n","\n","    #=============================== Logistic Regression (LR) ========================================#\n","    if model_type.upper() == 'LR':\n","        model = cuml.linear_model.LogisticRegression(max_iter=1000)\n","\n","        param_grid = {'C': [0.01, 0.1, 1, 10, 100]}\n","\n","        outer_cv = cuml.model_selection.StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n","        inner_cv = cuml.model_selection.StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n","\n","        print(\"Running nested cross-validation for Logistic Regression.\")\n","\n","        outer_scores = []\n","\n","        for train_index, val_index in tqdm(outer_cv.split(X_scaled, y)):\n","            X_train_fold, X_val_fold = X_scaled[train_index], X_scaled[val_index]\n","            y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[val_index]\n","\n","            grid_search = cuml.model_selection.GridSearchCV(\n","                estimator=model,\n","                param_grid=param_grid,\n","                cv=inner_cv,\n","                scoring='roc_auc',\n","            )\n","\n","            grid_search.fit(X_train_fold, y_train_fold)\n","            best_score = grid_search.score(X_val_fold, y_val_fold)\n","            outer_scores.append(best_score)\n","\n","        print(f\"Average Validation ROC AUC from nested cross-validation: {np.mean(outer_scores)}\")\n","\n","    #=============================== Random Forest (RF) ========================================#\n","    elif model_type.upper() == 'RF':\n","        model = cuml.ensemble.RandomForestClassifier()\n","        param_grid_rf = {\n","            'n_estimators': [50, 100, 200],\n","            'max_depth': [3, 5, 10],\n","            'min_samples_split': [2, 10, 20],\n","            'min_samples_leaf': [1, 5, 10],\n","        }\n","\n","        outer_cv = cuml.model_selection.StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n","        inner_cv = cuml.model_selection.StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n","\n","        print(\"Running nested cross-validation for Random Forest.\")\n","\n","        outer_scores = []\n","\n","        for train_index, val_index in tqdm(outer_cv.split(X_scaled, y)):\n","            X_train_fold, X_val_fold = X_scaled[train_index], X_scaled[val_index]\n","            y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[val_index]\n","\n","            grid_search = cuml.model_selection.GridSearchCV(\n","                estimator=model,\n","                param_grid=param_grid_rf,\n","                cv=inner_cv,\n","                scoring='roc_auc',\n","            )\n","\n","            grid_search.fit(X_train_fold, y_train_fold)\n","            best_score = grid_search.score(X_val_fold, y_val_fold)\n","            outer_scores.append(best_score)\n","\n","        print(f\"Average Validation ROC AUC from nested cross-validation: {np.mean(outer_scores)}\")\n","\n","    else:\n","        print(\"Invalid model type. Please choose 'LR' for Logistic Regression or 'RF' for Random Forest.\")"],"metadata":{"id":"ZqRkXuXnMjmJ","executionInfo":{"status":"aborted","timestamp":1730246396486,"user_tz":0,"elapsed":6,"user":{"displayName":"Archie Goodman","userId":"05960694724077487952"}}},"execution_count":null,"outputs":[]}]}