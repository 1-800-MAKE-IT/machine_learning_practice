{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1cX2lOjHCdxCQCb5mu2Ta83cyJpI45yXA","authorship_tag":"ABX9TyNTrebyxg/Mzmlf4KZAfHWq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","\n","---\n","\n","# Intro"],"metadata":{"id":"zcLoV-sasrXj"}},{"cell_type":"markdown","source":["**Plan**: Import credit card fraud data. Use encoder only transformer network for classifying time series credit card data\n","\n","**Purpose/Intro**: Task is to develop transformer architecture proof of concept for potential application at work, detecting fraud. In a normal data science project it might be considered best practice to begin with more interpretable models first, for research purposes, but this project is solely for the purpose of assessing the viability of a transformer for this task.\n","\n","**Hypothesis**: The attention mechanism of the transformer, when combined with an appropriate positional embedding method, is able to capture both long-term and short-term dependencies in time series credit-card fraud data.\n","\n","**Methodology**: Using cross valdiation techniques on test dataset to calculate appropriate accuracy metrics (adjusting for the significant class imbalance for the dataset), with an aim to assess the viability of transformer networks for fraud classification.\n","\n"],"metadata":{"id":"m8GsCk0-ImyX"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n","# Data Sourcing and Processing\n","\n"],"metadata":{"id":"xlTtmyBAt_Wb"}},{"cell_type":"code","source":["\n","#import packages:\n","\n","import os\n","os.environ['NUMBAPRO_NVVM'] = '/usr/local/cuda/nvvm/lib64/libnvvm.so'\n","os.environ['NUMBAPRO_LIBDEVICE'] = '/usr/local/cuda/nvvm/libdevice/'\n","\n","from google.colab import drive\n","\n","try:\n","  import google.colab\n","  IN_COLAB = True\n","except:\n","  IN_COLAB = False\n","\n","if IN_COLAB:\n","  # Check if drive is mounted by looking for the mount point in the file system.\n","  # This is a more robust approach than relying on potentially internal variables.\n","  import os\n","  if not os.path.exists('/content/drive'):\n","    drive.mount('/content/drive')\n","\n","#basics\n","import os\n","from google.colab import drive\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","#table one\n","!pip install tableone\n","from tableone import TableOne\n","\n","#torch\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","#sklearn\n","from imblearn.over_sampling import RandomOverSampler\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import StratifiedKFold, GridSearchCV\n","from sklearn.metrics import roc_auc_score\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import RepeatedStratifiedKFold\n","\n","from imblearn.over_sampling import RandomOverSampler"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_jBqgg1I0MCo","executionInfo":{"status":"ok","timestamp":1733171624989,"user_tz":0,"elapsed":12726,"user":{"displayName":"Archie Goodman","userId":"05960694724077487952"}},"outputId":"9c586b30-0678-4c7e-91c9-9b5dcb040b9a"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tableone\n","  Downloading tableone-0.9.1-py3-none-any.whl.metadata (8.5 kB)\n","Requirement already satisfied: jinja2>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from tableone) (3.1.4)\n","Requirement already satisfied: numpy>=1.19.1 in /usr/local/lib/python3.10/dist-packages (from tableone) (1.26.4)\n","Requirement already satisfied: openpyxl>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from tableone) (3.1.5)\n","Requirement already satisfied: pandas>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from tableone) (2.2.2)\n","Requirement already satisfied: scipy>=1.10.1 in /usr/local/lib/python3.10/dist-packages (from tableone) (1.13.1)\n","Requirement already satisfied: statsmodels>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from tableone) (0.14.4)\n","Requirement already satisfied: tabulate>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tableone) (0.9.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=3.1.4->tableone) (3.0.2)\n","Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl>=3.1.2->tableone) (2.0.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.3->tableone) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.3->tableone) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.3->tableone) (2024.2)\n","Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.14.1->tableone) (1.0.1)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.14.1->tableone) (24.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.3->tableone) (1.16.0)\n","Downloading tableone-0.9.1-py3-none-any.whl (41 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tableone\n","Successfully installed tableone-0.9.1\n"]}]},{"cell_type":"code","source":["data_set_filepath = '/content/drive/MyDrive/Colab_Notebooks/Data/creditcard.feather'\n","\n","df = pd.read_feather(data_set_filepath)\n","\n","columns = df.columns.tolist()\n","\n","print(f\"The dataset lenghth is {str(len(df))}\")\n","print(f\"The number of columns is {str(len(columns))}\")\n","print(f\"The column names are {str(columns)}\")\n","df.head(10)\n","\n","table1 = TableOne(df, columns=columns, groupby= 'class', pval=True)\n","print(table1)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"pHk36Lkm6cUT","executionInfo":{"status":"error","timestamp":1733254997320,"user_tz":0,"elapsed":716,"user":{"displayName":"Archie Goodman","userId":"05960694724077487952"}},"outputId":"f838872b-4be9-42aa-af7a-6d16277d5ce5"},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'pd' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-ad6a8a356a54>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_set_filepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Colab_Notebooks/Data/creditcard.feather'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_feather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_set_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"]}]},{"cell_type":"markdown","source":["\n","\n","---\n","# Transformer Model\n","\n"],"metadata":{"id":"waNhrbSAsk_6"}},{"cell_type":"markdown","source":["**No HP Tuning**: First we will implement our model without HP tuning and try to overfit, to just prove that we have the generalization power, and just check that we can actually set up and run the architecture"],"metadata":{"id":"WSxQ_mLEsXTr"}},{"cell_type":"code","source":["\n","# **Set device for GPU acceleration**\n","# If CUDA (NVIDIA GPU) is available, computations will use it. Otherwise, it defaults to CPU.\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","#get data\n","data = df\n","\n","# Separate features (input data) and labels (output/target)\n","X = data.iloc[:, :-1].values\n","y = data.iloc[:, -1].values\n","\n","# **Normalize features**\n","# Standardize the `V1` to `V28` columns (zero mean, unit variance) for numerical stability.\n","# Log-transform the `Amount` column to reduce the effect of large values.\n","scaler = StandardScaler()\n","X[:, :-1] = scaler.fit_transform(X[:, :-1])  # Standardize PCA-transformed features\n","X[:, -1] = np.log1p(X[:, -1])  # Apply log(1 + x) to the 'Amount' column to normalize it\n","\n","# **Split the data into training, validation, and test sets**\n","X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n","X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n","\n","# **Convert data to PyTorch tensors**\n","train_data = TensorDataset(torch.tensor(X_train, dtype=torch.float32).to(device),\n","                           torch.tensor(y_train, dtype=torch.float32).to(device))\n","val_data = TensorDataset(torch.tensor(X_val, dtype=torch.float32).to(device),\n","                         torch.tensor(y_val, dtype=torch.float32).to(device))\n","test_data = TensorDataset(torch.tensor(X_test, dtype=torch.float32).to(device),\n","                          torch.tensor(y_test, dtype=torch.float32).to(device))\n","\n","# **DataLoader for batching**\n","batch_size = 64\n","train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_data, batch_size=batch_size)\n","test_loader = DataLoader(test_data, batch_size=batch_size)\n","\n","# **2. Custom Positional Embedding Layer (from the book)**\n","\n","class PositionalEmbedding(nn.Module):\n","    def __init__(self, sequence_length, input_dim, output_dim):\n","        super(PositionalEmbedding, self).__init__()\n","        # Token embedding: maps each input feature into a higher-dimensional space\n","        self.token_embeddings = nn.Embedding(input_dim, output_dim)\n","        # Position embedding: maps each position in the sequence to a higher-dimensional space\n","        self.position_embeddings = nn.Embedding(sequence_length, output_dim)\n","        self.sequence_length = sequence_length\n","\n","    def forward(self, x):\n","        # Generate positions (0, 1, ..., sequence_length - 1)\n","        positions = torch.arange(0, x.size(1), device=x.device).unsqueeze(0)\n","        # Lookup embeddings for tokens and positions, then add them\n","        embedded_tokens = self.token_embeddings(x)\n","        embedded_positions = self.position_embeddings(positions)\n","        return embedded_tokens + embedded_positions\n","\n","# **3. Define the Transformer Model with Custom Positional Embedding**\n","\n","class FraudDetectionTransformer(nn.Module):\n","    def __init__(self, input_dim, embed_dim, num_heads, ff_dim, num_layers, sequence_length):\n","        super(FraudDetectionTransformer, self).__init__()\n","        # Positional Embedding Layer (from the book)\n","        self.positional_embedding = PositionalEmbedding(sequence_length, input_dim, embed_dim)\n","\n","        # Transformer Encoder Layer (using pre-built PyTorch functionality)\n","        self.encoder_layer = nn.TransformerEncoderLayer(\n","            d_model=embed_dim,  # Dimension of the embedding space\n","            nhead=num_heads,  # Number of attention heads\n","            dim_feedforward=ff_dim,  # Dimension of the feed-forward layer\n","            dropout=0.1  # Dropout for regularization\n","        )\n","        # Stack multiple encoder layers\n","        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n","\n","        # Global Pooling Layer\n","        self.pooling = nn.AdaptiveAvgPool1d(1)\n","\n","        # Fully Connected Output Layer\n","        self.fc = nn.Linear(embed_dim, 1)\n","\n","        # Sigmoid Activation\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        # Get positional embeddings and add them to the token embeddings\n","        x = self.positional_embedding(x)\n","\n","        # Transpose the input to match the shape expected by the transformer (sequence, batch, feature)\n","        x = x.permute(1, 0, 2)  # (batch, seq_len, feature) -> (seq_len, batch, feature)\n","\n","        # Pass through the transformer encoder\n","        x = self.transformer_encoder(x)\n","\n","        # Apply global pooling (average over the sequence length)\n","        x = self.pooling(x.permute(1, 2, 0))  # (seq_len, batch, feature) -> (batch, feature, 1)\n","\n","        # Flatten the output to match the input of the fully connected layer\n","        x = x.view(x.size(0), -1)\n","\n","        # Final classification layer\n","        x = self.fc(x)\n","\n","        # Sigmoid for binary classification (fraud or not)\n","        x = self.sigmoid(x)\n","\n","        return x\n","\n","# **4. Instantiate the Model**\n","input_dim = X_train.shape[1]  # Number of input features\n","embed_dim = 256  # Dimensionality of embedding space\n","num_heads = 4  # Number of attention heads\n","ff_dim = 512  # Feed-forward layer dimension\n","num_layers = 4  # Number of transformer layers\n","sequence_length = X_train.shape[1]  # Sequence length is the number of input features\n","\n","model = FraudDetectionTransformer(input_dim, embed_dim, num_heads, ff_dim, num_layers, sequence_length).to(device)\n","\n","# **5. Training Setup**\n","criterion = nn.BCELoss()  # Binary Cross Entropy loss for binary classification\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","\n","# **6. Training Loop**\n","def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10):\n","    model.train()  # Set model to training mode\n","\n","    for epoch in range(epochs):\n","        running_loss = 0.0\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()  # Zero the gradients\n","\n","            # Forward pass\n","            outputs = model(inputs)\n","\n","            # Calculate the loss\n","            loss = criterion(outputs.squeeze(), labels)\n","\n","            # Backward pass and optimization\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","\n","        # Print training loss for every epoch\n","        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}\")\n","\n","        # Optionally, validate the model performance after each epoch (on the validation set)\n","        validate_model(model, val_loader, criterion)\n","\n","# **7. Validation Function**\n","def validate_model(model, val_loader, criterion):\n","    model.eval()  # Set model to evaluation mode\n","\n","    with torch.no_grad():\n","        total_loss = 0\n","        correct = 0\n","        total = 0\n","        for inputs, labels in val_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            # Forward pass\n","            outputs = model(inputs)\n","\n","            # Calculate the loss\n","            loss = criterion(outputs.squeeze(), labels)\n","            total_loss += loss.item()\n","\n","            # Calculate accuracy\n","            predicted = (outputs.squeeze() > 0.5).float()\n","            correct += (predicted == labels).sum().item()\n","            total += labels.size(0)\n","\n","        print(f\"Validation Loss: {total_loss/len(val_loader)}, Accuracy: {correct/total * 100}%\")\n","\n","# **8. Start Training**\n","train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10)\n","\n","# **9. Testing the Model**\n","def test_model(model, test_loader):\n","    model.eval()  # Set model to evaluation mode\n","    with torch.no_grad():\n","        correct = 0\n","        total = 0\n","        for inputs, labels in test_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            # Forward pass\n","            outputs = model(inputs)\n","\n","            # Calculate accuracy\n","            predicted = (outputs.squeeze() > 0.5).float()\n","            correct += (predicted == labels).sum().item()\n","            total += labels.size(0)\n","\n","        print(f\"Test Accuracy: {correct/total * 100}%\")\n","\n","# **10. Evaluate on the Test Set**\n","test_model(model, test_loader)\n"],"metadata":{"id":"9evuMo2oImJG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import datetime\n","import os\n","\n","# Navigate to the repository directory (if not already there)\n","%cd /content/drive/MyDrive/Colab_Notebooks/Deep_Learning_Practice\n","\n","with open('/content/drive/MyDrive/IAM/PAT.txt', 'r') as file:\n","      github_pat = file.read().strip()\n","os.environ['GITHUB_PAT'] = github_pat\n","\n","!git remote add origin \"https://github.com/archiegoodman2/machine_learning_practice\"\n","\n","# Replace with your actual username and email (or configure globally)\n","USERNAME=\"archiegoodman2\"\n","EMAIL=\"archiegoodman2011@gmail.com\"\n","\n","# Set global username and email configuration\n","!git config --global user.name \"$USERNAME\"\n","!git config --global user.email \"$EMAIL\"\n","\n","now = datetime.datetime.now()\n","current_datetime = now.strftime(\"%Y-%m-%d %H:%M\")\n","\n","# Set remote URL using the PAT from environment variable\n","!git remote set-url origin https://{os.environ['GITHUB_PAT']}@github.com/archiegoodman2/machine_learning_practice.git\n","\n","# Replace with your desired commit message\n","COMMIT_MESSAGE = str(current_datetime) + \" commit \"\n","\n","# Stage all changes\n","!git add .\n","\n","# Commit the changes\n","!git commit -m \"$COMMIT_MESSAGE\"\n","\n","# Push to origin\n","!git push origin master\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xWxH5PbcOdSb","outputId":"0683680f-8222-4b44-d03a-8e35ce34a687"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab_Notebooks/Deep_Learning_Practice\n","error: remote origin already exists.\n"]}]}]}