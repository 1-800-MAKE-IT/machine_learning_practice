{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1cX2lOjHCdxCQCb5mu2Ta83cyJpI45yXA","authorship_tag":"ABX9TyM4z82J7uGeGF4W36puSPI+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# TO DO\n","\n","1. finish objective function - look at code on how to call dataloaders and our custom classes on chatgpt\n","2. get it to plot train loss vs val loss on best model???\n","3. get it to spit out accuracy, hit rate, f1 and ROC AUC. get it to plot ROC AUC graph??\n","4. get it to output best HP combo\n","5. check paper for other tips and tricks they used.\n","6. look into Cell-based Architectures: Explore more structured approaches for dynamic architectures, such as cell-based architectures (like those used in NASNet or EfficientNet), which offer a balance between flexibility and control"],"metadata":{"id":"z_-I-6HM6dbi"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n","# Intro"],"metadata":{"id":"zcLoV-sasrXj"}},{"cell_type":"markdown","source":["**Plan**: Import credit card fraud data. Use encoder only transformer network for classifying time series credit card data\n","\n","**Purpose/Intro**: Task is to develop transformer architecture proof of concept for potential application at work, detecting fraud. In a normal data science project it might be considered best practice to begin with more interpretable models first, for research purposes, but this project is solely for the purpose of assessing the viability of a transformer for this task.\n","\n","**Hypothesis**: The attention mechanism of the transformer, when combined with an appropriate positional embedding method, is able to capture both long-term and short-term dependencies in time series credit-card fraud data.\n","\n","**Methodology**: Using cross valdiation techniques on test dataset to calculate appropriate accuracy metrics (adjusting for the significant class imbalance for the dataset), with an aim to assess the viability of transformer networks for fraud classification.\n","\n","Credit to the below paper, **referred to as Source 1**, for the methodology design: Yu, C., Xu, Y., Cao, J., Zhang, Y., Jin, Y. and Zhu, M. (2024) 'Credit Card Fraud Detection Using Advanced Transformer Model', arXiv preprint arXiv:2406.03733. Available at: https://arxiv.org/abs/2406.03733 (Accessed: 18 December 2024)\n","\n","This paper has demonstrated the utility for the transformer that we are about to create, by comparing the methodology with various other shallow learning techniques. In future projects I aim to validate this myself.\n","\n"],"metadata":{"id":"m8GsCk0-ImyX"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n","# Data Sourcing and Package loading\n","\n"],"metadata":{"id":"xlTtmyBAt_Wb"}},{"cell_type":"code","source":["\n","#import packages:\n","\n","import os\n","os.environ['NUMBAPRO_NVVM'] = '/usr/local/cuda/nvvm/lib64/libnvvm.so'\n","os.environ['NUMBAPRO_LIBDEVICE'] = '/usr/local/cuda/nvvm/libdevice/'\n","\n","from google.colab import drive\n","\n","try:\n","  import google.colab\n","  IN_COLAB = True\n","except:\n","  IN_COLAB = False\n","\n","if IN_COLAB:\n","  # Check if drive is mounted by looking for the mount point in the file system.\n","  import os\n","  if not os.path.exists('/content/drive'):\n","    drive.mount('/content/drive')\n","\n","#basics\n","import os\n","from google.colab import drive\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","!pip install optuna\n","import optuna\n","\n","\n","#table one\n","!pip install tableone\n","from tableone import TableOne\n","\n","#torch\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader, TensorDataset\n","\n","\n","#sklearn\n","from imblearn.over_sampling import RandomOverSampler\n","from imblearn.over_sampling import SMOTE\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import StratifiedKFold, GridSearchCV\n","from sklearn.metrics import roc_auc_score\n","from sklearn.model_selection import cross_val_score, train_test_split, RepeatedStratifiedKFold, KFold\n","from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n","from sklearn.preprocessing import MinMaxScaler\n","\n","from imblearn.over_sampling import RandomOverSampler"],"metadata":{"id":"_jBqgg1I0MCo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_set_filepath = '/content/drive/MyDrive/Colab_Notebooks/Data/creditcard.feather'\n","\n","df = pd.read_feather(data_set_filepath)\n","\n","columns = df.columns.tolist()\n","\n","print(f\"The dataset lenghth is {str(len(df))}\")\n","print(f\"The number of columns is {str(len(columns))}\")\n","print(f\"The column names are {str(columns)}\")\n","df.head(10)\n","\n","table1 = TableOne(df, columns=columns, groupby= 'Class', pval=True)\n","print(table1)\n","\n","data = df\n","\n","\n","\n"],"metadata":{"id":"pHk36Lkm6cUT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","# Data loading and preprocessing:\n","In Source 1 (listed above), it was found that there are performance boosts associated with removing outliers, as it may help with overfitting. This will be done on the training data only. This is to prevent information leakage from our training set.\n","\n","The source also suggests there is value in oversampling the minority class. This may be due to the unique challenges of such a large class imbalance. This will be done on the training data only. This is to prevent information leakage from our training set.\n","\n","In addition, we will min-max scale our validation and training sets, and apply this same scaling to the test data."],"metadata":{"id":"C94hJDCHuNz7"}},{"cell_type":"code","source":["# **Set device for GPU acceleration**\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# **Error warning if no GPU is detected**\n","if device.type != 'cuda':\n","    print(\"WARNING: GPU is not available. The model will run on the CPU, which might be slower.\")\n","else:\n","    print(\"Cuda setup successful\")\n","X = data.iloc[:, :-1]  # Features (all columns except the last one)\n","y = data.iloc[:, -1]   # Labels (the last column)\n","\n","# Data Preprocessing Transformation Class\n","class DataPreprocessingTransform:\n","    def __init__(self):\n","        self.scaler = MinMaxScaler()\n","\n","    def fit_transform(self, X_train):\n","        \"\"\"\n","        Fit scaler on the training set and transform it.\n","        \"\"\"\n","        return pd.DataFrame(self.scaler.fit_transform(X_train), columns=X_train.columns)\n","\n","    def transform(self, X):\n","        \"\"\"\n","        Apply the scaling transformation based on the training set scaling.\n","        \"\"\"\n","        return pd.DataFrame(self.scaler.transform(X), columns=X.columns)\n","\n","\n","# Custom PyTorch Dataset\n","class CustomDataset(Dataset):\n","    def __init__(self, X, y, transform=None, resample=False):\n","        \"\"\"\n","        Custom dataset to handle transformations and optional resampling.\n","\n","        Parameters:\n","        - X: Features (pandas DataFrame)\n","        - y: Labels (pandas Series)\n","        - transform: Transformation function for features\n","        - resample: Whether to apply SMOTE for resampling the minority class\n","        \"\"\"\n","        self.transform = transform\n","\n","        if resample:\n","            X, y = self._apply_smote(X, y)\n","\n","        self.X = X.reset_index(drop=True)  # Ensure consistent indexing\n","        self.y = y.reset_index(drop=True)\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, index):\n","        sample_X = self.X.iloc[index]\n","        sample_y = self.y.iloc[index]\n","\n","        if self.transform:\n","            sample_X = self.transform(sample_X)\n","\n","        return sample_X.values, sample_y\n","\n","    def _apply_smote(self, X, y):\n","        \"\"\"\n","        Applies SMOTE to the data to resample the minority class.\n","        \"\"\"\n","        smote = SMOTE(sampling_strategy='auto', random_state=42)\n","        X_resampled, y_resampled = smote.fit_resample(X, y)\n","        return pd.DataFrame(X_resampled, columns=X.columns), pd.Series(y_resampled)\n","\n","\n","# Function to remove outliers using IQR\n","def remove_outliers(X):\n","    \"\"\"\n","    Removes outliers from the dataset based on the IQR method.\n","    \"\"\"\n","    Q1 = X.quantile(0.25)\n","    Q3 = X.quantile(0.75)\n","    IQR = Q3 - Q1\n","    lower_bound = Q1 - 1.5 * IQR\n","    upper_bound = Q3 + 1.5 * IQR\n","    X_filtered = X[(X >= lower_bound) & (X <= upper_bound)]\n","    return X_filtered\n","\n"],"metadata":{"id":"tqGSD-BjuSvr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","# Transformer Model\n","\n"],"metadata":{"id":"waNhrbSAsk_6"}},{"cell_type":"markdown","source":["We are going to implement the transformer and optimise the hyperparameters using the Optuna package. As per the paper listed above, we will resample from the minority class when training."],"metadata":{"id":"WSxQ_mLEsXTr"}},{"cell_type":"code","source":["\n","# **Transformer Model with Batch Normalization and Residual Connections**\n","class FraudDetectionTransformer(nn.Module):\n","\n","      #init method dynamically builds model architecture\n","      def __init__(self, input_dim, embed_dim, num_heads, ff_dim_base, dropout, ff_dropout, activation_function, num_ff_layers, use_batchnorm, use_layernorm):\n","          super(FraudDetectionTransformer, self).__init__()\n","\n","          self.embedding = nn.Linear(input_dim, embed_dim)  # Embedding layer\n","          self.use_batchnorm = use_batchnorm # Store whether to use batchnorm\n","\n","          self.batch_norm = nn.BatchNorm1d(input_dim) if self.use_batchnorm else None # Batch Normalization before embedding\n","\n","          # Store the activation function\n","          self.activation_function = activation_function\n","\n","          # Multi-head Attention Layer\n","          self.multihead_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n","\n","          # Dynamically create feedforward network in __init__\n","          ff_network = []\n","          for i in range(num_ff_layers):\n","              ff_dim = ff_dim_base * (2 ** i)\n","              ff_network.extend([\n","                  nn.Linear(embed_dim if i == 0 else ff_dim // 2, ff_dim),\n","                  self.get_activation_function(),\n","                  nn.Dropout(ff_dropout)\n","              ])\n","\n","          self.ff_network = nn.Sequential(*ff_network)  # Store the created network\n","\n","          # Layer Normalization after the dynamic feedforward network\n","          self.layer_norm = nn.LayerNorm(embed_dim) if use_layernorm else None\n","\n","          self.pooling = nn.AdaptiveAvgPool1d(1)  # Global average pooling\n","          self.fc = nn.Linear(embed_dim, 1)  # Fully connected layer for classification\n","          self.sigmoid = nn.Sigmoid()  # Sigmoid activation for binary classification\n","\n","\n","      def forward(self, x):\n","          # Apply batch normalization if specified\n","          if self.use_batchnorm:\n","              x = self.batch_norm(x)\n","\n","          # Apply embedding layer\n","          embedded_x = self.embedding(x)\n","\n","          # Add sequence dimension for the transformer\n","          embedded_x = embedded_x.unsqueeze(1)\n","\n","          # Transpose for multi-head attention (sequence_length, batch_size, embed_dim)\n","          embedded_x = embedded_x.permute(1, 0, 2)\n","\n","          # Pass through multi-head attention and add residual connection\n","          attn_output, _ = self.multihead_attn(embedded_x, embedded_x, embedded_x)\n","          attn_output = attn_output + embedded_x\n","\n","          # Pass through feedforward layers with residual connection\n","          transformer_output = self.ff_network(attn_output.permute(1, 0, 2))  # (batch_size, sequence_length, embed_dim)\n","          transformer_output = transformer_output + attn_output.permute(1, 0, 2)\n","\n","          # Layer normalization if specified\n","          if self.layer_norm:\n","              transformer_output = self.layer_norm(transformer_output)\n","\n","          # Global pooling over the sequence length dimension\n","          pooled_output = self.pooling(transformer_output.permute(0, 2, 1)).squeeze()\n","\n","          # Final fully connected layer and sigmoid activation\n","          fc_output = self.fc(pooled_output)\n","          return self.sigmoid(fc_output)\n","\n","\n","# **Training Function with Early Stopping**\n","def train_model_with_early_stopping(model, train_loader, val_loader, epochs, lr, patience):\n","\n","    model = model.to(device) #switch to GPU\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr) #select Adam for optimization\n","    criterion = nn.BCELoss() #set loss funct\n","\n","    #history\n","    train_losses = []\n","    val_losses = []\n","    best_val_loss = float('inf')\n","    patience_counter = 0\n","    best_model = None\n","\n","\n","    for epoch in range(epochs):\n","        model.train() #set model to training mode\n","        train_loss = 0.0\n","\n","        for inputs, labels in train_loader:\n","            optimizer.zero_grad()\n","            outputs = model(inputs).squeeze()\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item()\n","        train_losses.append(train_loss / len(train_loader))\n","\n","        # Validation phase\n","        model.eval()\n","        val_loss = 0.0\n","\n","        with torch.no_grad(): #no gradient needed for val phase\n","            for inputs, labels in val_loader: #unpack data\n","                outputs = model(inputs).squeeze() #squeeze removes unecessary dimensions of tensors e.g. [batch_size,1] -> [batch_size]\n","                loss = criterion(outputs, labels)\n","                val_loss += loss.item()\n","        val_losses.append(val_loss / len(val_loader)) #avg loss\n","\n","        # Early stopping check\n","        if val_losses[-1] < best_val_loss:\n","            best_val_loss = val_losses[-1]\n","            best_model = model.state_dict()  # Save the best model\n","            patience_counter = 0\n","        else:\n","            patience_counter += 1\n","            if patience_counter >= patience:\n","                print(\"Early stopping triggered.\")\n","                break\n","\n","        print(f\"Epoch {epoch + 1}/{epochs} - Train Loss: {train_losses[-1]:.4f} - Val Loss: {val_losses[-1]:.4f}\")\n","\n","    model.load_state_dict(best_model)  # Load the best model before returning\n","    return train_losses, val_losses, best_model\n","\n","# **Validation Metrics Calculation**\n","def evaluate(model, data_loader):\n","    model.eval() #set to eval mode\n","    y_true = []\n","    y_pred = []\n","\n","    with torch.no_grad():\n","        for inputs, labels in data_loader:\n","            outputs = model(inputs).squeeze()\n","            y_true.extend(labels.cpu().numpy()) #extend is like appending for multiple elements at once\n","            y_pred.extend(outputs.cpu().numpy())\n","\n","    # Convert predictions to binary format for accuracy and hit rate\n","    y_pred_binary = [1 if pred >= 0.5 else 0 for pred in y_pred]\n","\n","    # Metrics\n","    accuracy = accuracy_score(y_true, y_pred_binary)\n","    hit_rate = np.sum(np.logical_and(np.array(y_true) == 1, np.array(y_pred_binary) == 1)) / np.sum(np.array(y_true) == 1)\n","    roc_auc = roc_auc_score(y_true, y_pred)\n","\n","    return accuracy, hit_rate, roc_auc\n","\n","# **Plotting Function**\n","def plot_losses(train_losses, val_losses):\n","    plt.figure(figsize=(10, 6))\n","    plt.plot(train_losses, label=\"Training Loss\")\n","    plt.plot(val_losses, label=\"Validation Loss\")\n","    plt.xlabel(\"Epochs\")\n","    plt.ylabel(\"Loss\")\n","    plt.title(\"Training and Validation Loss\")\n","    plt.legend()\n","    plt.show()\n","import torch\n","import optuna\n","from sklearn.model_selection import KFold\n","from torch.utils.data import DataLoader\n","import torch.nn as nn\n","\n","# Modify the objective function to save the best model\n","def objective(trial):\n","    # Define hyperparameters to tune\n","    num_heads = trial.suggest_int(\"num_heads\", low=4, high=8, step=2)\n","    embed_dim = trial.suggest_int(\"embed_dim\", low=64, high=512, step=num_heads)\n","    ff_dim_base = trial.suggest_int(\"ff_dim_base\", low=64, high=512, step=64)\n","    dropout = trial.suggest_float(\"dropout\", low=0.1, high=0.5, step=0.1)\n","    ff_dropout = trial.suggest_float(\"ff_dropout\", low=0.1, high=0.5, step=0.1)\n","    activation_function = trial.suggest_categorical(\"activation_function\", [\"relu\", \"tanh\", \"sigmoid\"])\n","    num_ff_layers = trial.suggest_int(\"num_ff_layers\", low=1, high=3, step=1)\n","    use_batchnorm = trial.suggest_categorical(\"use_batchnorm\", [True, False])\n","    use_layernorm = trial.suggest_categorical(\"use_layernorm\", [True, False])\n","    batch_size = trial.suggest_int(\"batch_size\", 64, 512, step=64)\n","\n","    # Print the hyperparameters being trialed\n","    print(f\"Trial {trial.number}:\")\n","    print(f\"  num_heads: {num_heads}\")\n","    print(f\"  embed_dim: {embed_dim}\")\n","    print(f\"  ff_dim_base: {ff_dim_base}\")\n","    print(f\"  dropout: {dropout}\")\n","    print(f\"  ff_dropout: {ff_dropout}\")\n","    print(f\"  activation_function: {activation_function}\")\n","    print(f\"  num_ff_layers: {num_ff_layers}\")\n","    print(f\"  use_batchnorm: {use_batchnorm}\")\n","    print(f\"  use_layernorm: {use_layernorm}\")\n","    print(f\"  batch_size: {batch_size}\")\n","    print(\"-\" * 50)\n","\n","    # K-Fold Cross Validation\n","    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","    fold_scores = []\n","    best_val_score = -float('inf')\n","    best_model_state_dict = None\n","\n","    for train_index, val_index in kf.split(X):\n","        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n","        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n","\n","        # Preprocess the data\n","        transform = DataPreprocessingTransform()\n","        X_train_scaled = transform.fit_transform(X_train)\n","        X_val_scaled = transform.transform(X_val)\n","\n","        # Remove outliers\n","        X_train_filtered = X_train_scaled.apply(remove_outliers)\n","        X_val_filtered = X_val_scaled.apply(remove_outliers)\n","\n","        # Create PyTorch datasets\n","        train_dataset = CustomDataset(X_train_filtered, y_train, resample=True)\n","        val_dataset = CustomDataset(X_val_filtered, y_val)\n","\n","        # Create DataLoaders\n","        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","\n","        # Initialize the model, loss function, and optimizer\n","        model = FraudDetectionTransformer(\n","            input_dim=input_dim,\n","            embed_dim=embed_dim,\n","            num_heads=num_heads,\n","            ff_dim_base=ff_dim_base,\n","            dropout=dropout,\n","            ff_dropout=ff_dropout,\n","            activation_function=activation_function,\n","            num_ff_layers=num_ff_layers,\n","            use_batchnorm=use_batchnorm,\n","            use_layernorm=use_layernorm\n","        ).to(device)\n","\n","        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","        criterion = nn.BCELoss()\n","\n","        # Train the model\n","        train_model(model, train_loader, val_loader, optimizer, criterion)\n","        val_score = evaluate_model(model, val_loader)\n","\n","        # Keep track of the best model\n","        if val_score > best_val_score:\n","            best_val_score = val_score\n","            best_model_state_dict = model.state_dict()\n","\n","        fold_scores.append(val_score)\n","\n","    avg_fold_score = sum(fold_scores) / len(fold_scores)\n","\n","    # Save the best model after all folds\n","    if best_model_state_dict is not None:\n","        torch.save(best_model_state_dict, 'best_model.pth')\n","\n","    return avg_fold_score\n","\n","\n","# **Validation Metrics**\n","accuracy, hit_rate, roc_auc = evaluate(model, val_loader)\n","print(f\"Validation Accuracy: {accuracy:.4f}\")\n","print(f\"Validation Hit Rate: {hit_rate:.4f}\")\n","print(f\"Validation ROC AUC: {roc_auc:.4f}\")\n","\n","\n","#=============================================== Test on Test Set ===============================================#\n","\n","# Load the best model\n","best_model = FraudDetectionTransformer(\n","    input_dim=input_dim,\n","    embed_dim=best_embed_dim,  # Use the best values from the Optuna study\n","    num_heads=best_num_heads,\n","    ff_dim_base=best_ff_dim_base,\n","    dropout=best_dropout,\n","    ff_dropout=best_ff_dropout,\n","    activation_function=best_activation_function,\n","    num_ff_layers=best_num_ff_layers,\n","    use_batchnorm=best_use_batchnorm,\n","    use_layernorm=best_use_layernorm\n",").to(device)\n","\n","best_model.load_state_dict(torch.load('best_model.pth'))\n","\n","# Evaluate on test set\n","test_loader = DataLoader(test_dataset, batch_size=32)\n","test_accuracy, test_hit_rate, test_roc_auc = evaluate(best_model, test_loader)\n","print(f\"Test Accuracy: {test_accuracy:.4f}\")\n","print(f\"Test Hit Rate: {test_hit_rate:.4f}\")\n","print(f\"Test ROC AUC: {test_roc_auc:.4f}\")\n"],"metadata":{"id":"9evuMo2oImJG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","# Auto commit to github"],"metadata":{"id":"y0JuVWk0R5k6"}},{"cell_type":"code","source":["import datetime\n","import os\n","\n","# Navigate to the repository directory (if not already there)\n","%cd /content/drive/MyDrive/Colab_Notebooks/Deep_Learning_Practice\n","\n","with open('/content/drive/MyDrive/IAM/PAT.txt', 'r') as file:\n","      github_pat = file.read().strip()\n","os.environ['GITHUB_PAT'] = github_pat\n","\n","!git remote add origin \"https://github.com/archiegoodman2/machine_learning_practice\"\n","\n","# Replace with your actual username and email (or configure globally)\n","USERNAME=\"archiegoodman2\"\n","EMAIL=\"archiegoodman2011@gmail.com\"\n","\n","# Set global username and email configuration\n","!git config --global user.name \"$USERNAME\"\n","!git config --global user.email \"$EMAIL\"\n","\n","now = datetime.datetime.now()\n","current_datetime = now.strftime(\"%Y-%m-%d %H:%M\")\n","\n","# Set remote URL using the PAT from environment variable\n","!git remote set-url origin https://{os.environ['GITHUB_PAT']}@github.com/archiegoodman2/machine_learning_practice.git\n","\n","# Replace with your desired commit message\n","COMMIT_MESSAGE = str(current_datetime) + \" \" + \" began code for HP tuning \"\n","\n","# Stage all changes\n","!git add .\n","\n","# Commit the changes\n","!git commit -m \"$COMMIT_MESSAGE\"\n","\n","# Push to origin\n","!git push origin master\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xWxH5PbcOdSb","outputId":"fc0787b5-03bf-4d7f-aebc-e965cac622f2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab_Notebooks/Deep_Learning_Practice\n","error: remote origin already exists.\n"]}]}]}