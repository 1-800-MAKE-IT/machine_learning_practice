{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1cX2lOjHCdxCQCb5mu2Ta83cyJpI45yXA","authorship_tag":"ABX9TyM1gqZkNU1j5zwd+53CUSws"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","\n","---\n","\n","# Intro"],"metadata":{"id":"zcLoV-sasrXj"}},{"cell_type":"markdown","source":["**Plan**: Import credit card fraud data. Use encoder only transformer network for classifying time series credit card data\n","\n","**Purpose/Intro**: Task is to develop transformer architecture proof of concept for potential application at work, detecting fraud. In a normal data science project it might be considered best practice to begin with more interpretable models first, for research purposes, but this project is solely for the purpose of assessing the viability of a transformer for this task.\n","\n","**Hypothesis**: The attention mechanism of the transformer, when combined with an appropriate positional embedding method, is able to capture both long-term and short-term dependencies in time series credit-card fraud data.\n","\n","**Methodology**: Using cross valdiation techniques on test dataset to calculate appropriate accuracy metrics (adjusting for the significant class imbalance for the dataset), with an aim to assess the viability of transformer networks for fraud classification.\n","\n"],"metadata":{"id":"m8GsCk0-ImyX"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n","# Data Sourcing and Processing\n","\n"],"metadata":{"id":"xlTtmyBAt_Wb"}},{"cell_type":"code","source":["\n","#import packages:\n","\n","import os\n","os.environ['NUMBAPRO_NVVM'] = '/usr/local/cuda/nvvm/lib64/libnvvm.so'\n","os.environ['NUMBAPRO_LIBDEVICE'] = '/usr/local/cuda/nvvm/libdevice/'\n","\n","from google.colab import drive\n","\n","try:\n","  import google.colab\n","  IN_COLAB = True\n","except:\n","  IN_COLAB = False\n","\n","if IN_COLAB:\n","  # Check if drive is mounted by looking for the mount point in the file system.\n","  # This is a more robust approach than relying on potentially internal variables.\n","  import os\n","  if not os.path.exists('/content/drive'):\n","    drive.mount('/content/drive')\n","\n","#basics\n","import os\n","from google.colab import drive\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","#table one\n","!pip install tableone\n","from tableone import TableOne\n","\n","#torch\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","#sklearn\n","from imblearn.over_sampling import RandomOverSampler\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import StratifiedKFold, GridSearchCV\n","from sklearn.metrics import roc_auc_score\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import RepeatedStratifiedKFold\n","\n","from imblearn.over_sampling import RandomOverSampler"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_jBqgg1I0MCo","executionInfo":{"status":"ok","timestamp":1733257959262,"user_tz":0,"elapsed":12466,"user":{"displayName":"Archie Goodman","userId":"05960694724077487952"}},"outputId":"f79ec0b9-c354-419e-e0b4-3a97e29153ae"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tableone\n","  Downloading tableone-0.9.1-py3-none-any.whl.metadata (8.5 kB)\n","Requirement already satisfied: jinja2>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from tableone) (3.1.4)\n","Requirement already satisfied: numpy>=1.19.1 in /usr/local/lib/python3.10/dist-packages (from tableone) (1.26.4)\n","Requirement already satisfied: openpyxl>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from tableone) (3.1.5)\n","Requirement already satisfied: pandas>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from tableone) (2.2.2)\n","Requirement already satisfied: scipy>=1.10.1 in /usr/local/lib/python3.10/dist-packages (from tableone) (1.13.1)\n","Requirement already satisfied: statsmodels>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from tableone) (0.14.4)\n","Requirement already satisfied: tabulate>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tableone) (0.9.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=3.1.4->tableone) (3.0.2)\n","Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl>=3.1.2->tableone) (2.0.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.3->tableone) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.3->tableone) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.3->tableone) (2024.2)\n","Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.14.1->tableone) (1.0.1)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.14.1->tableone) (24.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.3->tableone) (1.16.0)\n","Downloading tableone-0.9.1-py3-none-any.whl (41 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tableone\n","Successfully installed tableone-0.9.1\n"]}]},{"cell_type":"code","source":["data_set_filepath = '/content/drive/MyDrive/Colab_Notebooks/Data/creditcard.feather'\n","\n","df = pd.read_feather(data_set_filepath)\n","\n","columns = df.columns.tolist()\n","\n","print(f\"The dataset lenghth is {str(len(df))}\")\n","print(f\"The number of columns is {str(len(columns))}\")\n","print(f\"The column names are {str(columns)}\")\n","df.head(10)\n","\n","table1 = TableOne(df, columns=columns, groupby= 'Class', pval=True)\n","print(table1)\n","\n","data = df\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":480},"id":"pHk36Lkm6cUT","executionInfo":{"status":"error","timestamp":1733258860561,"user_tz":0,"elapsed":7827,"user":{"displayName":"Archie Goodman","userId":"05960694724077487952"}},"outputId":"4d85204a-548c-4c4d-d252-dd953526aa8a"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["The dataset lenghth is 284807\n","The number of columns is 31\n","The column names are ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount', 'Class']\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-d589acc12852>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtable1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTableOne\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroupby\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'Class'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tableone/tableone.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, columns, categorical, continuous, groupby, nonnormal, min_max, pval, pval_adjust, htest_name, pval_test_name, htest, isnull, missing, ddof, labels, rename, sort, limit, order, remarks, label_suffix, decimals, smd, overall, row_percent, display_all, dip_test, normal_test, tukey_test, pval_threshold, include_null)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;31m# Create all intermediate tables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_intermediate_tables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;31m# Assemble Table 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tableone/tableone.py\u001b[0m in \u001b[0;36mcreate_intermediate_tables\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;31m# forgive me jraffa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pval\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m             self.htest_table = self.tables.create_htest_table(data, self._continuous, self._categorical,\n\u001b[0m\u001b[1;32m    359\u001b[0m                                                               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nonnormal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_groupby\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m                                                               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_groupbylvls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_htest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tableone/tables.py\u001b[0m in \u001b[0;36mcreate_htest_table\u001b[0;34m(self, data, continuous, categorical, nonnormal, groupby, groupbylvls, htest, pval, pval_adjust)\u001b[0m\n\u001b[1;32m     70\u001b[0m                     \u001b[0mlvl_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                     \u001b[0;31m# coerce to numeric and drop non-numeric data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                     lvl_data = lvl_data.apply(pd.to_numeric,\n\u001b[0m\u001b[1;32m     73\u001b[0m                                               errors='coerce').dropna()\n\u001b[1;32m     74\u001b[0m                     \u001b[0;31m# append to overall group data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4922\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4923\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4924\u001b[0;31m         ).apply()\n\u001b[0m\u001b[1;32m   4925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4926\u001b[0m     def _reindex_indexer(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0;31m# self.func is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;31m#  Categorical (GH51645).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m         mapped = obj._map_values(\n\u001b[0m\u001b[1;32m   1508\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         return lib.map_infer_mask(\n","\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mcurried\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1494\u001b[0m             \u001b[0;31m# _map_values does not support args/kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1495\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mcurried\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1496\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/tools/numeric.py\u001b[0m in \u001b[0;36mto_numeric\u001b[0;34m(arg, errors, downcast, dtype_backend)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"O\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_decimal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["\n","\n","---\n","# Transformer Model\n","\n"],"metadata":{"id":"waNhrbSAsk_6"}},{"cell_type":"markdown","source":["**No HP Tuning**: First we will implement our model without HP tuning and try to overfit, to just prove that we have the generalization power, and just check that we can actually set up and run the architecture"],"metadata":{"id":"WSxQ_mLEsXTr"}},{"cell_type":"code","source":["\n","\n","# **Set device for GPU acceleration**\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# **Data Preparation**\n","# Assume `data` is a pandas DataFrame containing your dataset\n","X = data.iloc[:, :-1].values  # Features\n","y = data.iloc[:, -1].values  # Labels (fraud or not)\n","\n","# **Split data**\n","X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n","X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n","\n","# **Convert to PyTorch tensors**\n","# **Convert to PyTorch tensors**\n","# Ensure tensors have 3 dimensions by adding a dimension if necessary\n","train_data = TensorDataset(torch.tensor(X_train, dtype=torch.float32).to(device).unsqueeze(1),\n","                           torch.tensor(y_train, dtype=torch.float32).to(device))\n","val_data = TensorDataset(torch.tensor(X_val, dtype=torch.float32).to(device).unsqueeze(1),\n","                         torch.tensor(y_val, dtype=torch.float32).to(device))\n","test_data = TensorDataset(torch.tensor(X_test, dtype=torch.float32).to(device).unsqueeze(1),\n","                          torch.tensor(y_test, dtype=torch.float32).to(device))\n","\n","# **DataLoader for batching**\n","batch_size = 64\n","train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_data, batch_size=batch_size)\n","\n","# **Transformer Model with Batch Normalization**\n","# **Transformer Model with Batch Normalization**\n","# **Transformer Model with Batch Normalization**\n","class FraudDetectionTransformer(nn.Module):\n","    def __init__(self, input_dim, embed_dim, num_heads, ff_dim, num_layers, dropout=0.1):\n","        super(FraudDetectionTransformer, self).__init__()\n","        self.embedding = nn.Linear(input_dim, embed_dim)  # Embedding layer\n","        self.batch_norm = nn.BatchNorm1d(input_dim)  # Batch Normalization before embedding\n","\n","        # Define the Transformer encoder\n","        encoder_layer = nn.TransformerEncoderLayer(\n","            d_model=embed_dim,  # Embedding dimension\n","            nhead=num_heads,  # Number of attention heads\n","            dim_feedforward=ff_dim,  # Feed-forward network dimension\n","            dropout=dropout  # Dropout rate\n","        )\n","        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)  # Stack layers\n","\n","        self.pooling = nn.AdaptiveAvgPool1d(1)  # Global average pooling\n","        self.fc = nn.Linear(embed_dim, 1)  # Fully connected layer for classification\n","        self.sigmoid = nn.Sigmoid()  # Sigmoid activation for binary classification\n","\n","    def forward(self, x):\n","        x = self.batch_norm(x.squeeze(1))  # Apply BatchNorm and squeeze before embedding\n","        x = self.embedding(x).unsqueeze(1)  # Embedding and unsqueeze to add sequence dimension\n","        x = x.permute(1, 0, 2)  # Reshape for transformer input (sequence_length, batch_size, embedding_dim)\n","        x = self.transformer(x)  # Pass input to encoder only\n","        x = self.pooling(x.permute(1, 2, 0)).squeeze()  # Global pooling to get a single feature vector\n","        x = self.fc(x)  # Apply fully connected layer for classification\n","        return self.sigmoid(x)  # Apply sigmoid to get probability\n","\n","# **Training Function**\n","def train_model(model, train_loader, val_loader, epochs, lr):\n","    model = model.to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","    criterion = nn.BCELoss()\n","\n","    train_losses = []\n","    val_losses = []\n","\n","    for epoch in range(epochs):\n","        # Training phase\n","        model.train()\n","        train_loss = 0.0\n","        for inputs, labels in train_loader:\n","            optimizer.zero_grad()\n","            outputs = model(inputs).squeeze()\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item()\n","        train_losses.append(train_loss / len(train_loader))\n","\n","        # Validation phase\n","        model.eval()\n","        val_loss = 0.0\n","        with torch.no_grad():\n","            for inputs, labels in val_loader:\n","                outputs = model(inputs).squeeze()\n","                loss = criterion(outputs, labels)\n","                val_loss += loss.item()\n","        val_losses.append(val_loss / len(val_loader))\n","\n","        print(f\"Epoch {epoch + 1}/{epochs} - Train Loss: {train_losses[-1]:.4f} - Val Loss: {val_losses[-1]:.4f}\")\n","\n","    return train_losses, val_losses\n","\n","# **Plotting Function**\n","def plot_losses(train_losses, val_losses):\n","    plt.figure(figsize=(10, 6))\n","    plt.plot(range(1, len(train_losses) + 1), train_losses, label=\"Training Loss\")\n","    plt.plot(range(1, len(val_losses) + 1), val_losses, label=\"Validation Loss\")\n","    plt.xlabel(\"Epochs\")\n","    plt.ylabel(\"Loss\")\n","    plt.title(\"Training and Validation Loss\")\n","    plt.legend()\n","    plt.grid(True)\n","    plt.show()\n","\n","# Model Initialization and Training\n","input_dim = X_train.shape[1]\n","embed_dim = 128\n","num_heads = 4\n","ff_dim = 256\n","num_layers = 2\n","dropout = 0.1  # Set dropout rate\n","\n","model = FraudDetectionTransformer(input_dim=input_dim, embed_dim=embed_dim, num_heads=num_heads,\n","                                   ff_dim=ff_dim, num_layers=num_layers, dropout=dropout)\n","\n","# Train the model and plot results\n","train_losses, val_losses = train_model(model, train_loader, val_loader, epochs=20, lr=1e-3)\n","plot_losses(train_losses, val_losses)\n"],"metadata":{"id":"9evuMo2oImJG","colab":{"base_uri":"https://localhost:8080/"},"outputId":"42bbf9e5-f9c5-474f-d95a-0e30084b21d2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/20 - Train Loss: 0.0055 - Val Loss: 0.0042\n","Epoch 2/20 - Train Loss: 0.0043 - Val Loss: 0.0045\n","Epoch 3/20 - Train Loss: 0.0040 - Val Loss: 0.0038\n","Epoch 4/20 - Train Loss: 0.0039 - Val Loss: 0.0039\n","Epoch 5/20 - Train Loss: 0.0039 - Val Loss: 0.0041\n","Epoch 6/20 - Train Loss: 0.0038 - Val Loss: 0.0038\n","Epoch 7/20 - Train Loss: 0.0037 - Val Loss: 0.0039\n","Epoch 8/20 - Train Loss: 0.0036 - Val Loss: 0.0037\n","Epoch 9/20 - Train Loss: 0.0036 - Val Loss: 0.0037\n","Epoch 10/20 - Train Loss: 0.0036 - Val Loss: 0.0036\n","Epoch 11/20 - Train Loss: 0.0035 - Val Loss: 0.0038\n","Epoch 12/20 - Train Loss: 0.0034 - Val Loss: 0.0038\n","Epoch 13/20 - Train Loss: 0.0035 - Val Loss: 0.0033\n","Epoch 14/20 - Train Loss: 0.0036 - Val Loss: 0.0035\n","Epoch 15/20 - Train Loss: 0.0034 - Val Loss: 0.0034\n","Epoch 16/20 - Train Loss: 0.0035 - Val Loss: 0.0033\n","Epoch 17/20 - Train Loss: 0.0034 - Val Loss: 0.0033\n","Epoch 18/20 - Train Loss: 0.0033 - Val Loss: 0.0034\n","Epoch 19/20 - Train Loss: 0.0034 - Val Loss: 0.0036\n"]}]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","# Auto commit to github"],"metadata":{"id":"y0JuVWk0R5k6"}},{"cell_type":"code","source":["import datetime\n","import os\n","\n","# Navigate to the repository directory (if not already there)\n","%cd /content/drive/MyDrive/Colab_Notebooks/Deep_Learning_Practice\n","\n","with open('/content/drive/MyDrive/IAM/PAT.txt', 'r') as file:\n","      github_pat = file.read().strip()\n","os.environ['GITHUB_PAT'] = github_pat\n","\n","!git remote add origin \"https://github.com/archiegoodman2/machine_learning_practice\"\n","\n","# Replace with your actual username and email (or configure globally)\n","USERNAME=\"archiegoodman2\"\n","EMAIL=\"archiegoodman2011@gmail.com\"\n","\n","# Set global username and email configuration\n","!git config --global user.name \"$USERNAME\"\n","!git config --global user.email \"$EMAIL\"\n","\n","now = datetime.datetime.now()\n","current_datetime = now.strftime(\"%Y-%m-%d %H:%M\")\n","\n","# Set remote URL using the PAT from environment variable\n","!git remote set-url origin https://{os.environ['GITHUB_PAT']}@github.com/archiegoodman2/machine_learning_practice.git\n","\n","# Replace with your desired commit message\n","COMMIT_MESSAGE = str(current_datetime) + \" debugged and applied default pytorch embeddings layer instead of the custom one. \"\n","\n","# Stage all changes\n","!git add .\n","\n","# Commit the changes\n","!git commit -m \"$COMMIT_MESSAGE\"\n","\n","# Push to origin\n","!git push origin master\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xWxH5PbcOdSb","executionInfo":{"status":"ok","timestamp":1733257568206,"user_tz":0,"elapsed":160042,"user":{"displayName":"Archie Goodman","userId":"05960694724077487952"}},"outputId":"0683680f-8222-4b44-d03a-8e35ce34a687"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab_Notebooks/Deep_Learning_Practice\n","error: remote origin already exists.\n","[master 593ccc6] 2024-12-03 20:23 commit\n"," 2 files changed, 2 insertions(+), 2 deletions(-)\n"," rewrite nn_transformer_creditcardfraud.ipynb (80%)\n","Enumerating objects: 7, done.\n","Counting objects: 100% (7/7), done.\n","Delta compression using up to 2 threads\n","Compressing objects: 100% (4/4), done.\n","Writing objects: 100% (4/4), 1.77 KiB | 139.00 KiB/s, done.\n","Total 4 (delta 3), reused 0 (delta 0), pack-reused 0\n","remote: Resolving deltas: 100% (3/3), completed with 3 local objects.\u001b[K\n","To https://github.com/archiegoodman2/machine_learning_practice.git\n","   cea3757..593ccc6  master -> master\n"]}]}]}