{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1cX2lOjHCdxCQCb5mu2Ta83cyJpI45yXA","authorship_tag":"ABX9TyOP171HnUE67whHL2tT/G+U"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# TO DO\n","\n","1. finish objective function - look at code on how to call dataloaders and our custom classes on chatgpt\n","2. get it to plot train loss vs val loss on best model???\n","3. get it to spit out accuracy, hit rate, f1 and ROC AUC. get it to plot ROC AUC graph??\n","4. get it to output best HP combo\n","5. check paper for other tips and tricks they used.\n","6. look into Cell-based Architectures: Explore more structured approaches for dynamic architectures, such as cell-based architectures (like those used in NASNet or EfficientNet), which offer a balance between flexibility and control"],"metadata":{"id":"z_-I-6HM6dbi"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n","# Intro"],"metadata":{"id":"zcLoV-sasrXj"}},{"cell_type":"markdown","source":["**Plan**: Import credit card fraud data. Use encoder only transformer network for classifying time series credit card data\n","\n","**Purpose/Intro**: Task is to develop transformer architecture proof of concept for potential application at work, detecting fraud. In a normal data science project it might be considered best practice to begin with more interpretable models first, for research purposes, but this project is solely for the purpose of assessing the viability of a transformer for this task.\n","\n","**Hypothesis**: The attention mechanism of the transformer, when combined with an appropriate positional embedding method, is able to capture both long-term and short-term dependencies in time series credit-card fraud data.\n","\n","**Methodology**: Using cross valdiation techniques on test dataset to calculate appropriate accuracy metrics (adjusting for the significant class imbalance for the dataset), with an aim to assess the viability of transformer networks for fraud classification.\n","\n","Credit to the below paper, **referred to as Source 1**, for the methodology design: Yu, C., Xu, Y., Cao, J., Zhang, Y., Jin, Y. and Zhu, M. (2024) 'Credit Card Fraud Detection Using Advanced Transformer Model', arXiv preprint arXiv:2406.03733. Available at: https://arxiv.org/abs/2406.03733 (Accessed: 18 December 2024)\n","\n","This paper has demonstrated the utility for the transformer that we are about to create, by comparing the methodology with various other shallow learning techniques. In future projects I aim to validate this myself.\n","\n"],"metadata":{"id":"m8GsCk0-ImyX"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n","# Data Sourcing and Package loading\n","\n"],"metadata":{"id":"xlTtmyBAt_Wb"}},{"cell_type":"code","source":["\n","#import packages:\n","\n","import os\n","os.environ['NUMBAPRO_NVVM'] = '/usr/local/cuda/nvvm/lib64/libnvvm.so'\n","os.environ['NUMBAPRO_LIBDEVICE'] = '/usr/local/cuda/nvvm/libdevice/'\n","\n","from google.colab import drive\n","\n","try:\n","  import google.colab\n","  IN_COLAB = True\n","except:\n","  IN_COLAB = False\n","\n","if IN_COLAB:\n","  # Check if drive is mounted by looking for the mount point in the file system.\n","  import os\n","  if not os.path.exists('/content/drive'):\n","    drive.mount('/content/drive')\n","\n","#basics\n","import os\n","from google.colab import drive\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","!pip install optuna\n","import optuna\n","\n","#cuML\n","from cuml.preprocessing import MinMaxScaler\n","import cudf\n","import cupy as cp\n","from imblearn.over_sampling import SMOTE  # GPU-enabled SMOTE with cuML\n","\n","\n","#table one\n","!pip install tableone\n","from tableone import TableOne\n","\n","#torch\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader, TensorDataset\n","\n","\n","#sklearn\n","from imblearn.over_sampling import RandomOverSampler\n","from imblearn.over_sampling import SMOTE\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import StratifiedKFold, GridSearchCV\n","from sklearn.metrics import roc_auc_score\n","from sklearn.model_selection import cross_val_score, train_test_split, RepeatedStratifiedKFold, KFold\n","from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n","from sklearn.preprocessing import MinMaxScaler\n","\n","from imblearn.over_sampling import RandomOverSampler"],"metadata":{"id":"_jBqgg1I0MCo","executionInfo":{"status":"aborted","timestamp":1734624456555,"user_tz":0,"elapsed":5,"user":{"displayName":"Archie Goodman","userId":"05960694724077487952"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_set_filepath = '/content/drive/MyDrive/Colab_Notebooks/Data/creditcard.feather'\n","\n","df = pd.read_feather(data_set_filepath)\n","\n","missing_values = df.isnull().sum()\n","\n","if missing_values.any():  # Check if any missing values exist\n","    print(\"Missing Values Found:\")\n","    print(missing_values)\n","else:\n","    print(\"No Missing Values Found\")\n","\n","columns = df.columns.tolist()\n","\n","print(f\"The dataset lenghth is {str(len(df))}\")\n","print(f\"The number of columns is {str(len(columns))}\")\n","print(f\"The column names are {str(columns)}\")\n","df.head(10)\n","\n","#table1 = TableOne(df, columns=columns, groupby= 'Class', pval=True)\n","#print(table1)\n","\n","data = df\n","\n","\n","\n"],"metadata":{"id":"pHk36Lkm6cUT","executionInfo":{"status":"aborted","timestamp":1734624456555,"user_tz":0,"elapsed":5,"user":{"displayName":"Archie Goodman","userId":"05960694724077487952"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","# Data loading and preprocessing:\n","In Source 1 (listed above), it was found that there are performance boosts associated with removing outliers, as it may help with overfitting. This will be done on the training data only. This is to prevent information leakage from our training set.\n","\n","The source also suggests there is value in oversampling the minority class. This may be due to the unique challenges of such a large class imbalance. This will be done on the training data only. This is to prevent information leakage from our training set.\n","\n","In addition, we will min-max scale our validation and training sets, and apply this same scaling to the test data."],"metadata":{"id":"C94hJDCHuNz7"}},{"cell_type":"code","source":["# **Set device for GPU acceleration**\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# **Error warning if no GPU is detected**\n","if device.type != 'cuda':\n","    print(\"WARNING: GPU is not available. The model will run on the CPU, which might be slower.\")\n","else:\n","    print(\"Cuda setup successful\")\n","\n","#separates all data into data and targets\n","X = data.iloc[:, :-1]  # Features (all columns except the last one)\n","y = data.iloc[:, -1]   # Labels (the last column)\n","\n","#sets X and y as the training + validation set, and x_test and y_test as the test set\n","X, X_test, y, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42, stratify=y  # stratify for class imbalance\n",")\n","\n","# Data Preprocessing Transformation Class\n","class DataPreprocessingTransform:\n","    def __init__(self):\n","        self.scaler = MinMaxScaler()\n","\n","    def fit_transform(self, X_train):\n","        \"\"\"\n","        Fit the scaler on the training set and transform it.\n","        \"\"\"\n","        X_train_cudf = cudf.DataFrame(X_train)\n","        X_scaled = self.scaler.fit_transform(X_train_cudf)\n","        return X_scaled  # Returns a cuDF DataFrame\n","\n","    def transform(self, X):\n","        \"\"\"\n","        Apply the scaling transformation based on the training set scaling.\n","        \"\"\"\n","        X_cudf = cudf.DataFrame(X)\n","        X_scaled = self.scaler.transform(X_cudf)\n","        return X_scaled  # Returns a cuDF DataFrame\n","\n","\n","\n","# Custom PyTorch Dataset\n","class CustomDataset(Dataset):\n","    def __init__(self, X, y, transform=None):\n","        \"\"\"\n","        Custom dataset to handle GPU-enabled data directly.\n","        \"\"\"\n","        self.X = X  # Should be cuDF or GPU tensor\n","        self.y = y  # Should be cuDF or GPU tensor\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, index):\n","        sample_X = self.X.iloc[index]\n","        sample_y = self.y.iloc[index]\n","\n","        if self.transform:\n","            sample_X = self.transform(sample_X)\n","\n","        # Convert to GPU tensor if not already\n","        sample_X = cp.array(sample_X.values) if not isinstance(sample_X, cp.ndarray) else sample_X\n","        sample_y = cp.array(sample_y.values) if not isinstance(sample_y, cp.ndarray) else sample_y\n","\n","        return torch.tensor(sample_X, dtype=torch.float32, device=\"cuda\"), torch.tensor(sample_y, dtype=torch.float32, device=\"cuda\")\n","\n","    def _apply_smote(self, X, y):\n","        \"\"\"\n","        Applies GPU-accelerated SMOTE and returns cuDF DataFrame and Series.\n","        \"\"\"\n","        X_cudf = cudf.DataFrame(X)\n","        y_cudf = cudf.Series(y)\n","\n","        smote = SMOTE(sampling_strategy=\"auto\", random_state=42)\n","        X_resampled, y_resampled = smote.fit_resample(X_cudf, y_cudf)\n","\n","        return X_resampled, y_resampled  # Returns cuDF DataFrame and Series\n","\n","\n","# Function to remove outliers using IQR\n","def remove_outliers_gpu(X):\n","    \"\"\"\n","    Removes outliers from the dataset based on the IQR method using cuDF.\n","    This function is optimized for GPU processing.\n","\n","    Parameters:\n","    - X: cuDF DataFrame\n","\n","    Returns:\n","    - X_filtered: cuDF DataFrame with outliers removed\n","    \"\"\"\n","    # Compute Q1, Q3, and IQR\n","    Q1 = X.quantile(0.25)\n","    Q3 = X.quantile(0.75)\n","    IQR = Q3 - Q1\n","\n","    # Calculate bounds\n","    lower_bound = Q1 - 1.5 * IQR\n","    upper_bound = Q3 + 1.5 * IQR\n","\n","    # Filter out rows outside bounds\n","    X_filtered = X.fillna(X.median())  # Fill NaNs first\n","    mask = (X >= lower_bound) & (X <= upper_bound)  # Boolean mask for filtering\n","    X_filtered = X.where(mask, other=None).dropna()  # Apply mask and drop rows with NaNs\n","\n","    return X_filtered\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tqGSD-BjuSvr","executionInfo":{"status":"ok","timestamp":1734624528065,"user_tz":0,"elapsed":220,"user":{"displayName":"Archie Goodman","userId":"05960694724077487952"}},"outputId":"12e09375-656f-4db8-bfc4-9aad4ef4277a"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Cuda setup successful\n"]}]},{"cell_type":"markdown","source":["\n","\n","---\n","# Transformer Model\n","\n"],"metadata":{"id":"waNhrbSAsk_6"}},{"cell_type":"markdown","source":["We are going to implement the transformer and optimise the hyperparameters using the Optuna package. As per the paper listed above, we will resample from the minority class when training."],"metadata":{"id":"WSxQ_mLEsXTr"}},{"cell_type":"code","source":["# **Transformer Model with Batch Normalization and Residual Connections**\n","class FraudDetectionTransformer(nn.Module):\n","\n","      #init method dynamically builds model architecture\n","      def __init__(self, input_dim, embed_dim, num_heads, ff_dim_base, dropout, ff_dropout, activation_function, num_ff_layers, use_batchnorm, use_layernorm):\n","          super(FraudDetectionTransformer, self).__init__()\n","\n","          self.embedding = nn.Linear(input_dim, embed_dim)  # Embedding layer\n","          self.use_batchnorm = use_batchnorm # Store whether to use batchnorm\n","\n","          self.batch_norm = nn.BatchNorm1d(input_dim) if self.use_batchnorm else None # Batch Normalization before embedding\n","\n","          # Store the activation function\n","          self.activation_function = activation_function\n","\n","          # Multi-head Attention Layer\n","          self.multihead_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n","\n","          # Dynamically create feedforward network in __init__\n","          ff_network = []\n","          for i in range(num_ff_layers):\n","              ff_dim = ff_dim_base * (2 ** i)\n","              ff_network.extend([\n","                  nn.Linear(embed_dim if i == 0 else ff_dim // 2, ff_dim),\n","                  self.get_activation_function(),\n","                  nn.Dropout(ff_dropout)\n","              ])\n","\n","          self.ff_network = nn.Sequential(*ff_network)  # Store the created network\n","\n","          # Layer Normalization after the dynamic feedforward network\n","          self.layer_norm = nn.LayerNorm(embed_dim) if use_layernorm else None\n","\n","          self.pooling = nn.AdaptiveAvgPool1d(1)  # Global average pooling\n","          self.fc = nn.Linear(embed_dim, 1)  # Fully connected layer for classification\n","          self.sigmoid = nn.Sigmoid()  # Sigmoid activation for binary classification\n","\n","\n","      def forward(self, x):\n","          # Apply batch normalization if specified\n","          if self.use_batchnorm:\n","              x = self.batch_norm(x)\n","\n","          # Apply embedding layer\n","          embedded_x = self.embedding(x)\n","\n","          # Add sequence dimension for the transformer\n","          embedded_x = embedded_x.unsqueeze(1)\n","\n","          # Transpose for multi-head attention (sequence_length, batch_size, embed_dim)\n","          embedded_x = embedded_x.permute(1, 0, 2)\n","\n","          # Pass through multi-head attention and add residual connection\n","          attn_output, _ = self.multihead_attn(embedded_x, embedded_x, embedded_x)\n","          attn_output = attn_output + embedded_x\n","\n","          # Pass through feedforward layers with residual connection\n","          transformer_output = self.ff_network(attn_output.permute(1, 0, 2))  # (batch_size, sequence_length, embed_dim)\n","          transformer_output = transformer_output + attn_output.permute(1, 0, 2)\n","\n","          # Layer normalization if specified\n","          if self.layer_norm:\n","              transformer_output = self.layer_norm(transformer_output)\n","\n","          # Global pooling over the sequence length dimension\n","          pooled_output = self.pooling(transformer_output.permute(0, 2, 1)).squeeze()\n","\n","          # Final fully connected layer and sigmoid activation\n","          fc_output = self.fc(pooled_output)\n","          return self.sigmoid(fc_output)\n","\n","\n","# **Training Function with Early Stopping**\n","def train_model_with_early_stopping(model, train_loader, val_loader, epochs, lr, patience):\n","\n","    model = model.to(device) #switch to GPU\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr) #select Adam for optimization\n","    criterion = nn.BCELoss() #set loss funct\n","\n","    #history\n","    train_losses = []\n","    val_losses = []\n","    best_val_loss = float('inf')\n","    patience_counter = 0\n","    best_model = None\n","\n","\n","    for epoch in range(epochs):\n","        model.train() #set model to training mode\n","        train_loss = 0.0\n","\n","        for inputs, labels in train_loader:\n","            inputs = inputs.to(device)  # Move inputs to GPU\n","            labels = labels.to(device)  # Move labels to GPU\n","            optimizer.zero_grad()    #zero gradients\n","            outputs = model(inputs).squeeze()\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item()\n","        train_losses.append(train_loss / len(train_loader))\n","\n","        # Validation phase\n","        model.eval()\n","        val_loss = 0.0\n","\n","        with torch.no_grad(): #no gradient needed for val phase\n","            for inputs, labels in val_loader: #unpack data\n","                inputs = inputs.to(device)  # Move inputs to GPU\n","                labels = labels.to(device)  # Move labels to GPU\n","                outputs = model(inputs).squeeze() #squeeze removes unecessary dimensions of tensors e.g. [batch_size,1] -> [batch_size]\n","                loss = criterion(outputs, labels)\n","                val_loss += loss.item()\n","\n","        val_losses.append(val_loss / len(val_loader)) #avg loss\n","\n","        # Early stopping check\n","        if val_losses[-1] < best_val_loss:\n","            best_val_loss = val_losses[-1]\n","            best_model = model.state_dict()  # Save the best model\n","            patience_counter = 0\n","        else:\n","            patience_counter += 1\n","            if patience_counter >= patience:\n","                print(\"Early stopping triggered.\")\n","                break\n","\n","        print(f\"Epoch {epoch + 1}/{epochs} - Train Loss: {train_losses[-1]:.4f} - Val Loss: {val_losses[-1]:.4f}\")\n","\n","    model.load_state_dict(best_model)  # Load the best model before returning\n","    return train_losses, val_losses, best_model\n","\n","# **Validation Metrics Calculation**\n","def evaluate(model, data_loader):\n","    model.eval() #set to eval mode\n","    y_true = []\n","    y_pred = []\n","\n","     with torch.no_grad():\n","        for inputs, labels in data_loader:\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","            outputs = model(inputs).squeeze()\n","\n","            # Keep data as tensors on the GPU\n","            y_true.extend(labels)\n","            y_pred.extend(outputs)\n","\n","    # Convert predictions to binary format for accuracy and hit rate\n","    y_pred_binary = [1 if pred >= 0.5 else 0 for pred in y_pred]\n","\n","    # Metrics\n","    accuracy = accuracy_score(y_true, y_pred_binary)\n","    hit_rate = np.sum(np.logical_and(np.array(y_true) == 1, np.array(y_pred_binary) == 1)) / np.sum(np.array(y_true) == 1)\n","    roc_auc = roc_auc_score(y_true, y_pred)\n","\n","    return accuracy, hit_rate, roc_auc\n","\n","# **Plotting Function**\n","def plot_losses(train_losses, val_losses):\n","    plt.figure(figsize=(10, 6))\n","    plt.plot(train_losses, label=\"Training Loss\")\n","    plt.plot(val_losses, label=\"Validation Loss\")\n","    plt.xlabel(\"Epochs\")\n","    plt.ylabel(\"Loss\")\n","    plt.title(\"Training and Validation Loss\")\n","    plt.legend()\n","    plt.show()\n","\n","\n","# Modify the objective function to save the best model\n","def objective(trial):\n","    # Define hyperparameters to tune\n","    num_heads = trial.suggest_int(\"num_heads\", low=4, high=8, step=2)\n","    embed_dim = trial.suggest_int(\"embed_dim\", low=64, high=512, step=num_heads)\n","    ff_dim_base = trial.suggest_int(\"ff_dim_base\", low=64, high=512, step=64)\n","    dropout = trial.suggest_float(\"dropout\", low=0.1, high=0.5, step=0.1)\n","    ff_dropout = trial.suggest_float(\"ff_dropout\", low=0.1, high=0.5, step=0.1)\n","    activation_function = trial.suggest_categorical(\"activation_function\", [\"relu\", \"tanh\", \"sigmoid\"])\n","    num_ff_layers = trial.suggest_int(\"num_ff_layers\", low=1, high=3, step=1)\n","    use_batchnorm = trial.suggest_categorical(\"use_batchnorm\", [True, False])\n","    use_layernorm = trial.suggest_categorical(\"use_layernorm\", [True, False])\n","    batch_size = trial.suggest_int(\"batch_size\", 64, 512, step=64)\n","\n","    # Print the hyperparameters being trialed\n","    print(f\"Trial {trial.number}:\")\n","    print(f\"  num_heads: {num_heads}\")\n","    print(f\"  embed_dim: {embed_dim}\")\n","    print(f\"  ff_dim_base: {ff_dim_base}\")\n","    print(f\"  dropout: {dropout}\")\n","    print(f\"  ff_dropout: {ff_dropout}\")\n","    print(f\"  activation_function: {activation_function}\")\n","    print(f\"  num_ff_layers: {num_ff_layers}\")\n","    print(f\"  use_batchnorm: {use_batchnorm}\")\n","    print(f\"  use_layernorm: {use_layernorm}\")\n","    print(f\"  batch_size: {batch_size}\")\n","    print(\"-\" * 50)\n","\n","    # K-Fold Cross Validation\n","    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","    fold_scores = []\n","    best_val_score = -float('inf')\n","    best_model_state_dict = None\n","\n","    for train_index, val_index in kf.split(X):\n","        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n","        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n","\n","        # Preprocess the data\n","        transform = DataPreprocessingTransform()\n","        X_train_scaled = transform.fit_transform(X_train)\n","        X_val_scaled = transform.transform(X_val)\n","\n","        # Remove outliers\n","        X_train_filtered = X_train_scaled.apply(remove_outliers, axis=1) # Apply outlier removal to each row\n","        X_val_filtered = X_val_scaled.apply(remove_outliers, axis=1)\n","\n","        # Reset index for BOTH X_train_filtered and y_train after outlier removal\n","        X_train_filtered = X_train_filtered.reset_index(drop=True)\n","        y_train = y_train.reset_index(drop=True)\n","\n","        # Now align y_train based on the shared index after outlier removal\n","        # This will ensure that both DataFrames have the same number of rows and consistent indexing\n","        common_index = X_train_filtered.index.intersection(y_train.index)\n","        X_train_filtered = X_train_filtered.loc[common_index]\n","        y_train = y_train.loc[common_index]\n","\n","        # Create PyTorch datasets\n","        train_dataset = CustomDataset(X_train_filtered, y_train, resample=True)\n","        val_dataset = CustomDataset(X_val_filtered, y_val)\n","\n","        # Create DataLoaders\n","        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","\n","        # Initialize the model, loss function, and optimizer\n","        model = FraudDetectionTransformer(\n","            input_dim=input_dim,\n","            embed_dim=embed_dim,\n","            num_heads=num_heads,\n","            ff_dim_base=ff_dim_base,\n","            dropout=dropout,\n","            ff_dropout=ff_dropout,\n","            activation_function=activation_function,\n","            num_ff_layers=num_ff_layers,\n","            use_batchnorm=use_batchnorm,\n","            use_layernorm=use_layernorm\n","        ).to(device)\n","\n","        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","        criterion = nn.BCELoss()\n","\n","        # Train the model\n","        train_model(model, train_loader, val_loader, optimizer, criterion)\n","        val_score = evaluate_model(model, val_loader)\n","\n","        # Keep track of the best model\n","        if val_score > best_val_score:\n","            best_val_score = val_score\n","            best_model_state_dict = model.state_dict()\n","\n","        fold_scores.append(val_score)\n","\n","    avg_fold_score = sum(fold_scores) / len(fold_scores)\n","\n","    # Save the best model after all folds\n","    if best_model_state_dict is not None:\n","        torch.save(best_model_state_dict, 'best_model.pth')\n","\n","    return avg_fold_score\n","\n","# Run Optuna optimization\n","study = optuna.create_study(direction=\"minimize\")\n","study.optimize(objective, n_trials=100)\n","\n","# Output the best parameters\n","print(\"Best hyperparameters:\", study.best_params)\n","\n","\n","\n","#=============================================== Test on Test Set ===============================================#\n","\n","# Evaluate on holdout test set\n","# Evaluate on holdout test set\n","best_params = study.best_params\n","\n","best_model = FraudDetectionTransformer(\n","            input_dim=best_params['input_dim'],\n","            embed_dim=best_params['embed_dim'],\n","            num_heads=best_params['num_heads'],\n","            ff_dim_base=best_params['ff_dim_base'],\n","            dropout=best_params['dropout'],\n","            ff_dropout=best_params['ff_dropout'],\n","            activation_function=best_params['activation_function'],\n","            num_ff_layers=best_params['num_ff_layers'],\n","            use_batchnorm=best_params['use_batchnorm'],\n","            use_layernorm=best_params['use_layernorm']\n","        ).to(device)\n","\n","# scale the data based on the training-set-fit but don't resample\n","# 1. Data Preprocessing for Test Set (using same transformations as in objective)\n","transform = DataPreprocessingTransform()\n","X_scaled = transform.fit_transform(X)  # Fit on training data\n","X_test_scaled = transform.transform(X_test)  # Transform test data\n","\n","# Remove outliers (if applied in objective)\n","X_filtered = X_scaled.apply(remove_outliers, axis=1).reset_index(drop=True)  #reset index for outlier removal\n","y = y.reset_index(drop=True)\n","\n","# Now align y_train based on the shared index after outlier removal\n","# This will ensure that both DataFrames have the same number of rows and consistent indexing\n","common_index = X_filtered.index.intersection(y.index)\n","X_filtered = X_filtered.loc[common_index]\n","y = y.loc[common_index]\n","\n","X_test_filtered = X_test_scaled.apply(remove_outliers, axis=1)  #reset index for outlier removal\n","\n","# 2. Create Datasets and DataLoaders\n","train_val_dataset = CustomDataset(X_filtered, y, transform=None, resample=True)  # Resample for training\n","test_dataset = CustomDataset(X_test_filtered, y_test, transform=None, resample=False)\n","\n","train_val_loader = DataLoader(train_val_dataset, batch_size=best_params[\"batch_size\"], shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=best_params[\"batch_size\"], shuffle=False)\n","\n","# 3. Initialize Final Model and Optimizer\n","final_model = FraudDetectionTransformer(\n","    input_dim=X.shape[1],  # Assuming input_dim wasn't tuned\n","    # ... other hyperparameters from best_params ...\n",").to(device)\n","\n","optimizer = torch.optim.Adam(final_model.parameters(), lr=best_params.get(\"lr\", 1e-3))  # Use best lr or default\n","criterion = nn.BCELoss()\n","\n","# 4. Train Final Model (using train_val_loader and test_loader for validation/early stopping)\n","train_losses, val_losses, best_model_state_dict = train_model_with_early_stopping(\n","    final_model, train_val_loader, test_loader, epochs=epochs, lr=best_params.get(\"lr\", 1e-3), patience=patience\n",")\n","\n","# Load best model weights\n","final_model.load_state_dict(best_model_state_dict)\n","\n","# 5. Evaluate on Test Set\n","test_accuracy, test_hit_rate, test_roc_auc = evaluate(final_model, test_loader)\n","\n","print(\"Test Accuracy:\", test_accuracy)\n","print(\"Test Hit Rate:\", test_hit_rate)\n","print(\"Test ROC AUC:\", test_roc_auc)\n","\n"],"metadata":{"id":"9evuMo2oImJG","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1734625014164,"user_tz":0,"elapsed":484539,"user":{"displayName":"Archie Goodman","userId":"05960694724077487952"}},"outputId":"c1d69e2e-ad2c-46a6-c5e3-8348dc66efd0"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["[I 2024-12-19 16:08:49,955] A new study created in memory with name: no-name-eac7626b-9e09-4f78-919d-ea6dbda603ab\n"]},{"output_type":"stream","name":"stdout","text":["Trial 0:\n","  num_heads: 4\n","  embed_dim: 100\n","  ff_dim_base: 256\n","  dropout: 0.1\n","  ff_dropout: 0.5\n","  activation_function: relu\n","  num_ff_layers: 2\n","  use_batchnorm: True\n","  use_layernorm: False\n","  batch_size: 448\n","--------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/utils/_tags.py:354: FutureWarning: The SMOTE or classes from which it inherits use `_get_tags` and `_more_tags`. Please define the `__sklearn_tags__` method, or inherit from `sklearn.base.BaseEstimator` and/or other appropriate mixins such as `sklearn.base.TransformerMixin`, `sklearn.base.ClassifierMixin`, `sklearn.base.RegressorMixin`, and `sklearn.base.OutlierMixin`. From scikit-learn 1.7, not defining `__sklearn_tags__` will raise an error.\n","  warnings.warn(\n","[W 2024-12-19 16:16:54,181] Trial 0 failed with parameters: {'num_heads': 4, 'embed_dim': 100, 'ff_dim_base': 256, 'dropout': 0.1, 'ff_dropout': 0.5, 'activation_function': 'relu', 'num_ff_layers': 2, 'use_batchnorm': True, 'use_layernorm': False, 'batch_size': 448} because of the following error: ValueError('Input X contains NaN.\\nSMOTE does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values').\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n","    value_or_values = func(trial)\n","  File \"<ipython-input-14-209009d1b6af>\", line 227, in objective\n","    train_dataset = CustomDataset(X_train_filtered, y_train, resample=True)\n","  File \"<ipython-input-13-fea6f523b519>\", line 45, in __init__\n","    X, y = self._apply_smote(X, y)\n","  File \"<ipython-input-13-fea6f523b519>\", line 71, in _apply_smote\n","    X_resampled, y_resampled = smote.fit_resample(X, y)\n","  File \"/usr/local/lib/python3.10/dist-packages/imblearn/base.py\", line 208, in fit_resample\n","    return super().fit_resample(X, y)\n","  File \"/usr/local/lib/python3.10/dist-packages/imblearn/base.py\", line 106, in fit_resample\n","    X, y, binarize_y = self._check_X_y(X, y)\n","  File \"/usr/local/lib/python3.10/dist-packages/imblearn/base.py\", line 161, in _check_X_y\n","    X, y = self._validate_data(X, y, reset=True, accept_sparse=accept_sparse)\n","  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 480, in _validate_data\n","    return validate_data(self, *args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 2961, in validate_data\n","    X, y = check_X_y(X, y, **check_params)\n","  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 1370, in check_X_y\n","    X = check_array(\n","  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 1107, in check_array\n","    _assert_all_finite(\n","  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 120, in _assert_all_finite\n","    _assert_all_finite_element_wise(\n","  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 169, in _assert_all_finite_element_wise\n","    raise ValueError(msg_err)\n","ValueError: Input X contains NaN.\n","SMOTE does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n","[W 2024-12-19 16:16:54,183] Trial 0 failed with value None.\n"]},{"output_type":"error","ename":"ValueError","evalue":"Input X contains NaN.\nSMOTE does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-209009d1b6af>\u001b[0m in \u001b[0;36m<cell line: 272>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;31m# Run Optuna optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"minimize\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;31m# Output the best parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-209009d1b6af>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;31m# Create PyTorch datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_filtered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m         \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_filtered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-fea6f523b519>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, X, y, transform, resample)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_smote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Ensure consistent indexing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-fea6f523b519>\u001b[0m in \u001b[0;36m_apply_smote\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \"\"\"\n\u001b[1;32m     70\u001b[0m         \u001b[0msmote\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampling_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mX_resampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_resampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_resampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_resampled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \"\"\"\n\u001b[1;32m    207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_more_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0marrays_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArraysTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinarize_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         self.sampling_strategy_ = check_sampling_strategy(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/imblearn/base.py\u001b[0m in \u001b[0;36m_check_X_y\u001b[0;34m(self, X, y, accept_sparse)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0maccept_sparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinarize_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_target_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindicate_one_vs_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinarize_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m         )\n\u001b[0;32m--> 480\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalidate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[0;31m# TODO(1.7): Remove this method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2959\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2960\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2961\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2962\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0mensure_all_finite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deprecate_force_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_all_finite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m   1371\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m             _assert_all_finite(\n\u001b[0m\u001b[1;32m   1108\u001b[0m                 \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     _assert_all_finite_element_wise(\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             )\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nSMOTE does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"]}]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","# Auto commit to github"],"metadata":{"id":"y0JuVWk0R5k6"}},{"cell_type":"code","source":["import datetime\n","import os\n","\n","# Navigate to the repository directory (if not already there)\n","%cd /content/drive/MyDrive/Colab_Notebooks/Deep_Learning_Practice\n","\n","with open('/content/drive/MyDrive/IAM/PAT.txt', 'r') as file:\n","      github_pat = file.read().strip()\n","os.environ['GITHUB_PAT'] = github_pat\n","\n","!git remote add origin \"https://github.com/archiegoodman2/machine_learning_practice\"\n","\n","# Replace with your actual username and email (or configure globally)\n","USERNAME=\"archiegoodman2\"\n","EMAIL=\"archiegoodman2011@gmail.com\"\n","\n","# Set global username and email configuration\n","!git config --global user.name \"$USERNAME\"\n","!git config --global user.email \"$EMAIL\"\n","\n","now = datetime.datetime.now()\n","current_datetime = now.strftime(\"%Y-%m-%d %H:%M\")\n","\n","# Set remote URL using the PAT from environment variable\n","!git remote set-url origin https://{os.environ['GITHUB_PAT']}@github.com/archiegoodman2/machine_learning_practice.git\n","\n","# Replace with your desired commit message\n","COMMIT_MESSAGE = str(current_datetime) + \" \" + \" second attempt \"\n","\n","# Stage all changes\n","!git add .\n","\n","# Commit the changes\n","!git commit -m \"$COMMIT_MESSAGE\"\n","\n","# Push to origin\n","!git push origin master\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xWxH5PbcOdSb","outputId":"bc021e2e-7058-4056-b57d-aef7b9b9154f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab_Notebooks/Deep_Learning_Practice\n","error: remote origin already exists.\n"]}]}]}