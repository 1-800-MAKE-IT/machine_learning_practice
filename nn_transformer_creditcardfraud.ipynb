{"cells":[{"cell_type":"markdown","metadata":{"id":"z_-I-6HM6dbi"},"source":["# TO DO\n","\n","1. finish objective function - look at code on how to call dataloaders and our custom classes on chatgpt\n","2. get it to plot train loss vs val loss on best model???\n","3. get it to spit out accuracy, hit rate, f1 and ROC AUC. get it to plot ROC AUC graph??\n","4. get it to output best HP combo\n","5. check paper for other tips and tricks they used.\n","6. look into Cell-based Architectures: Explore more structured approaches for dynamic architectures, such as cell-based architectures (like those used in NASNet or EfficientNet), which offer a balance between flexibility and control"]},{"cell_type":"markdown","metadata":{"id":"zcLoV-sasrXj"},"source":["\n","\n","---\n","\n","# Intro"]},{"cell_type":"markdown","metadata":{"id":"m8GsCk0-ImyX"},"source":["**Plan**: Import credit card fraud data. Use encoder only transformer network for classifying time series credit card data\n","\n","**Purpose/Intro**: Task is to develop transformer architecture proof of concept for potential application at work, detecting fraud. In a normal data science project it might be considered best practice to begin with more interpretable models first, for research purposes, but this project is solely for the purpose of assessing the viability of a transformer for this task.\n","\n","**Hypothesis**: The attention mechanism of the transformer, when combined with an appropriate positional embedding method, is able to capture both long-term and short-term dependencies in time series credit-card fraud data.\n","\n","**Methodology**: Using cross valdiation techniques on test dataset to calculate appropriate accuracy metrics (adjusting for the significant class imbalance for the dataset), with an aim to assess the viability of transformer networks for fraud classification.\n","\n","Credit to the below paper, **referred to as Source 1**, for the methodology design: Yu, C., Xu, Y., Cao, J., Zhang, Y., Jin, Y. and Zhu, M. (2024) 'Credit Card Fraud Detection Using Advanced Transformer Model', arXiv preprint arXiv:2406.03733. Available at: https://arxiv.org/abs/2406.03733 (Accessed: 18 December 2024)\n","\n","This paper has demonstrated the utility for the transformer that we are about to create, by comparing the methodology with various other shallow learning techniques. In future projects I aim to validate this myself.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xlTtmyBAt_Wb"},"source":["\n","\n","---\n","\n","# Data Sourcing and Package loading\n","\n"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":8108,"status":"ok","timestamp":1734709518304,"user":{"displayName":"Archie Goodman","userId":"05960694724077487952"},"user_tz":0},"id":"_jBqgg1I0MCo","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c34c8ea4-2a5d-4874-96db-664b28705764"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (4.1.0)\n","Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.14.0)\n","Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.9.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.2)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.36)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.67.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n","Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.8)\n","Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n","Requirement already satisfied: tableone in /usr/local/lib/python3.10/dist-packages (0.9.1)\n","Requirement already satisfied: jinja2>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from tableone) (3.1.4)\n","Requirement already satisfied: numpy>=1.19.1 in /usr/local/lib/python3.10/dist-packages (from tableone) (1.26.4)\n","Requirement already satisfied: openpyxl>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from tableone) (3.1.5)\n","Requirement already satisfied: pandas>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from tableone) (2.2.2)\n","Requirement already satisfied: scipy>=1.10.1 in /usr/local/lib/python3.10/dist-packages (from tableone) (1.13.1)\n","Requirement already satisfied: statsmodels>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from tableone) (0.14.4)\n","Requirement already satisfied: tabulate>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tableone) (0.9.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=3.1.4->tableone) (3.0.2)\n","Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl>=3.1.2->tableone) (2.0.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.3->tableone) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.3->tableone) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.3->tableone) (2024.2)\n","Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.14.1->tableone) (1.0.1)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.14.1->tableone) (24.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.3->tableone) (1.17.0)\n"]}],"source":["\n","#import packages:\n","\n","import os\n","os.environ['NUMBAPRO_NVVM'] = '/usr/local/cuda/nvvm/lib64/libnvvm.so'\n","os.environ['NUMBAPRO_LIBDEVICE'] = '/usr/local/cuda/nvvm/libdevice/'\n","\n","from google.colab import drive\n","\n","import locale\n","def getpreferredencoding(do_setlocale = True):\n","    return \"UTF-8\"\n","locale.getpreferredencoding = getpreferredencoding\n","\n","try:\n","  import google.colab\n","  IN_COLAB = True\n","except:\n","  IN_COLAB = False\n","\n","if IN_COLAB:\n","  # Check if drive is mounted by looking for the mount point in the file system.\n","  import os\n","  if not os.path.exists('/content/drive'):\n","    drive.mount('/content/drive')\n","\n","#basics\n","import os\n","from google.colab import drive\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","!pip install optuna\n","import optuna\n","\n","#cuML\n","import cudf\n","from cudf import DataFrame as cudfDataFrame\n","import cupy as cp\n","from imblearn.over_sampling import SMOTE, RandomOverSampler\n","\n","#table one\n","!pip install tableone\n","from tableone import TableOne\n","\n","#torch\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader, TensorDataset\n","\n","\n","#sklearn\n","from imblearn.over_sampling import RandomOverSampler\n","from imblearn.over_sampling import SMOTE\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import StratifiedKFold, GridSearchCV\n","from sklearn.metrics import roc_auc_score\n","from sklearn.model_selection import cross_val_score, train_test_split, RepeatedStratifiedKFold, KFold\n","from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n","from sklearn.preprocessing import MinMaxScaler\n","\n","from imblearn.over_sampling import RandomOverSampler"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":1000,"status":"ok","timestamp":1734709441992,"user":{"displayName":"Archie Goodman","userId":"05960694724077487952"},"user_tz":0},"id":"pHk36Lkm6cUT","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d07277f4-d294-4f83-b5fb-a210a3729e86"},"outputs":[{"output_type":"stream","name":"stdout","text":["No Missing Values Found\n","The dataset lenghth is 284807\n","The number of columns is 31\n","The column names are ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount', 'Class']\n"]}],"source":["data_set_filepath = '/content/drive/MyDrive/Colab_Notebooks/Data/creditcard.feather'\n","\n","df = pd.read_feather(data_set_filepath)\n","\n","missing_values = df.isnull().sum()\n","\n","if missing_values.any():  # Check if any missing values exist\n","    print(\"Missing Values Found:\")\n","    print(missing_values)\n","else:\n","    print(\"No Missing Values Found\")\n","\n","columns = df.columns.tolist()\n","\n","print(f\"The dataset lenghth is {str(len(df))}\")\n","print(f\"The number of columns is {str(len(columns))}\")\n","print(f\"The column names are {str(columns)}\")\n","df.head(10)\n","\n","#table1 = TableOne(df, columns=columns, groupby= 'Class', pval=True)\n","#print(table1)\n","\n","data = df\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"C94hJDCHuNz7"},"source":["\n","\n","---\n","\n","\n","# Data loading and preprocessing:\n","In Source 1 (listed above), it was found that there are performance boosts associated with removing outliers, as it may help with overfitting. This will be done on the training data only. This is to prevent information leakage from our training set.\n","\n","The source also suggests there is value in oversampling the minority class. This may be due to the unique challenges of such a large class imbalance. This will be done on the training data only. This is to prevent information leakage from our training set.\n","\n","In addition, we will min-max scale our validation and training sets, and apply this same scaling to the test data."]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":968,"status":"ok","timestamp":1734709524472,"user":{"displayName":"Archie Goodman","userId":"05960694724077487952"},"user_tz":0},"id":"tqGSD-BjuSvr","outputId":"45f9f5ef-591f-4acc-f792-feb09f6613ab"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cuda setup successful\n"]}],"source":["\n","# Set device for GPU acceleration\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Warning if no GPU is detected\n","if device.type != 'cuda':\n","    print(\"WARNING: GPU is not available. The model will run on the CPU, which might be slower.\")\n","else:\n","    print(\"Cuda setup successful\")\n","\n","# Separate data into features and targets\n","X = data.iloc[:, :-1]  # Features (all columns except the last one)\n","y = data.iloc[:, -1]   # Labels (the last column)\n","\n","# Split into train/val and test sets\n","X, X_test, y, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42, stratify=y\n",")\n","\n","# Data Preprocessing Transformation Class\n","class DataPreprocessingTransform:\n","    def __init__(self):\n","        self.scaler = MinMaxScaler()\n","\n","    def fit_transform(self, X_train):\n","        \"\"\"\n","        Fit the scaler on the training set and transform it.\n","        Returns a numpy array.\n","        \"\"\"\n","        # Ensure data is a numpy array\n","        if isinstance(X_train, pd.DataFrame):\n","            X_train = X_train.values\n","        X_scaled = self.scaler.fit_transform(X_train)\n","        return X_scaled\n","\n","    def transform(self, X):\n","        \"\"\"\n","        Apply scaling transformation based on the training set scaling.\n","        Returns a numpy array.\n","        \"\"\"\n","        if isinstance(X, pd.DataFrame):\n","            X = X.values\n","        X_scaled = self.scaler.transform(X)\n","        return X_scaled\n","\n","# Custom PyTorch Dataset\n","class CustomDataset(Dataset):\n","    def __init__(self, X, y, transform=None, resample=False, device=\"cuda\"):\n","        \"\"\"\n","        Custom dataset to handle preprocessing, transformations, and oversampling.\n","\n","        Parameters:\n","        - X (pd.DataFrame or np.ndarray): Features.\n","        - y (pd.Series or np.ndarray): Labels.\n","        - transform (callable): A function to apply to the features.\n","        - resample (bool): Whether to apply random oversampling to balance classes.\n","        - device (str): Device to which the data should be moved (e.g., 'cuda' or 'cpu').\n","        \"\"\"\n","        # Ensure X and y are numpy arrays\n","        if isinstance(X, pd.DataFrame):\n","            X = X.values\n","        if isinstance(y, pd.Series):\n","            y = y.values\n","\n","        # Apply random oversampling if needed\n","        if resample:\n","            X, y = self._apply_random_oversampler(X, y)\n","\n","        # Apply optional transformation (e.g., scaling)\n","        if transform:\n","            X = transform(X)\n","\n","        # Convert to torch tensors and move to device\n","        self.X = torch.tensor(X, dtype=torch.float32, device=device)\n","        self.y = torch.tensor(y, dtype=torch.float32, device=device)\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, index):\n","        return self.X[index], self.y[index]\n","\n","    @staticmethod\n","    def _apply_random_oversampler(X, y):\n","        \"\"\"\n","        Applies random oversampling to balance the dataset.\n","        \"\"\"\n","        ros = RandomOverSampler(random_state=42)\n","        X_resampled, y_resampled = ros.fit_resample(X, y)\n","        return X_resampled, y_resampled\n","\n","# Function to remove outliers using IQR on GPU (optional)\n","\n","def remove_outliers_gpu(X):\n","    \"\"\"\n","    Removes outliers from the dataset based on the IQR method using cuDF for GPU acceleration.\n","    This function expects a pandas DataFrame as input, converts it to cuDF, performs outlier removal,\n","    and returns a pandas DataFrame.\n","\n","    Steps:\n","    1. Convert pandas DataFrame (X) to cuDF DataFrame for GPU computations.\n","    2. Compute the 1st (Q1) and 3rd (Q3) quartiles.\n","    3. Calculate the Interquartile Range (IQR = Q3 - Q1).\n","    4. Determine the lower and upper bounds for outliers (Q1 - 1.5*IQR and Q3 + 1.5*IQR).\n","    5. Fill NaNs with median values to avoid issues with NaNs during filtering.\n","    6. Create a boolean mask to keep only rows within the [lower_bound, upper_bound] range for all columns.\n","    7. Apply the mask and drop rows that become NaN after filtering (i.e., outliers).\n","    8. Convert the filtered cuDF DataFrame back to a pandas DataFrame.\n","    9. Return the pandas DataFrame with outliers removed.\n","\n","    Parameters:\n","    - X (pd.DataFrame): A pandas DataFrame containing the data from which outliers need to be removed.\n","\n","    Returns:\n","    - X_filtered (pd.DataFrame): A pandas DataFrame with outliers removed.\n","    \"\"\"\n","    # Convert pandas DataFrame to cuDF DataFrame for GPU operations\n","    X_cudf = cudf.DataFrame(X)\n","\n","    # Compute quartiles and IQR\n","    Q1 = X_cudf.quantile(0.25)\n","    Q3 = X_cudf.quantile(0.75)\n","    IQR = Q3 - Q1\n","\n","    # Determine outlier bounds\n","    lower_bound = Q1 - 1.5 * IQR\n","    upper_bound = Q3 + 1.5 * IQR\n","\n","    # Fill NaNs with median to avoid filtering issues\n","    X_filled = X_cudf.fillna(X_cudf.median())\n","\n","    # Create a boolean mask to keep rows within the outlier bounds for all columns\n","    mask = (X_filled >= lower_bound) & (X_filled <= upper_bound)\n","\n","    # Apply mask; 'other=None' sets out-of-bound values to None which are dropped subsequently\n","    X_filtered_cudf = X_filled.where(mask, other=None).dropna()\n","\n","    # Convert back to pandas DataFrame\n","    X_filtered = X_filtered_cudf.to_pandas()\n","\n","    return X_filtered\n",""]},{"cell_type":"markdown","metadata":{"id":"waNhrbSAsk_6"},"source":["\n","\n","---\n","# Transformer Model\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WSxQ_mLEsXTr"},"source":["We are going to implement the transformer and optimise the hyperparameters using the Optuna package. As per the paper listed above, we will resample from the minority class when training."]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"9evuMo2oImJG","executionInfo":{"status":"error","timestamp":1734713500562,"user_tz":0,"elapsed":3686086,"user":{"displayName":"Archie Goodman","userId":"05960694724077487952"}},"outputId":"703316dc-3213-45ea-8dfe-84522b6f098c"},"outputs":[{"output_type":"stream","name":"stderr","text":["[I 2024-12-20 15:50:13,719] A new study created in memory with name: no-name-f4845451-6826-48c8-a1a0-6599c46b992c\n"]},{"output_type":"stream","name":"stdout","text":["Trial 0:\n","  num_heads: 8\n","  embed_dim: 176\n","  ff_dim_base: 320\n","  dropout: 0.1\n","  ff_dropout: 0.2\n","  activation_function: relu\n","  num_ff_layers: 3\n","  use_batchnorm: False\n","  use_layernorm: False\n","  batch_size: 64\n","  lr: 4.368287250420761e-05\n","--------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Early stopping at epoch 38\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Early stopping at epoch 20\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Early stopping at epoch 38\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-21-d8109248e5ea>:183: RuntimeWarning: invalid value encountered in scalar divide\n","  hit_rate = sum((np.array(y_true) == 1) & (np.array(y_pred_binary) == 1)) / sum(np.array(y_true) == 1)\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py:375: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Early stopping at epoch 53\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Early stopping at epoch 30\n"]},{"output_type":"stream","name":"stderr","text":["[W 2024-12-20 16:37:25,457] Trial 0 failed with parameters: {'num_heads': 8, 'embed_dim': 176, 'ff_dim_base': 320, 'dropout': 0.1, 'ff_dropout': 0.2, 'activation_function': 'relu', 'num_ff_layers': 3, 'use_batchnorm': False, 'use_layernorm': False, 'batch_size': 64, 'lr': 4.368287250420761e-05} because of the following error: The value nan is not acceptable.\n","[W 2024-12-20 16:37:25,458] Trial 0 failed with value nan.\n"]},{"output_type":"stream","name":"stdout","text":["Trial 1:\n","  num_heads: 8\n","  embed_dim: 112\n","  ff_dim_base: 64\n","  dropout: 0.2\n","  ff_dropout: 0.4\n","  activation_function: relu\n","  num_ff_layers: 2\n","  use_batchnorm: True\n","  use_layernorm: True\n","  batch_size: 256\n","  lr: 0.00019730912870221125\n","--------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Early stopping at epoch 36\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Early stopping at epoch 41\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Early stopping at epoch 36\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-21-d8109248e5ea>:183: RuntimeWarning: invalid value encountered in scalar divide\n","  hit_rate = sum((np.array(y_true) == 1) & (np.array(y_pred_binary) == 1)) / sum(np.array(y_true) == 1)\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py:375: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Early stopping at epoch 32\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Early stopping at epoch 14\n"]},{"output_type":"stream","name":"stderr","text":["[W 2024-12-20 16:51:38,409] Trial 1 failed with parameters: {'num_heads': 8, 'embed_dim': 112, 'ff_dim_base': 64, 'dropout': 0.2, 'ff_dropout': 0.4, 'activation_function': 'relu', 'num_ff_layers': 2, 'use_batchnorm': True, 'use_layernorm': True, 'batch_size': 256, 'lr': 0.00019730912870221125} because of the following error: The value nan is not acceptable.\n","[W 2024-12-20 16:51:38,411] Trial 1 failed with value nan.\n","/usr/local/lib/python3.10/dist-packages/optuna/distributions.py:708: UserWarning: The distribution is specified by [64, 512] and step=6, but the range is not divisible by `step`. It will be replaced by [64, 508].\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Trial 2:\n","  num_heads: 6\n","  embed_dim: 208\n","  ff_dim_base: 384\n","  dropout: 0.4\n","  ff_dropout: 0.4\n","  activation_function: tanh\n","  num_ff_layers: 3\n","  use_batchnorm: False\n","  use_layernorm: False\n","  batch_size: 256\n","  lr: 0.001314988509114278\n","--------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n","  warnings.warn(\n","[W 2024-12-20 16:51:39,706] Trial 2 failed with parameters: {'num_heads': 6, 'embed_dim': 208, 'ff_dim_base': 384, 'dropout': 0.4, 'ff_dropout': 0.4, 'activation_function': 'tanh', 'num_ff_layers': 3, 'use_batchnorm': False, 'use_layernorm': False, 'batch_size': 256, 'lr': 0.001314988509114278} because of the following error: AssertionError('embed_dim must be divisible by num_heads').\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n","    value_or_values = func(trial)\n","  File \"<ipython-input-21-d8109248e5ea>\", line 279, in objective\n","    model = FraudDetectionTransformer(\n","  File \"<ipython-input-21-d8109248e5ea>\", line 28, in __init__\n","    self.multihead_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py\", line 1071, in __init__\n","    self.head_dim * num_heads == self.embed_dim\n","AssertionError: embed_dim must be divisible by num_heads\n","[W 2024-12-20 16:51:39,708] Trial 2 failed with value None.\n"]},{"output_type":"error","ename":"AssertionError","evalue":"embed_dim must be divisible by num_heads","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-d8109248e5ea>\u001b[0m in \u001b[0;36m<cell line: 328>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;31m# Run Optuna optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"maximize\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;31m# Output the best parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-21-d8109248e5ea>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;31m# Initialize the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         model = FraudDetectionTransformer(\n\u001b[0m\u001b[1;32m    280\u001b[0m             \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0membed_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-21-d8109248e5ea>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dim, embed_dim, num_heads, ff_dim_base, dropout, ff_dropout, activation_function, num_ff_layers, use_batchnorm, use_layernorm)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Multi-head attention layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultihead_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiheadAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Build the feed-forward network layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, embed_dim, num_heads, dropout, bias, add_bias_kv, add_zero_attn, kdim, vdim, batch_first, device, dtype)\u001b[0m\n\u001b[1;32m   1069\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed_dim\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m         assert (\n\u001b[0;32m-> 1071\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_heads\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m         ), \"embed_dim must be divisible by num_heads\"\n\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: embed_dim must be divisible by num_heads"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import accuracy_score, roc_auc_score\n","from sklearn.model_selection import KFold\n","import optuna\n","import matplotlib.pyplot as plt\n","\n","# Assuming these are defined from previous code snippets:\n","# from data_import_code import X, y, X_test, y_test, DataPreprocessingTransform, CustomDataset, remove_outliers_gpu, device, epochs, patience\n","\n","#===================================== MODEL DEFINITION =====================================#\n","class FraudDetectionTransformer(nn.Module):\n","    def __init__(self, input_dim, embed_dim, num_heads, ff_dim_base, dropout, ff_dropout, activation_function, num_ff_layers, use_batchnorm, use_layernorm):\n","        super(FraudDetectionTransformer, self).__init__()\n","\n","        # Embedding layer to project input features to embed_dim\n","        self.embedding = nn.Linear(input_dim, embed_dim)\n","\n","        # Optional batch normalization on input features\n","        self.use_batchnorm = use_batchnorm\n","        self.batch_norm = nn.BatchNorm1d(input_dim) if use_batchnorm else None\n","\n","        # Multi-head attention layer\n","        self.multihead_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n","\n","        # Build the feed-forward network layers\n","        ff_layers = []\n","        current_dim = embed_dim\n","        for i in range(num_ff_layers):\n","            # Increase dimension according to ff_dim_base * 2^i\n","            next_dim = ff_dim_base * (2 ** i)\n","            ff_layers.append(nn.Linear(current_dim, next_dim))\n","            ff_layers.append(self.get_activation_function(activation_function))\n","            ff_layers.append(nn.Dropout(ff_dropout))\n","            current_dim = next_dim\n","\n","        # **Important Fix**:\n","        # Add a final linear layer to bring the dimension back to embed_dim.\n","        # This ensures ff_output and x have matching dimensions for the residual connection.\n","        ff_layers.append(nn.Linear(current_dim, embed_dim))\n","\n","        self.ff_network = nn.Sequential(*ff_layers)\n","\n","        # Optional layer normalization\n","        self.use_layernorm = use_layernorm\n","        self.layer_norm = nn.LayerNorm(embed_dim) if use_layernorm else None\n","\n","        # Adaptive average pooling to aggregate sequence dimension\n","        self.pooling = nn.AdaptiveAvgPool1d(1)\n","\n","        # Final classification layer\n","        self.fc = nn.Linear(embed_dim, 1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        # Apply batch normalization if enabled\n","        if self.use_batchnorm:\n","            x = self.batch_norm(x)\n","\n","        # Embed the input\n","        x = self.embedding(x)  # shape: (batch, embed_dim)\n","\n","        # Transformer expects (seq_len, batch, embed_dim)\n","        # Current shape: (batch, embed_dim) -> add seq dim at dim=1 -> (batch, 1, embed_dim)\n","        # Then permute: (1, batch, embed_dim)\n","        x = x.unsqueeze(1).permute(1, 0, 2)\n","\n","        # Multi-head self-attention\n","        attn_output, _ = self.multihead_attn(x, x, x)\n","        x = x + attn_output  # Residual connection\n","\n","        # Feed-forward network\n","        # Permute to (batch, seq, embed_dim) to feed into ff_network\n","        ff_output = self.ff_network(x.permute(1, 0, 2))\n","\n","        # Residual connection: ff_output and x now match in embed_dim after the fix\n","        x = x.permute(1, 0, 2) + ff_output\n","\n","        # Optional layer normalization\n","        if self.use_layernorm:\n","            x = self.layer_norm(x)\n","\n","        # Pooling: (batch, seq, embed_dim) -> permute to (batch, embed_dim, seq)\n","        # AdaptiveAvgPool1d(1) -> (batch, embed_dim, 1) -> squeeze -> (batch, embed_dim)\n","        x = self.pooling(x.permute(0, 2, 1)).squeeze()\n","\n","        # Final linear layer and sigmoid\n","        x = self.fc(x)\n","        return self.sigmoid(x)\n","\n","    def get_activation_function(self, activation_function):\n","        if activation_function == \"relu\":\n","            return nn.ReLU()\n","        elif activation_function == \"tanh\":\n","            return nn.Tanh()\n","        elif activation_function == \"sigmoid\":\n","            return nn.Sigmoid()\n","        else:\n","            raise ValueError(\"Unsupported activation function!\")\n","#===================================== TRAINING WITH EARLY STOPPING =====================================#\n","def train_model_with_early_stopping(model, train_loader, val_loader, criterion, optimizer, epochs, patience):\n","    \"\"\"\n","    Train the model with early stopping. Early stopping will stop training if the validation\n","    loss doesn't improve after a certain number of epochs (patience).\n","    \"\"\"\n","    best_val_loss = float('inf')\n","    best_model = None\n","    patience_counter = 0\n","    train_losses = []\n","    val_losses = []\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        running_loss = 0.0\n","\n","        # Training loop\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(inputs).squeeze()\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item()\n","\n","        # Compute average training loss for this epoch\n","        train_epoch_loss = running_loss / len(train_loader)\n","        train_losses.append(train_epoch_loss)\n","\n","        # Validation loop\n","        model.eval()\n","        val_loss = 0.0\n","        with torch.no_grad():\n","            for inputs, labels in val_loader:\n","                inputs, labels = inputs.to(device), labels.to(device)\n","                outputs = model(inputs).squeeze()\n","                loss = criterion(outputs, labels)\n","                val_loss += loss.item()\n","\n","        # Compute average validation loss\n","        val_epoch_loss = val_loss / len(val_loader)\n","        val_losses.append(val_epoch_loss)\n","\n","        # Check for improvement in validation loss\n","        if val_epoch_loss < best_val_loss:\n","            best_val_loss = val_epoch_loss\n","            best_model = model.state_dict()\n","            patience_counter = 0\n","        else:\n","            patience_counter += 1\n","            if patience_counter >= patience:\n","                print(f\"Early stopping at epoch {epoch + 1}\")\n","                break\n","\n","    # Load the best model state found during training\n","    model.load_state_dict(best_model)\n","    return train_losses, val_losses, best_model\n","\n","#===================================== EVALUATION =====================================#\n","def evaluate(model, data_loader):\n","    \"\"\"\n","    Evaluate the model on a given dataset. Returns accuracy, hit_rate, and ROC AUC.\n","    \"\"\"\n","    model.eval()\n","    y_true, y_pred = [], []\n","    with torch.no_grad():\n","        for inputs, labels in data_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs).squeeze()\n","            y_true.extend(labels.cpu().numpy())\n","            y_pred.extend(outputs.cpu().numpy())\n","\n","    # Convert predicted probabilities to binary labels\n","    y_pred_binary = [1 if p >= 0.5 else 0 for p in y_pred]\n","\n","    # Compute metrics\n","    accuracy = accuracy_score(y_true, y_pred_binary)\n","    # Hit rate: Among the actual fraud cases (y=1), how many did we correctly predict as fraud?\n","    hit_rate = sum((np.array(y_true) == 1) & (np.array(y_pred_binary) == 1)) / sum(np.array(y_true) == 1)\n","    roc_auc = roc_auc_score(y_true, y_pred)\n","\n","    return accuracy, hit_rate, roc_auc\n","\n","def plot_losses(train_losses, val_losses):\n","    \"\"\"\n","    Plot training and validation loss over epochs.\n","    \"\"\"\n","    plt.figure(figsize=(10, 6))\n","    plt.plot(train_losses, label=\"Training Loss\")\n","    plt.plot(val_losses, label=\"Validation Loss\")\n","    plt.xlabel(\"Epochs\")\n","    plt.ylabel(\"Loss\")\n","    plt.title(\"Training and Validation Loss\")\n","    plt.legend()\n","    plt.show()\n","\n","#===================================== OPTUNA OBJECTIVE FUNCTION =====================================#\n","def objective(trial):\n","    \"\"\"\n","    Objective function for Optuna optimization. Defines the hyperparameters to tune,\n","    runs K-fold cross-validation, and returns the average validation score across folds.\n","    \"\"\"\n","\n","    # Suggest hyperparameters for this trial\n","    num_heads = trial.suggest_int(\"num_heads\", low=4, high=8, step=2)\n","    embed_dim = trial.suggest_int(\"embed_dim\", low=64, high=512, step=num_heads)\n","    ff_dim_base = trial.suggest_int(\"ff_dim_base\", low=64, high=512, step=64)\n","    dropout = trial.suggest_float(\"dropout\", low=0.1, high=0.5, step=0.1)\n","    ff_dropout = trial.suggest_float(\"ff_dropout\", low=0.1, high=0.5, step=0.1)\n","    activation_function = trial.suggest_categorical(\"activation_function\", [\"relu\", \"tanh\", \"sigmoid\"])\n","    num_ff_layers = trial.suggest_int(\"num_ff_layers\", low=1, high=3, step=1)\n","    use_batchnorm = trial.suggest_categorical(\"use_batchnorm\", [True, False])\n","    use_layernorm = trial.suggest_categorical(\"use_layernorm\", [True, False])\n","    batch_size = trial.suggest_int(\"batch_size\", 64, 512, step=64)\n","\n","    # New: Tune the learning rate as well\n","    # Using a log-uniform suggestion: a typical range for LR might be [1e-5, 1e-2]\n","    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n","\n","    print(f\"Trial {trial.number}:\")\n","    print(f\"  num_heads: {num_heads}\")\n","    print(f\"  embed_dim: {embed_dim}\")\n","    print(f\"  ff_dim_base: {ff_dim_base}\")\n","    print(f\"  dropout: {dropout}\")\n","    print(f\"  ff_dropout: {ff_dropout}\")\n","    print(f\"  activation_function: {activation_function}\")\n","    print(f\"  num_ff_layers: {num_ff_layers}\")\n","    print(f\"  use_batchnorm: {use_batchnorm}\")\n","    print(f\"  use_layernorm: {use_layernorm}\")\n","    print(f\"  batch_size: {batch_size}\")\n","    print(f\"  lr: {lr}\")\n","    print(\"-\" * 50)\n","\n","    # K-Fold Cross Validation\n","    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","    fold_scores = []\n","    best_val_score = -float('inf')\n","    best_model_state_dict = None\n","\n","    for train_index, val_index in kf.split(X):\n","        # Split the data into training and validation sets for this fold\n","        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n","        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n","\n","        # Preprocess the data (scaling)\n","        transform = DataPreprocessingTransform()\n","        X_train_scaled = transform.fit_transform(X_train)  # numpy array\n","        X_val_scaled = transform.transform(X_val)          # numpy array\n","\n","        X_train_scaled_df = pd.DataFrame(X_train_scaled, index=X_train.index, columns=X_train.columns)\n","        X_val_scaled_df = pd.DataFrame(X_val_scaled, index=X_val.index, columns=X_val.columns)\n","\n","        # Now call remove_outliers_gpu on the DataFrames\n","        X_train_filtered = remove_outliers_gpu(X_train_scaled_df)\n","        X_val_filtered = remove_outliers_gpu(X_val_scaled_df)\n","\n","        # Align indices after outlier removal since both are DataFrames\n","        common_index = X_train_filtered.index.intersection(y_train.index)\n","        X_train_filtered = X_train_filtered.loc[common_index]\n","        y_train = y_train.loc[common_index]\n","\n","        common_val_index = X_val_filtered.index.intersection(y_val.index)\n","        X_val_filtered = X_val_filtered.loc[common_val_index]\n","        y_val = y_val.loc[common_val_index]\n","\n","        # Now you can create datasets as before\n","        train_dataset = CustomDataset(X_train_filtered, y_train, resample=True)\n","        val_dataset = CustomDataset(X_val_filtered, y_val)\n","\n","        # Create DataLoaders\n","        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","\n","        # Initialize the model\n","        model = FraudDetectionTransformer(\n","            input_dim=X_train.shape[1],\n","            embed_dim=embed_dim,\n","            num_heads=num_heads,\n","            ff_dim_base=ff_dim_base,\n","            dropout=dropout,\n","            ff_dropout=ff_dropout,\n","            activation_function=activation_function,\n","            num_ff_layers=num_ff_layers,\n","            use_batchnorm=use_batchnorm,\n","            use_layernorm=use_layernorm\n","        ).to(device)\n","\n","        # Define optimizer and loss\n","        optimizer = optim.Adam(model.parameters(), lr=lr)\n","        criterion = nn.BCELoss()\n","\n","        # Train the model with early stopping\n","        # For simplicity, define epochs and patience here or assume globally defined\n","        fold_epochs = 80\n","        fold_patience = 10\n","        train_losses, val_losses, _ = train_model_with_early_stopping(\n","            model, train_loader, val_loader, criterion, optimizer, epochs=fold_epochs, patience=fold_patience\n","        )\n","\n","        # Evaluate the model on the validation set\n","        accuracy, hit_rate, roc_auc = evaluate(model, val_loader)\n","\n","        # Use ROC AUC as the metric to maximize\n","        val_score = roc_auc\n","\n","        # Keep track of the best model across folds\n","        if val_score > best_val_score:\n","            best_val_score = val_score\n","            best_model_state_dict = model.state_dict()\n","\n","        fold_scores.append(val_score)\n","\n","    # Compute average score across all folds\n","    avg_fold_score = np.mean(fold_scores)\n","\n","    # Save the best model found during the folds\n","    if best_model_state_dict is not None:\n","        torch.save(best_model_state_dict, 'best_model.pth')\n","\n","    return avg_fold_score\n","\n","# Run Optuna optimization\n","study = optuna.create_study(direction=\"maximize\")\n","study.optimize(objective, n_trials=100)\n","\n","# Output the best parameters\n","print(\"Best hyperparameters:\", study.best_params)\n","\n","#=============================================== Test on Holdout Set ===============================================#\n","best_params = study.best_params\n","\n","# Build best model with tuned hyperparameters\n","best_model = FraudDetectionTransformer(\n","    input_dim=X.shape[1],\n","    embed_dim=best_params['embed_dim'],\n","    num_heads=best_params['num_heads'],\n","    ff_dim_base=best_params['ff_dim_base'],\n","    dropout=best_params['dropout'],\n","    ff_dropout=best_params['ff_dropout'],\n","    activation_function=best_params['activation_function'],\n","    num_ff_layers=best_params['num_ff_layers'],\n","    use_batchnorm=best_params['use_batchnorm'],\n","    use_layernorm=best_params['use_layernorm']\n",").to(device)\n","\n","# Preprocess train and test data using the same pipeline\n","transform = DataPreprocessingTransform()\n","X_scaled = transform.fit_transform(X)\n","X_test_scaled = transform.transform(X_test)\n","\n","# Convert to DataFrame before outlier removal\n","X_scaled_df = pd.DataFrame(X_scaled, index=X.index, columns=X.columns)\n","X_test_scaled_df = pd.DataFrame(X_test_scaled, index=X_test.index, columns=X_test.columns)\n","\n","# Remove outliers\n","X_filtered = remove_outliers_gpu(X_scaled_df)\n","common_index = X_filtered.index.intersection(y.index)\n","X_filtered = X_filtered.loc[common_index]\n","y = y.loc[common_index]\n","\n","X_test_filtered = remove_outliers_gpu(X_test_scaled_df)\n","common_test_index = X_test_filtered.index.intersection(y_test.index)\n","X_test_filtered = X_test_filtered.loc[common_test_index]\n","y_test = y_test.loc[common_test_index]\n","\n","# Create Datasets and Loaders\n","train_val_dataset = CustomDataset(X_filtered, y, resample=True)\n","test_dataset = CustomDataset(X_test_filtered, y_test)\n","\n","train_val_loader = DataLoader(train_val_dataset, batch_size=best_params[\"batch_size\"], shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=best_params[\"batch_size\"], shuffle=False)\n","\n","# Final training on combined train+val (if desired)\n","final_optimizer = optim.Adam(best_model.parameters(), lr=best_params['lr'])\n","final_criterion = nn.BCELoss()\n","\n","train_losses, val_losses, best_model_state_dict = train_model_with_early_stopping(\n","    best_model, train_val_loader, test_loader, criterion=final_criterion, optimizer=final_optimizer,\n","    epochs=epochs, patience=patience\n",")\n","\n","# Load the best model weights after final training\n","best_model.load_state_dict(best_model_state_dict)\n","\n","# Evaluate on Test Set\n","test_accuracy, test_hit_rate, test_roc_auc = evaluate(best_model, test_loader)\n","\n","print(\"Test Accuracy:\", test_accuracy)\n","print(\"Test Hit Rate:\", test_hit_rate)\n","print(\"Test ROC AUC:\", test_roc_auc)\n"]},{"cell_type":"markdown","metadata":{"id":"y0JuVWk0R5k6"},"source":["\n","\n","---\n","\n","# Auto commit to github"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"aborted","timestamp":1734713500562,"user":{"displayName":"Archie Goodman","userId":"05960694724077487952"},"user_tz":0},"id":"xWxH5PbcOdSb"},"outputs":[],"source":["import datetime\n","import os\n","\n","# Navigate to the repository directory (if not already there)\n","%cd /content/drive/MyDrive/Colab_Notebooks/Deep_Learning_Practice\n","\n","with open('/content/drive/MyDrive/IAM/PAT.txt', 'r') as file:\n","      github_pat = file.read().strip()\n","os.environ['GITHUB_PAT'] = github_pat\n","\n","!git remote add origin \"https://github.com/archiegoodman2/machine_learning_practice\"\n","\n","# Replace with your actual username and email (or configure globally)\n","USERNAME=\"archiegoodman2\"\n","EMAIL=\"archiegoodman2011@gmail.com\"\n","\n","# Set global username and email configuration\n","!git config --global user.name \"$USERNAME\"\n","!git config --global user.email \"$EMAIL\"\n","\n","now = datetime.datetime.now()\n","current_datetime = now.strftime(\"%Y-%m-%d %H:%M\")\n","\n","# Set remote URL using the PAT from environment variable\n","!git remote set-url origin https://{os.environ['GITHUB_PAT']}@github.com/archiegoodman2/machine_learning_practice.git\n","\n","# Replace with your desired commit message\n","COMMIT_MESSAGE = str(current_datetime) + \" \" + \" bug fixes, more comments \"\n","\n","# Stage all changes\n","!git add .\n","\n","# Commit the changes\n","!git commit -m \"$COMMIT_MESSAGE\"\n","\n","# Push to origin\n","!git push origin master\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"mount_file_id":"1cX2lOjHCdxCQCb5mu2Ta83cyJpI45yXA","authorship_tag":"ABX9TyPyEDaZDChglkT8xmeRHmJY"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}